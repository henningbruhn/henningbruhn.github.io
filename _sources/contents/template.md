
%%%%%%%%%%%%% figure %%%%%%%%%%%%%%%%%%%%%%%%
```{figure} pix/sgd_time.png
:name: gdtimefig
:width: 15cm

Comparison of  logistic loss per running time for batch gradient descent, 
and online and minibatch gradient descent. 
All algorithms were applied to an artificial logistic regression problem. Minibatch size was equal 
to 20, total sample size was 5000. Online and minibatch SGD converge much faster than 
batch gradient descent. Note the different scales of the axes.
```

