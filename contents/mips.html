

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Maximum inner product search &mdash; A test run  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="A test run" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            A test run
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Maximum inner product search</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#vector-quantisation">Vector quantisation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">A test run</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Maximum inner product search</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/contents/mips.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="maximum-inner-product-search">
<h1>Maximum inner product search<a class="headerlink" href="#maximum-inner-product-search" title="Link to this heading"></a></h1>
<p><span class="target" id="index-0"></span><em>Vector databases</em> are becoming more and more important.
What’s a vector database? A system to store a large number of vectors
<span class="math notranslate nohighlight">\(x^{(1)},\ldots, x^{(n)}\in\mathbb R^d\)</span>
in such a way that a (approximate) nearest neighbour search can be performed efficiently.
A recommender system, for instance, might
store the preferences of the users encoded as vectors; for a new user the five most similar
known users could be computed in order to recommend the products or services they prefered.
Another application comes from word or document embeddings: A number of vector representation
of documents are stored in the database; a user may then formulate a query (“which Tom Stoppard play
features Hamlet as a side character?”) that is transformed into a vector; the documents with
most similar vector representation are then returned.</p>
<p>What <em>most similar</em> means will differ from application to application. Often it may
simply mean: the largest scalar product. That is, given a query
<span class="math notranslate nohighlight">\(q\in\mathbb R^d\)</span> we look for the
<span class="math notranslate nohighlight">\(x^{(i)}\)</span>
with largest
<span class="math notranslate nohighlight">\(q^\intercal x^{(i)}\)</span>.
In that case, the problem is known as <span class="target" id="index-1"></span><em>maximum inner product search</em> (or MIPS).</p>
<div class="admonition-retrieval-augmented-generation admonition">
<p class="admonition-title">Retrieval augmented generation</p>
<p>It is not easy to keep an AI chatbot up-to-date with current affairs.
The large language model (LLM) it is based on is trained on a snapshot of data
available at training time (perhaps all of wikipedia as of 07/10/24). Later
events (announcement of the Physics Nobel Prize on 08/10/24) are thus not immediately accessible
to the bot.</p>
<p>Re-training an LLM to incorporate later events is resource intensive and therefore only
done rarely. An alternative is to provide the AI chatbot with an external memory. One such
method is <span class="target" id="index-2"></span><em>retrieval augmented generation</em>, or RAG.</p>
<p>In RAG, the user query is transformed into a vector via a topic embedding that is compared to
data in a vector database (this could be a current snapshot of wikipedia). The perhaps ten documents
that are most similar to the query are located and added to the prompt of the AI chatbot. The
bot then generates the response based on the query and the search results.</p>
<p><em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>,
P. Lewis et al. (2021), <a class="reference external" href="https://arxiv.org/pdf/2005.11401">arXiv:2005.11401</a></p>
</div>
<p>At first, the computational problem may appear to have an easy solution – after all, scalar
products can be computed very efficiently. With <span class="math notranslate nohighlight">\(n\)</span> vectors in the database, each with length <span class="math notranslate nohighlight">\(d\)</span>,
checking every scalar product amounts to a running time of <span class="math notranslate nohighlight">\(O(nd)\)</span>. The number of vectors, <span class="math notranslate nohighlight">\(n\)</span>,
however, will usually be extremely large, perhaps even in the billions, and the dimension <span class="math notranslate nohighlight">\(d\)</span> may
have three or four digits. Moreover, queries typically need to be answered very quickly. A recommender system,
for example, could easily need to address hundreds or thousands of queries every second.
As a result, a running time of <span class="math notranslate nohighlight">\(O(nd)\)</span> may be too slow.
How can this be sped up?</p>
<section id="vector-quantisation">
<h2>Vector quantisation<a class="headerlink" href="#vector-quantisation" title="Link to this heading"></a></h2>
<p>Let <span class="math notranslate nohighlight">\(x^{(1)},\ldots x^{(n)}\in\mathbb R^d\)</span> be the vectors  that make up the database,
let <span class="math notranslate nohighlight">\(k,m&gt;0\)</span> be integers, and let <span class="math notranslate nohighlight">\(\ell=\tfrac{d}{m}\)</span>, which we assume to be an integer. <a class="footnote-reference brackets" href="#fquant" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p>
<p>We split each vector <span class="math notranslate nohighlight">\(x\in\mathbb R^d\)</span> into <span class="math notranslate nohighlight">\(m\)</span> vectors each of length <span class="math notranslate nohighlight">\(\ell\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}x=\begin{pmatrix}x_1\\x_2\\\vdots\\x_m\end{pmatrix}, \text{ with }x_j\in\mathbb R^\ell\text{ for }j=1,\ldots, m\end{split}\]</div>
<p>(In practice, this partition of <span class="math notranslate nohighlight">\(\mathbb R^d\)</span> into subspaces is achieved by random sampling of the entry dimensions.)</p>
<p>Then, for <span class="math notranslate nohighlight">\(j=1,\ldots,m\)</span> we compute representative vectors <span class="math notranslate nohighlight">\(c_{1j},\ldots,c_{kj}\in\mathbb R^\ell\)</span>.
These are used to replace the database vectors by simpler vectors: For each <span class="math notranslate nohighlight">\(i=1,\ldots, n\)</span> and <span class="math notranslate nohighlight">\(j=1,\ldots, m\)</span>
we find a suitable <span class="math notranslate nohighlight">\(\hat x^{(i)}_j\in\{c_{j1},\ldots,c_{jk}\}\)</span>. That is, we basically replace <span class="math notranslate nohighlight">\(x^{(i)}_j\)</span> by one of <span class="math notranslate nohighlight">\(\{c_{j1},\ldots,c_{jk}\}\)</span>.
In this way <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is replaced by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat x^{(i)}=\begin{pmatrix}\hat x^{(i)}_1\\\vdots\\\hat x^{(i)}_m\end{pmatrix}\end{split}\]</div>
<p>For a query <span class="math notranslate nohighlight">\(q\in\mathbb R^d\)</span> we then approximate <span class="math notranslate nohighlight">\(q^\intercal x^{(i)}\approx q^\intercal\hat x^{(i)}\)</span>.</p>
<p>Before we look at <span class="math notranslate nohighlight">\(q\hat x^{(i)}\)</span> let me point out that replacing each <span class="math notranslate nohighlight">\(x^{(i)}\)</span> by <span class="math notranslate nohighlight">\(\hat x^{(i)}\)</span>
already results in a welcome compression of the data. Indeed, we only need to store all <span class="math notranslate nohighlight">\(c_{js}\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots, m\)</span>, <span class="math notranslate nohighlight">\(s=1,\ldots, k\)</span>
and, instead of <span class="math notranslate nohighlight">\(x^{(i)}\)</span> we store a vector <span class="math notranslate nohighlight">\(\hat z^{(i)}\in\{1,\ldots,k\}^m\)</span> with <span class="math notranslate nohighlight">\(\hat z^{(i)}_j=s\)</span> if and only if <span class="math notranslate nohighlight">\(\hat x^{(i)}_j=c_{js}\)</span>.</p>
<p>What is the computational complexity to compute <span class="math notranslate nohighlight">\(q^\intercal\hat x^{(i)}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>?
We write</p>
<div class="math notranslate nohighlight">
\[q^\intercal\hat x^{(i)} = \sum_{j=1}^mq^\intercal_j\hat x^{(i)}_j\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat x^{(i)}_j\)</span> is one of <span class="math notranslate nohighlight">\(c_{j1},\ldots, c_{jk}\)</span>. We first compute all scalar products <span class="math notranslate nohighlight">\(q^\intercal_jc_{js}\)</span> and
put them in a look-up table. Computing the scalar product takes <span class="math notranslate nohighlight">\(O(mk\ell)\)</span> time as <span class="math notranslate nohighlight">\(j\)</span> runs from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(m\)</span>, <span class="math notranslate nohighlight">\(s\)</span> runs
from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(k\)</span> and as the vectors have length <span class="math notranslate nohighlight">\(\ell\)</span>. With <span class="math notranslate nohighlight">\(\ell=\tfrac{d}{m}\)</span> the running time reduces to <span class="math notranslate nohighlight">\(O(kd)\)</span>.
Then, using the look-up table, we compute for all <span class="math notranslate nohighlight">\(i\)</span> the scalar product <span class="math notranslate nohighlight">\(q^\intercal\hat x^{(i)}\)</span>, with a running time of
<span class="math notranslate nohighlight">\(O(nm)\)</span>. In total we obtain a running time of <span class="math notranslate nohighlight">\(O(kd+nm)\)</span>. As <span class="math notranslate nohighlight">\(n\)</span> is typically the by far largest quantity among
<span class="math notranslate nohighlight">\(d,k,n,m\)</span> the running time is dominated by <span class="math notranslate nohighlight">\(O(nm)\)</span>, and as usually <span class="math notranslate nohighlight">\(m \ll d\)</span>, we achieve a substantial reduction
in comparison to the running time <span class="math notranslate nohighlight">\(O(nd)\)</span> of the naive algorithm.</p>
<p>How should the representatives <span class="math notranslate nohighlight">\(c_{ji},\ldots,c_{jk}\in\mathbb R^\ell\)</span> be chosen? Well, <span class="math notranslate nohighlight">\(q^\intercal\hat x^{(i)}\)</span>
should be a good approximation of <span class="math notranslate nohighlight">\(q^\intercal x^{(i)}\)</span>, which is the case if <span class="math notranslate nohighlight">\(q^\intercal_j\hat x^{(i)}_j\)</span> is a
good approximation of <span class="math notranslate nohighlight">\(q^\intercal_jx^{(i)}_j\)</span> for every <span class="math notranslate nohighlight">\(j\in\{1,\ldots, m\}\)</span>. So, let’s fix <span class="math notranslate nohighlight">\(j\)</span> and
let us assume that the queries <span class="math notranslate nohighlight">\(q\)</span> are drawn from some distribution <span class="math notranslate nohighlight">\(\mathcal D\)</span>.
Then, arguably, we should choose <span class="math notranslate nohighlight">\(c_{j1},\ldots,c_{jk}\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-vqobj">
<span class="eqno">(1)<a class="headerlink" href="#equation-vqobj" title="Link to this equation"></a></span>\[\mathbb E_{q\sim\mathcal D}\left[\sum_{i=1}^n(q^\intercal_jx^{(i)}_j-q^\intercal_j\hat x^{(i)}_j)^2\right]\]</div>
<p>is minimised. Let us rewrite that.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb E_{q\sim\mathcal D}\left[\sum_{i=1}^n\left(q^\intercal_jx^{(i)}_j-q^\intercal_j\hat x^{(i)}_j\right)^2\right]
&amp;= \sum_{i=1}^n\mathbb E_{q\sim\mathcal D}\left[\left(q^\intercal_j\left(x^{(i)}_j-\hat x^{(i)}_j\right)\right)^2\right]\\
&amp;=\sum_{i=1}^n\mathbb E_{q\sim\mathcal D}\left[\left( \cos\theta_{ij} ||q_j||\cdot ||x^{(i)}_j-\hat x^{(i)}_j|| \right)^2\right]\\
&amp;=\sum_{i=1}^n||x^{(i)}_j-\hat x^{(i)}_j||^2\cdot\mathbb E_{q\sim\mathcal D}\left[\left( \cos\theta_{ij} ||q_j||\right)^2\right],\end{split}\]</div>
<p>where we write <span class="math notranslate nohighlight">\(\theta_{ij}\)</span> for the angle between <span class="math notranslate nohighlight">\(q_j\)</span> and <span class="math notranslate nohighlight">\(x^{(i)}_j-\hat x^{(i)}_j\)</span>.</p>
<p>We’ve reached a point where we are stuck without further assumption on the distribution <span class="math notranslate nohighlight">\(\mathcal D\)</span>.
We do not want to impose strong conditions on it as it is quite outside our control.
However, not too far of a stretch seems to assume that <span class="math notranslate nohighlight">\(\mathcal D\)</span> is <em>isotropic</em>, ie, does not
depend on the direction. Under that assumption, we obtain:</p>
<div class="math notranslate nohighlight">
\[\mathbb E_{q\sim\mathcal D}\left[\sum_{i=1}^n\left(q^\intercal_jx^{(i)}_j-q^\intercal_j\hat x^{(i)}_j\right)^2\right]
=C\sum_{i=1}^n||x^{(i)}_j-\hat x^{(i)}_j||^2,\]</div>
<p>for some constant <span class="math notranslate nohighlight">\(C&gt;0\)</span> that only depends on <span class="math notranslate nohighlight">\(\mathcal D\)</span> but not on <span class="math notranslate nohighlight">\(\hat x^{(i)}_j\)</span>.</p>
<p>Recall that each <span class="math notranslate nohighlight">\(\hat x^{(i)}_j\)</span> is one of <span class="math notranslate nohighlight">\(c_{j1},\ldots,c_{jk}\)</span>. Thus</p>
<div class="math notranslate nohighlight">
\[\text{argmin}_{c_{j1},\ldots,c_{jk}} \mathbb E_{q\sim\mathcal D}\left[\sum_{i=1}^n\left(q^\intercal_jx^{(i)}_j-q^\intercal_j\hat x^{(i)}_j\right)^2\right]
= \text{argmin}_{c_{j1},\ldots,c_{jk}} \sum_{i=1}^n\min_{s}||x^{(i)}_j-c_{js}||^2,\]</div>
<p>which is nothing else than the <span class="math notranslate nohighlight">\(k\)</span>-means objective!</p>
<p>What does that mean? We split each database vector <span class="math notranslate nohighlight">\(x^{(i)}\)</span> into chunks of size <span class="math notranslate nohighlight">\(\tfrac{d}{m}\)</span>, and then, for each <span class="math notranslate nohighlight">\(j=1,\ldots, m\)</span>,
solve for the data <span class="math notranslate nohighlight">\(x^{(1)}_j,\ldots, x^{(n)}_j\)</span> a <span class="math notranslate nohighlight">\(k\)</span>-means clustering problem, resulting
in the centres <span class="math notranslate nohighlight">\(c_{j1},\ldots,c_{jk}\)</span>; finally we replace each <span class="math notranslate nohighlight">\(x^{(i)}_j\)</span> with the nearest cluster centre <span class="math notranslate nohighlight">\(c_{js}\)</span>.</p>
<p id="index-3">Replacing vectors in a dataset by simpler vectors is  called <em>vector quantisation</em>, and also used as a compression tool,
for example, in video or audio codecs.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The approach outlined here can be improved upon. Indeed, I’ve cheated with the objective <a class="reference internal" href="#equation-vqobj">(1)</a>: The objective
incorporates <em>all</em> scalar products <span class="math notranslate nohighlight">\(q^\intercal\hat x^{(i)}\)</span>. In the applications, however, we just want to find
the datapoint with largest scalar product with the query, or rather, the top-10 datapoints with largest scalar product.
Thus, it does not matter if <span class="math notranslate nohighlight">\(q^\intercal\hat x^{(i)}\)</span> deviates from <span class="math notranslate nohighlight">\(q^\intercal x^{(i)}\)</span> as long as both scalar products
are not too large. This insight can be used to devise a more complicated loss function that results in
better performance; see Guo et al. (2020). <a class="footnote-reference brackets" href="#fguo" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fquant" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><em>Quantization based Fast Inner Product Search</em>, R. Guo, S. Kumar, K. Choromanski and D. Simcha (2015), <a class="reference external" href="https://arxiv.org/abs/1509.01469">arXiv:1509.01469</a></p>
</aside>
<aside class="footnote brackets" id="fguo" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p><em>Accelerating Large-Scale Inference with Anisotropic Vector Quantization</em>, R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern and S. Kumar (2020), <a class="reference external" href="https://arxiv.org/pdf/1908.10396">arXiv:1908.10396</a></p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="A test run" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, me.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>