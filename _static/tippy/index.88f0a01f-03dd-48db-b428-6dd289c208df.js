selector_to_html = {"a[href=\"contents/rl.html#parameterised-policies\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.9. </span>Parameterised policies<a class=\"headerlink\" href=\"#parameterised-policies\" title=\"Link to this heading\">#</a></h2><p><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning is not always possible. If the set of possible states or the set of possible actions is\nlarge then it becomes infeasible to compute <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values. There are two ways out of this: We may\ntry to learn a predictor for the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values; or we may try to learn a policy that is not based on <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.\nLet\u2019s concentrate on the latter method.</p><p>Recall that every state in the <a class=\"reference internal\" href=\"#polesec\"><span class=\"std std-ref\">pole balancing</span></a> task is characterised by four parameters: position, velocity,\nangle and angular velocity.\nA linear policy in this case would, based on a weight vector <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^4\\)</span>,\ndecide for a state <span class=\"math notranslate nohighlight\">\\(s\\in\\mathbb R^4\\)</span>  as follows:</p>", "a[href=\"contents/rl.html#policy-improvement-theorem\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.7. </span>Policy improvement theorem<a class=\"headerlink\" href=\"#policy-improvement-theorem\" title=\"Link to this heading\">#</a></h2><p>Assume an agent is following a policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span> but is given the opportunity to change a single action in a state <span class=\"math notranslate nohighlight\">\\(s\\)</span>.\nWhich one should she choose? To answer this question, we introduce <em>state-action values</em>,\nor shorter <em><span class=\"math notranslate nohighlight\">\\(q\\)</span>-values</em>. Given a state <span class=\"math notranslate nohighlight\">\\(s\\)</span> and an action <span class=\"math notranslate nohighlight\">\\(a\\)</span>, the value <span class=\"math notranslate nohighlight\">\\(q_\\pi(s,a)\\)</span>\ngives the expected discounted return that is obtained by\nchoosing action <span class=\"math notranslate nohighlight\">\\(a\\)</span> in state <span class=\"math notranslate nohighlight\">\\(s\\)</span> and then following policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>:</p>", "a[href=\"contents/rl.html#policy-gradient-method\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.10. </span>Policy gradient method<a class=\"headerlink\" href=\"#policy-gradient-method\" title=\"Link to this heading\">#</a></h2><p>Assume that the agent is positioned in state <span class=\"math notranslate nohighlight\">\\(s\\)</span>, and assume that some\nstochastic parameterised policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span> is given.<label class=\"margin-toggle\" for=\"sidenote-role-5\"><span id=\"id5\">\n<sup>5</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-5\" name=\"sidenote-role-5\" type=\"checkbox\"/><span class=\"sidenote\"><sup>5</sup>This section is largely based on material of <a class=\"reference external\" href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\">OpenAI</a></span>\nHere the subscript <span class=\"math notranslate nohighlight\">\\(w\\)</span> indicates the set of weights\nthat describes the policy. The return we can expect with policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span> in state <span class=\"math notranslate nohighlight\">\\(s\\)</span> is then</p>", "a[href=\"contents/rl.html#baselines\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.11. </span>Baselines<a class=\"headerlink\" href=\"#baselines\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-6\"></label><input class=\"margin-toggle\" id=\"marginnote-role-6\" name=\"marginnote-role-6\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/sreinforcement_learning/policy_grad.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>policy grad</a></span>\nHow can we improve the policy gradient method? Here is what seems nonsensical about the\nmethod: No matter how good the trajectory <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is, we try to improve the likelihood\nof the trajectory by pushing the weights in the direction of it. The return <span class=\"math notranslate nohighlight\">\\(G(\\tau)\\)</span> of the\ntrajectory only influences how hard we push. If <span class=\"math notranslate nohighlight\">\\(G(\\tau)\\)</span> is large, the change <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> will\nbe large, if <span class=\"math notranslate nohighlight\">\\(H(\\tau)\\)</span> is small, <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> will be smaller \u2013 however, we always push to reinforce <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span>.</p><p>What would seem more promising: If <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is a trajectory that is exceptionally good, then we should\nmake it more likely, but if <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is worse than average, we should make it less likely, ie, push in\nthe opposite direction. That is, if <span class=\"math notranslate nohighlight\">\\(b\\)</span> is the average return of a trajectory then the sign of <span class=\"math notranslate nohighlight\">\\(G(\\tau)-b\\)</span>\ntells us whether <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is better or worse than an average trajectory.\n(How can we compute <span class=\"math notranslate nohighlight\">\\(b\\)</span>? Easy: We sample a number of trajectories and take the average of the returns.)</p>", "a[href=\"contents/rl.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9. </span>Reinforcement learning<a class=\"headerlink\" href=\"#reinforcement-learning\" title=\"Link to this heading\">#</a></h1><p>What is reinforcement learning?<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup>The material in this chapter is based on <em>Reinforcement Learning</em>, R.S. Sutton and A.G. Barto, MIT Press (2018)\nand <em>Spinning Up in Deep RL</em>, J. Achiam, <a class=\"reference external\" href=\"https://spinningup.openai.com/en/latest/index.html\">link</a></span>\nIn reinforcement learning an autonomous agent interacts with a possibly unknown\nenvironment. Each action the agent takes results in a state change and a reward or penalty.\nThe agent strives to maximise the total reward.</p>", "a[href=\"contents/rl.html#reinforcement-learning-and-llms\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.13. </span>Reinforcement learning and LLMs<a class=\"headerlink\" href=\"#reinforcement-learning-and-llms\" title=\"Link to this heading\">#</a></h2><p>After training on a large text corpus,\nan AI chatbot such as ChatGPT, Gemini or Claude needs to be finetuned\nto ensure that it produces helpful, harmless und honest output.\nThis is a challenge because it is hard to come by high quality training data:\nIn contrast to pre-training the language model, where basically a good part of the internet is ingested,\ngood and helpful answers to prompts would need to be written by humans. This is burdensome, costly and\ntime consuming. As a result any such dataset of prompts and model answers will be\nsmall.</p><p>Because of the dearth of good training datasets, current AI chatbots\nare finetuned in a different way. Later versions of ChatGPT, for example, are finetuned\nwith <em>reinforcement learning from human feedback</em>,<label class=\"margin-toggle\" for=\"sidenote-role-8\"><span id=\"id8\">\n<sup>8</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-8\" name=\"sidenote-role-8\" type=\"checkbox\"/><span class=\"sidenote\"><sup>8</sup><em>Training language models to follow instructions\nwith human feedback</em>, Ouyang et al. (2022), <a class=\"reference external\" href=\"https://arxiv.org/abs/2203.02155\">arXiv:2203.02155</a></span>\nwhich we\u2019ll briefly describe here.</p>", "a[href=\"contents/rl.html#markov-decision-processes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.1. </span>Markov decision processes<a class=\"headerlink\" href=\"#markov-decision-processes\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s consider a classical toy example, an example of a <em>gridworld</em>; see <a class=\"reference internal\" href=\"#gridworldfig\"><span class=\"std std-numref\">Fig. 9.2</span></a>.\nAn agent starts at position <span class=\"math notranslate nohighlight\">\\(z\\)</span> and moves in each time step from one square to the next. The agent\nis not allowed to leave the grid, and cannot enter the white (blocked) square above the starting position.\nThe aim of the agent is to reach, in the shortest possible way, the square in the upper right corner, where a reward of +1\nis waiting. The square just below it will result in a penalty of -1.\nThe task is stopped if the agent enters any of these two squares.</p>", "a[href=\"contents/rl.html#pole-balancing\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.2. </span>Pole balancing<a class=\"headerlink\" href=\"#pole-balancing\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-3\"></label><input class=\"margin-toggle\" id=\"marginnote-role-3\" name=\"marginnote-role-3\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/reinforcement_learning/pole.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>pole</a></span>\nLet\u2019s have a look at a classic reinforcement learning problem. The task consists in balancing a pole on a cart.\nWhen the pole starts falling over, the cart can move to counterbalance the movement of the pole. The aim\nis to keep the pole from falling as long as possible. See <a class=\"reference internal\" href=\"#polefig\"><span class=\"std std-numref\">Fig. 9.3</span></a>.</p>", "a[href=\"contents/rl.html#on-and-off-policy\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.12. </span>On- and off-policy<a class=\"headerlink\" href=\"#on-and-off-policy\" title=\"Link to this heading\">#</a></h2><p>The <a class=\"reference internal\" href=\"#pgmalg\">Algorithm 9.3</a> (policy gradient method) is <em>on-policy</em>: the algorithm needs to generate\ntrajectories that follow the current policy (see line 4).\nAt first glance this looks to be the case for <a class=\"reference internal\" href=\"#qlearnalg\">Algorithm 9.2</a> (<span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning), too.\nThe next action is chosen <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>-greedily, and thus most of the time\naccording to the current <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values. The algorithm, however, does not need that.\nWhat it needs is that every state/action pair is visited arbitrarily often,\nie, that the conditions of <a class=\"reference internal\" href=\"#qlthm\">Theorem 9.6</a> are satisfied.\nIn fact, <span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning is an <em>off-policy</em> method: the\ntrajectories do not have to come from the current policy.</p><p>Off-policy methods have an advantage over on-policy methods. They allow\ntraining with historical data. Often historical data is easier to collect.\nImagine the situation that a climate control system should be run by a\nreinforcement learning algorihm. The current system is either controlled\nby humans or by simple rules. In either case, plenty of data of past\nperformance is likely available and could be used to at least start\ntraining with an off-policy method. In a setting like a climate control\nsystem, on-policy learning may be very slow (climate settings won\u2019t be changed\na thousand times every hour) and a  badly performing initial\npolicy might lead to unacceptable climate control over an extended\nperiod of time (too hot or too cold because the policy still\nneeds to improve).</p>", "a[href=\"#mathematics-of-machine-learning\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Mathematics of Machine Learning<a class=\"headerlink\" href=\"#mathematics-of-machine-learning\" title=\"Link to this heading\">#</a></h1><p>What is this? This is a trial run to see how I can best publish my lecture notes publicly. It\u2019s very much in an\nalpha stage, with lots of errors, incongruities and omissions.</p>", "a[href=\"contents/rl.html#policies\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.5. </span>Policies<a class=\"headerlink\" href=\"#policies\" title=\"Link to this heading\">#</a></h2><p>It\u2019s time to turn to the agent. Once fully trained, how should the agent act? What the agent\nknows about the current situation is encapsulated in the state. Based on that state\nthe agent has to decide which action to take. That is, an agent is described by a function\nfrom states to actions. Such a function</p>", "a[href=\"contents/rl.html#what-else-is-there\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.14. </span>What else is there?<a class=\"headerlink\" href=\"#what-else-is-there\" title=\"Link to this heading\">#</a></h2><p>The methods treated here will not be enough to train a world-class\nchess engine or a fully autonomous robot.\nThe spectacular advances in reinforcement learning that we have seen\nin recent years become only possible if the basic ideas presented here\nare combined with the power of deep neural networks.</p><p>In <em>deep <span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning</em>, for instance, a deep neural network learns to\napproximate a <span class=\"math notranslate nohighlight\">\\(q\\)</span>-function.<label class=\"margin-toggle\" for=\"sidenote-role-9\"><span id=\"id9\">\n<sup>9</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-9\" name=\"sidenote-role-9\" type=\"checkbox\"/><span class=\"sidenote\"><sup>9</sup><em>Playing Atari with Deep Reinforcement Learning</em>,\nV. Mnih, K. Kavukcuoglu, D. Silver, D. Wierstra, A. Graves\nand I. Antonoglou (2013), <a class=\"reference external\" href=\"https://arxiv.org/abs/1312.5602\">arXiv:1312.5602</a></span>\n<em>Actor-critic</em> algorithms extend this: two neural networks\nare trained. One, the critic, learns to predict <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values by observing\nthe other neural network, the actor; while the actor constantly tries\nto improve a policy by exploiting the approximated <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values of the critic.</p>", "a[href=\"contents/rl.html#what-is-known-about-the-environment\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.3. </span>What is known about the environment?<a class=\"headerlink\" href=\"#what-is-known-about-the-environment\" title=\"Link to this heading\">#</a></h2><p>When we train a reinforcement learning system, what do we usually know about the environment?\nIn particular, do we have access to the transition probability and the reward function?\nSometimes yes, sometimes no. This depends on the scenario.</p><p>In toy problems we often know everything about the environment. What about a real-life scenario?\nWhen we try to train a control algorithm for a gas turbine, or a Go playing system,\nwe cannot directly train in the real environment: That would take too long (wait for 1000000 people\nto play against your at the beginning amusingly bad Go playing system), be too costly (pay the\n1000000 people), or result in catastrophic failure (gas turbine explodes).\nInstead, we may train\u2026</p>", "a[href=\"contents/rl.html#q-learning\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.8. </span><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning<a class=\"headerlink\" href=\"#q-learning\" title=\"Link to this heading\">#</a></h2><p>How can we learn an optimal policy? Above we sketched a procedure: Start with some policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>,\ncompute its <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values, check whether there is a\nstate <span class=\"math notranslate nohighlight\">\\(s\\)</span> and an action <span class=\"math notranslate nohighlight\">\\(a\\)</span> with <span class=\"math notranslate nohighlight\">\\(q_{\\pi}(s,{\\pi}(s))&lt;q_{\\pi}(s,a)\\)</span>,\nchange the policy <span class=\"math notranslate nohighlight\">\\(\\pi(s)=a\\)</span> and repeat. The procedure is very slow and suffers from a serious disadvantage:\nwe don\u2019t know how to compute the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.</p><p>Fortunately, there is a method, <em><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning</em>, that bypasses the policy improvement\nsteps and directly learns the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values <span class=\"math notranslate nohighlight\">\\(q^*\\)</span> of an optimal deterministic policy <span class=\"math notranslate nohighlight\">\\(\\pi^*\\)</span> \u2013 provided\nthe MDP is finite.\nHow then can we recover the policy from the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values?\nIt follows directly from <a class=\"reference internal\" href=\"#qoptthm\">Theorem 9.4</a> that</p>", "a[href=\"contents/rl.html#returns\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.4. </span>Returns<a class=\"headerlink\" href=\"#returns\" title=\"Link to this heading\">#</a></h2><p>Once the agent in a reinforcement learning task starts it follows a\ntrajectory of states, actions, and rewards:</p>", "a[href=\"contents/rl.html#value-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.6. </span>Value functions<a class=\"headerlink\" href=\"#value-functions\" title=\"Link to this heading\">#</a></h2><p>In many reinforcement learning tasks, a reward will only be awarded after many steps. A chess game has no immediate rewards;\nit is only won or lost at the end. In most tasks actions may have consequences that only become apparent many steps later.\nIn an inventory control problem, it may  be beneficial in the short run to not order anything, as no costs for purchases\nor storage are incurred; in the long run, however, we will pay dearly when we cannot satisfy customer demands.</p><p>Immediate rewards, therefore, are not a good basis for the next action. It\u2019s more important to estimate the long term\nconsequences of actions. This is where the <em>value function</em> comes in: It estimates the long term returns we can reap\nin a given state.\nHow we value a state depends on the setting of the task, whether we optimise total returns or discounted returns.\nHowever, as discounted returns default to total returns if the discounting factor <span class=\"math notranslate nohighlight\">\\(\\gamma\\)</span> is set to 1, we\ncan treat both settings in the same way.\nGiven a  policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>, we define for every state <span class=\"math notranslate nohighlight\">\\(s_0\\)</span> the value function as</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
