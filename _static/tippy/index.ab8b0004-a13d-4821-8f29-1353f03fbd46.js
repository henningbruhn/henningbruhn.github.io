selector_to_html = {"a[href=\"contents/loss.html#imbalanced-classes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.4. </span>Imbalanced classes<a class=\"headerlink\" href=\"#imbalanced-classes\" title=\"Link to this heading\">#</a></h2><p>Not all classes will always have the same importance in classification. A prime example is a spam filter: It is merely annoying if a spam email is not\ndetected, an important mail, however, that is banished to the spam folder may cause some grief. We\u2019d prefer if the\nspam filter is more cautious when classifying emails as spam.</p><p>A somewhat similar issue may arise when the classes are not present in equal numbers in the training set. Say, we try to classify cat and dog pictures.\nScraping the internet results in 90000 cat pictures but only 10000 dog pictures. For whatever reasons, we may still insist that our classifier perform\nequally well for cat and dog pictures \u2013 and that\u2019s a problem, as a classifier that always outputs `cat\u2019 will likely achieve an accuracy\nof around 90%.</p>", "a[href=\"contents/nets.html#local-minima\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.6. </span>Local minima<a class=\"headerlink\" href=\"#local-minima\" title=\"Link to this heading\">#</a></h2><p>Neural networks are trained with SGD, or with a variant of SGD.\nWhy does that work at all? The loss functions is highly\nnon-convex. Indeed, because of the symmetries in the weights it is almost guaranteed that\nthere will be many local minima: permuting the nodes of a layer (together with their weights)\nwill turn one local minimum into another one.</p><p>Why doesn\u2019t SGD regularly become trapped in a local minimum that is far away from the\nglobal minimum?<br/>\nWhy are local minima not a problem?</p>", "a[href=\"contents/convex.html#discussion-of-sgd\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.8. </span>Discussion of SGD<a class=\"headerlink\" href=\"#discussion-of-sgd\" title=\"Link to this heading\">#</a></h2><p>Descent methods are old, simple and have apparently been observed\nto be ``slow and unreliable\u2019\u2019.<label class=\"margin-toggle\" for=\"sidenote-role-4\"><span id=\"id5\">\n<sup>4</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-4\" name=\"sidenote-role-4\" type=\"checkbox\"/><span class=\"sidenote\"><sup>4</sup><em>Deep learning</em>, p.~148</span> Moreover,\nin convex optimisation, when both algorithms are known to converge,\nSGD has even worse convergence rates than vanilla gradient descent.\nIn fact, while the error <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>, the difference <span class=\"math notranslate nohighlight\">\\(f(x^{(t)})-f(x^*)\\)</span>\nis known to drop exponentially for gradient descent, ie</p>", "a[href=\"contents/intro.html#a-statistical-model\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.12. </span>A statistical model<a class=\"headerlink\" href=\"#a-statistical-model\" title=\"Link to this heading\">#</a></h2><p>On the face of it, what machine learning algorithms accomplish seems implausible.\nWe train the predictors on one dataset, the training set, and then expect\nthe predictors to perform well on a completely different dataset, ie, on new data, or\non the test set. How can that work at all?</p><p>That works \u2013 if training data and new data come from the same source. Mathematically,\nwe model this by a probability distribution  <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span> on <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\times\\mathcal Y\\)</span>.\nThat is, we assume that there is distribution that produces pairs <span class=\"math notranslate nohighlight\">\\((x,y)\\in\\mathcal X\\times\\mathcal Y\\)</span>\nof datapoints <span class=\"math notranslate nohighlight\">\\(x\\)</span> and classes <span class=\"math notranslate nohighlight\">\\(y\\)</span>, and we assume that the training set as well as\nany new data (or the test set) is drawn from this distribution.</p>", "a[href=\"contents/convex.html#sgdsec\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.6. </span>Stochastic gradient descent<a class=\"headerlink\" href=\"#sgdsec\" title=\"Link to this heading\">#</a></h2><p>Gradient descent is a quite efficient algorithm. Under mild assumptions and with the right\n(adaptable) learning rate it can be shown that the error <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>, the difference <span class=\"math notranslate nohighlight\">\\(f(\\overline x)-f(x^*)\\)</span>,\ndecreases exponentially with the number of iterations, i.e. that</p>", "a[href=\"contents/rl.html#on-and-off-policy\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.12. </span>On- and off-policy<a class=\"headerlink\" href=\"#on-and-off-policy\" title=\"Link to this heading\">#</a></h2><p>The <a class=\"reference internal\" href=\"#pgmalg\">Algorithm 7.3</a> (policy gradient method) is <em>on-policy</em>: the algorithm needs to generate\ntrajectories that follow the current policy (see line 4).\nAt first glance this looks to be the case for <a class=\"reference internal\" href=\"#qlearnalg\">Algorithm 7.2</a> (<span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning), too.\nThe next action is chosen <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>-greedily, and thus most of the time\naccording to the current <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values. The algorithm, however, does not need that.\nWhat it needs is that every state/action pair is visited arbitrarily often,\nie, that the conditions of <a class=\"reference internal\" href=\"#qlthm\">Theorem 7.6</a> are satisfied.\nIn fact, <span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning is an <em>off-policy</em> method: the\ntrajectories do not have to come from the current policy.</p><p>Off-policy methods have an advantage over on-policy methods. They allow\ntraining with historical data. Often historical data is easier to collect.\nImagine the situation that a climate control system should be run by a\nreinforcement learning algorihm. The current system is either controlled\nby humans or by simple rules. In either case, plenty of data of past\nperformance is likely available and could be used to at least start\ntraining with an off-policy method. In a setting like a climate control\nsystem, on-policy learning may be very slow (climate settings won\u2019t be changed\na thousand times every hour) and a  badly performing initial\npolicy might lead to unacceptable climate control over an extended\nperiod of time (too hot or too cold because the policy still\nneeds to improve).</p>", "a[href=\"contents/convex.html#strong-convexity\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.4. </span>Strong convexity<a class=\"headerlink\" href=\"#strong-convexity\" title=\"Link to this heading\">#</a></h2><p>Many of the functions we encounter in machine learning are at least locally convex,\nand usually these even exhibit a stronger notion of convexity that is called,\nwell, <em>strong</em> convexity. The difference between convexity and strong convexity\nis basically the difference between an affine function such as <span class=\"math notranslate nohighlight\">\\(x\\mapsto x\\)</span> and a\nquadratic function such as <span class=\"math notranslate nohighlight\">\\(x\\mapsto x^2\\)</span>. Affine functions are convex but barely so:\nthey satisfy the defining inequality of convexity <a class=\"reference internal\" href=\"#equation-convdef\">(3.1)</a> with equality. For\na strongly convex function this will never be the case.</p><p>A function <span class=\"math notranslate nohighlight\">\\(f:K\\to\\mathbb R\\)</span> on a convex set <span class=\"math notranslate nohighlight\">\\(K\\subseteq\\mathbb R^d\\)</span> is\n<em><span class=\"math notranslate nohighlight\">\\(\\mu\\)</span>-strongly convex</em> for <span class=\"math notranslate nohighlight\">\\(\\mu&gt;0\\)</span> if for all <span class=\"math notranslate nohighlight\">\\(\\lambda\\in [0,1]\\)</span>\nand <span class=\"math notranslate nohighlight\">\\(x,y\\in K\\)</span> it holds that</p>", "a[href=\"contents/intro.html#polynomial-predictors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.6. </span>Polynomial predictors<a class=\"headerlink\" href=\"#polynomial-predictors\" title=\"Link to this heading\">#</a></h2><p>Linear predictors are very simple and will often have a large training error.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-11\"></label><input class=\"margin-toggle\" id=\"marginnote-role-11\" name=\"marginnote-role-11\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/quadpred.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>quadpred</a></span>\nA classifier based on quadratic functions, or even higher polynomials,\nwill certainly be more powerful and will often have smaller training error.\nFor a quadratic classifier,\nwe look for weights <span class=\"math notranslate nohighlight\">\\(u\\in\\mathbb R^{d\\times d}\\)</span>, <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^d\\)</span> and <span class=\"math notranslate nohighlight\">\\(b\\in\\mathbb R\\)</span>\nsuch that</p>", "a[href=\"contents/convex.html#convex-optimisation-problems\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.2. </span>Convex optimisation problems<a class=\"headerlink\" href=\"#convex-optimisation-problems\" title=\"Link to this heading\">#</a></h2><p>A <em>convex optimisation problem</em> is any problem of the form</p>", "a[href=\"contents/rl.html#markov-decision-processes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.1. </span>Markov decision processes<a class=\"headerlink\" href=\"#markov-decision-processes\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s consider a classical toy example, an example of a <em>gridworld</em>; see <a class=\"reference internal\" href=\"#gridworldfig\"><span class=\"std std-numref\">Fig. 7.2</span></a>.\nAn agent starts at position <span class=\"math notranslate nohighlight\">\\(z\\)</span> and moves in each time step from one square to the next. The agent\nis not allowed to leave the grid, and cannot enter the white (blocked) square above the starting position.\nThe aim of the agent is to reach, in the shortest possible way, the square in the upper right corner, where a reward of +1\nis waiting. The square just below it will result in a penalty of -1.\nThe task is stopped if the agent enters any of these two squares.</p>", "a[href=\"contents/convex.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3. </span>Stochastic gradient descent<a class=\"headerlink\" href=\"#stochastic-gradient-descent\" title=\"Link to this heading\">#</a></h1><p>How is a neural network trained? How can we minimise logistic loss in order\nto learn the parameters of a logistic regression? Both cases reduce to an\noptimisation problem that requires a numerical optimisation algorithm, often a variant\nof a gradient descent technique. In the nicest and simplest setting, a convex optimisation\nproblem, these are even guaranteed to find an optimal solution.</p>", "a[href=\"contents/convex.html#convex-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.3. </span>Convex functions<a class=\"headerlink\" href=\"#convex-functions\" title=\"Link to this heading\">#</a></h2><p>Which functions are convex?\nNorms are convex. Indeed, the function <span class=\"math notranslate nohighlight\">\\(x\\mapsto ||x||\\)</span> is convex as for every <span class=\"math notranslate nohighlight\">\\(\\lambda\\in [0,1]\\)</span>\nthe triangle inequality implies:</p>", "a[href=\"contents/intro.html#vocabulary\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.2. </span>Vocabulary<a class=\"headerlink\" href=\"#vocabulary\" title=\"Link to this heading\">#</a></h2><p>In a classification task, we aim to train a classifier that assigns labels or a class to\ngiven input data.\nIn the MNIST digit recognition task, the input consists of grey-scale images of size\n28<span class=\"math notranslate nohighlight\">\\(\\times\\)</span>28 pixels. This is the <em>domain set</em>,\nthe set <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\)</span> that contains all (possible) data points for the task at hand.\nNormally <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\)</span> is a finite-dimensional vector space such as <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n\\)</span>, or at least a\nsubset of <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n\\)</span>. In the MNIST task, we could set <span class=\"math notranslate nohighlight\">\\(\\mathcal X=\\{0,\\ldots, 255\\}^{28\\times 28}\\)</span>,\nif we assume that each pixel has a grey value in 0,\u2026,255.</p><p>The entries of a data point <span class=\"math notranslate nohighlight\">\\(x\\in\\mathcal X\\)</span> are\nthe <em>features</em> or <em>attributes</em> of <span class=\"math notranslate nohighlight\">\\(x\\)</span>. This could be\nthe grey value of a pixel or the income of a customer.</p>", "a[href=\"contents/loss.html#bayes-consistent-loss-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.2. </span>Bayes consistent loss functions<a class=\"headerlink\" href=\"#bayes-consistent-loss-functions\" title=\"Link to this heading\">#</a></h2><p>What properties should a good loss function satisfy? Recall that logistic, exponential and square loss are <em>surrogate</em> loss\nfunctions: We are not really interested in a small logistic loss or a small square loss \u2013 rather, our aim\nis to minimise true risk, ie, expected zero-one loss. Zero-one loss, however, is not smooth and because of that difficult to minimise directly.\nThe surrogate losses discussed in the previous section are all smooth and thus easier to minimise.</p><p>As can be seen in <a class=\"reference internal\" href=\"#lossfunsfig\"><span class=\"std std-numref\">Fig. 6.1</span></a>, each of logistic, square and exponential loss upper-bounds zero-one loss.\nThat is good, because it means that when the surrogate loss becomes smaller then, usually, zero-loss will decrease as well.</p>", "a[href=\"contents/pac.html#overfitting-and-underfitting\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">2.3. </span>Overfitting and underfitting<a class=\"headerlink\" href=\"#overfitting-and-underfitting\" title=\"Link to this heading\">#</a></h2><p>By <a class=\"reference internal\" href=\"#testerrthm\">Theorem 2.2</a>, the first part can be seen to be reasonably small, and the last part is\nsimply the training error, aka, the empirical risk. The difference between test and training error\nis also called the \\defi{generalisation gap}. A large generalisation gap\nmeans that the classifier learns the training set and not the underlying distribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span>.\nIn other words, the classifier is <em>overfitting</em>. Often this is the case because the classifier\n(or rather the class <span class=\"math notranslate nohighlight\">\\(\\mathcal H\\)</span>) has too many degrees of freedom.\nIf the training error is large then\nthe classifier is not flexible enough to accomodate the training set \u2014 it is said to <em>underfit</em>; see <a class=\"reference internal\" href=\"#underfitfig\"><span class=\"std std-numref\">Fig. 2.4</span></a>.</p>", "a[href=\"contents/rl.html#parameterised-policies\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.9. </span>Parameterised policies<a class=\"headerlink\" href=\"#parameterised-policies\" title=\"Link to this heading\">#</a></h2><p><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning is not always possible. If the set of possible states or the set of possible actions is\nlarge then it becomes infeasible to compute <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values. There are two ways out of this: We may\ntry to learn a predictor for the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values; or we may try to learn a policy that is not based on <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.\nLet\u2019s concentrate on the latter method.</p><p>Recall that every state in the <a class=\"reference internal\" href=\"#polesec\"><span class=\"std std-ref\">pole balancing</span></a> task is characterised by four parameters: position, velocity,\nangle and angular velocity.\nA linear policy in this case would, based on a weight vector <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^4\\)</span>,\ndecide for a state <span class=\"math notranslate nohighlight\">\\(s\\in\\mathbb R^4\\)</span>  as follows:</p>", "a[href=\"contents/intro.html#nearest-neighbour\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.7. </span>Nearest neighbour<a class=\"headerlink\" href=\"#nearest-neighbour\" title=\"Link to this heading\">#</a></h2><p>A classic and very simple algorithm is <em>nearest neighbour</em>.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-12\"></label><input class=\"margin-toggle\" id=\"marginnote-role-12\" name=\"marginnote-role-12\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/nearest.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>nearest</a></span>\nDuring training\nthe algorithm simply memorises all training data points. When it is tasked to predict the\nclass of a new data point it determines the closest training data point and outputs the class\nof the training data point. The idea is that two data points that have similar features are\nlikely to have the same class.\\movtip{neighbour}%</p><p>As described the algorithm is very sensitive towards noise in the training set.\nA single erroneously classified data point in the training set, for instance, may lead to\nmany bad predictions of new data. Because of that, a more robust variant is often used,\n<em><span class=\"math notranslate nohighlight\">\\(k\\)</span>-nearest neighbour</em>: for each new data point the <span class=\"math notranslate nohighlight\">\\(k\\)</span> closest data points of the\ntraining sets are determined, and the output is then most common class among these\n<span class=\"math notranslate nohighlight\">\\(k\\)</span> data points. (Ties may be split randomly.)</p>", "a[href=\"#mathematics-of-machine-learning\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Mathematics of Machine Learning<a class=\"headerlink\" href=\"#mathematics-of-machine-learning\" title=\"Link to this heading\">#</a></h1><p>What is this? This is a trial run to see how I can best publish my lecture notes publicly. It\u2019s very much in an\nalpha stage, with lots of errors, incongruities and omissions.</p>", "a[href=\"contents/rl.html#pole-balancing\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.2. </span>Pole balancing<a class=\"headerlink\" href=\"#pole-balancing\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-3\"></label><input class=\"margin-toggle\" id=\"marginnote-role-3\" name=\"marginnote-role-3\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/reinforcement_learning/pole.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>pole</a></span>\nLet\u2019s have a look at a classic reinforcement learning problem. The task consists in balancing a pole on a cart.\nWhen the pole starts falling over, the cart can move to counterbalance the movement of the pole. The aim\nis to keep the pole from falling as long as possible. See <a class=\"reference internal\" href=\"#polefig\"><span class=\"std std-numref\">Fig. 7.3</span></a>.</p>", "a[href=\"contents/rl.html#policy-gradient-method\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.10. </span>Policy gradient method<a class=\"headerlink\" href=\"#policy-gradient-method\" title=\"Link to this heading\">#</a></h2><p>Assume that the agent is positioned in state <span class=\"math notranslate nohighlight\">\\(s\\)</span>, and assume that some\nstochastic parameterised policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span> is given.<label class=\"margin-toggle\" for=\"sidenote-role-5\"><span id=\"id5\">\n<sup>5</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-5\" name=\"sidenote-role-5\" type=\"checkbox\"/><span class=\"sidenote\"><sup>5</sup>This section is largely based on material of <a class=\"reference external\" href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\">OpenAI</a></span>\nHere the subscript <span class=\"math notranslate nohighlight\">\\(w\\)</span> indicates the set of weights\nthat describes the policy. The return we can expect with policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span> in state <span class=\"math notranslate nohighlight\">\\(s\\)</span> is then</p>", "a[href=\"contents/pac.html#test-error-and-generalisation-error\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">2.2. </span>Test error and generalisation error<a class=\"headerlink\" href=\"#test-error-and-generalisation-error\" title=\"Link to this heading\">#</a></h2><p>We have introduced the test set as a stand-in for new data. Let\u2019s see\nwhat the test error can tell us about the generalisation error.</p><p>We need a <em>measure concentration</em> inequality, an inequality that\nasserts that the mean of certain random variables is, with high probability,\nvery close to the expected value \u2013 provided a number of mild conditions are satisfied.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-2\"></label><input class=\"margin-toggle\" id=\"marginnote-role-2\" name=\"marginnote-role-2\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/pac_learning/concentration.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>concentration</a></span></p>", "a[href=\"contents/convex.html#convexity\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.1. </span>Convexity<a class=\"headerlink\" href=\"#convexity\" title=\"Link to this heading\">#</a></h2><p>A set <span class=\"math notranslate nohighlight\">\\(C\\subseteq\\mathbb R^n\\)</span> is <em>convex</em> if the connecting segment\nbetween any two points in <span class=\"math notranslate nohighlight\">\\(C\\)</span> is also contained in <span class=\"math notranslate nohighlight\">\\(C\\)</span>:</p>", "a[href=\"contents/pac.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">2. </span>PAC learning<a class=\"headerlink\" href=\"#pac-learning\" title=\"Link to this heading\">#</a></h1><p>Let\u2019s imagine we urgently need a good classifier that distinguishes cat pictures from dog pictures.<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup>Much of the material is based on <em>Understanding Machine Learning</em> by Shai Shalev-Shwartz and Shai Ben-David</span>\nHow do we go about this? We perhaps decide that a neural network will be suitable and we\nprepare a training set: we collect cat and dog pictures and we label each picture as either\na cat picture or a dog picture. Once we have done that we run an optimisation algorithm\nto adapt the weights of the neural network so that the training error, the\nmisclassification rate on the training set, is as small as possible. (We will discuss\nsuch optimisation algorithms in a later chapter.) What concerns us here is:\nWhy can we expect a neural network with small training error to perform well on new data?</p><p>Recall: We do not care about the training set. We already know which picture in the training\nset is a cat and which is a dog. What we care about is performance on new data, ie,\nwe strive for a small generalisation error.\nDoes a small training error <em>guarantee</em> a small generalisation error?\nIn short: No.</p>", "a[href=\"contents/rl.html#returns\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.4. </span>Returns<a class=\"headerlink\" href=\"#returns\" title=\"Link to this heading\">#</a></h2><p>Once the agent in a reinforcement learning task starts it follows a\ntrajectory of states, actions, and rewards:</p>", "a[href=\"contents/nets.html#softmax-and-loss\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.3. </span>Softmax and loss<a class=\"headerlink\" href=\"#softmax-and-loss\" title=\"Link to this heading\">#</a></h2><p>The preceding discussion applied to binary classification and a single output\nnode. What do we do when we have more classes? First, we\u2019ll have more outputs, namely\none for each class. Second, we\u2019ll use a softmax layer at the output.</p><p>Assume we do image classification\nwith three classes: cat, pumpkin and bird. Then there will be three output neurons.\nIf a training sample <span class=\"math notranslate nohighlight\">\\((x,y)\\)</span> shows a cat, then the class vector <span class=\"math notranslate nohighlight\">\\(y\\)</span> will be <span class=\"math notranslate nohighlight\">\\(y=(1,0,0\\trsp )\\)</span>.\nIf the image shows a pumpkin, we\u2019ll have <span class=\"math notranslate nohighlight\">\\(y=(0,1,0\\trsp )\\)</span>, and if it\u2019s a bird then\n<span class=\"math notranslate nohighlight\">\\(y=(0,0,1\\trsp )\\)</span>. This way of encoding the classes even has a fancy name: <em>one-hot encoding</em>,\nbecause there is always just one bit that is <em>hot</em>, ie, equal to 1.</p>", "a[href=\"contents/nets.html#kullback-leibler-divergence\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.5. </span>Kullback-Leibler divergence<a class=\"headerlink\" href=\"#kullback-leibler-divergence\" title=\"Link to this heading\">#</a></h2><p>The <em>Kullback-Leibler divergence</em> measures how different two probability distributions are.\nFor the definition let us consider two discrete probability distributions <span class=\"math notranslate nohighlight\">\\(p\\)</span> and <span class=\"math notranslate nohighlight\">\\(q\\)</span>\n(the general case is similar but involves integrals and measure-theoretic caveats). Then\nthe  Kullback-Leibler divergence between <span class=\"math notranslate nohighlight\">\\(p\\)</span> and <span class=\"math notranslate nohighlight\">\\(q\\)</span> is defined as</p>", "a[href=\"contents/rl.html#q-learning\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.8. </span><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning<a class=\"headerlink\" href=\"#q-learning\" title=\"Link to this heading\">#</a></h2><p>How can we learn an optimal policy? Above we sketched a procedure: Start with some policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>,\ncompute its <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values, check whether there is a\nstate <span class=\"math notranslate nohighlight\">\\(s\\)</span> and an action <span class=\"math notranslate nohighlight\">\\(a\\)</span> with <span class=\"math notranslate nohighlight\">\\(q_{\\pi}(s,{\\pi}(s))&lt;q_{\\pi}(s,a)\\)</span>,\nchange the policy <span class=\"math notranslate nohighlight\">\\(\\pi(s)=a\\)</span> and repeat. The procedure is very slow and suffers from a serious disadvantage:\nwe don\u2019t know how to compute the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.</p><p>Fortunately, there is a method, <em><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning</em>, that bypasses the policy improvement\nsteps and directly learns the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values <span class=\"math notranslate nohighlight\">\\(q^*\\)</span> of an optimal deterministic policy <span class=\"math notranslate nohighlight\">\\(\\pi^*\\)</span> \u2013 provided\nthe MDP is finite.\nHow then can we recover the policy from the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values?\nIt follows directly from <a class=\"reference internal\" href=\"#qoptthm\">Theorem 7.4</a> that</p>", "a[href=\"contents/mips.html#vector-quantisation\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.1. </span>Vector quantisation<a class=\"headerlink\" href=\"#vector-quantisation\" title=\"Link to this heading\">#</a></h2><p>Let <span class=\"math notranslate nohighlight\">\\(x^{(1)},\\ldots x^{(n)}\\in\\mathbb R^d\\)</span> be the vectors  that make up the database,\nlet <span class=\"math notranslate nohighlight\">\\(k,m&gt;0\\)</span> be integers, and let <span class=\"math notranslate nohighlight\">\\(\\ell=\\tfrac{d}{m}\\)</span>, which we assume to be an integer.<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup><em>Quantization based Fast Inner Product Search</em>, R. Guo, S. Kumar, K. Choromanski and D. Simcha (2015), <a class=\"reference external\" href=\"https://arxiv.org/abs/1509.01469\">arXiv:1509.01469</a></span></p><p>We split each vector <span class=\"math notranslate nohighlight\">\\(x\\in\\mathbb R^d\\)</span> into <span class=\"math notranslate nohighlight\">\\(m\\)</span> vectors each of length <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span>:</p>", "a[href=\"contents/intro.html#bayes-error\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.13. </span>Bayes error<a class=\"headerlink\" href=\"#bayes-error\" title=\"Link to this heading\">#</a></h2><p>Because\nof the inherent uncertainty in the distribution, the smallest generalisation error that can be achieved\nmight be larger than zero.</p><p>Let\u2019s assume, for example, that we want to predict the sex of a person based only on the height of the\nperson. A person of height 1.8m is likely male \u2013 but not necessarily so. Obviously, there are also\nwomen of 1.8m, but they are fewer than men of 1.8m. Height does not determine sex, and thus any\nclassifier based only on height will never be perfect. This residual error that any classifier\nis bound to make is called <em>Bayes error</em>. It is defined as</p>", "a[href=\"contents/netsprops.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5. </span>Properties of neural networks<a class=\"headerlink\" href=\"#properties-of-neural-networks\" title=\"Link to this heading\">#</a></h1><h2><span class=\"section-number\">5.1. </span>ReLU networks and piecewise affine functions<a class=\"headerlink\" href=\"#relu-networks-and-piecewise-affine-functions\" title=\"Link to this heading\">#</a></h2><p>We\u2019ve trained and studied neural networks \u2013 but what kind of functions do they actually encode?\nIt turns out, ReLU networks encode quite simple functions, namely piecewise affine functions.</p><p>A function <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R^m\\)</span> is <em>piecewise affine</em> if there\nare finitely many polyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\subseteq\\mathbb R^n\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n=\\bigcup_{i=1}^s\\ph Q_i\\)</span> and <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span>\nis an affine function for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots, s\\)</span>. The\npolyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_i\\)</span> are the <em>pieces</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.\nThe smallest number of pieces <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span> is affine for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots,s\\)</span>\nis the <em>piece-number</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.</p>", "a[href=\"contents/netsprops.html#relu-networks-and-piecewise-affine-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.1. </span>ReLU networks and piecewise affine functions<a class=\"headerlink\" href=\"#relu-networks-and-piecewise-affine-functions\" title=\"Link to this heading\">#</a></h2><p>We\u2019ve trained and studied neural networks \u2013 but what kind of functions do they actually encode?\nIt turns out, ReLU networks encode quite simple functions, namely piecewise affine functions.</p><p>A function <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R^m\\)</span> is <em>piecewise affine</em> if there\nare finitely many polyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\subseteq\\mathbb R^n\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n=\\bigcup_{i=1}^s\\ph Q_i\\)</span> and <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span>\nis an affine function for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots, s\\)</span>. The\npolyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_i\\)</span> are the <em>pieces</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.\nThe smallest number of pieces <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span> is affine for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots,s\\)</span>\nis the <em>piece-number</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.</p>", "a[href=\"contents/nets.html#no-traps\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.7. </span>No traps<a class=\"headerlink\" href=\"#no-traps\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s try again to argue that local minima are unlikely to trap\nSGD when training a neural network.\nIn the setting we will consider, there are more weights, more parameters of the\nneural network, than points in the training set.\nThis is not unusual. Alexnet, a well-known\nneural network for image recognition had about 60 million parameters but was trained\non just 1.2 million datapoints.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-8\"></label><input class=\"margin-toggle\" id=\"marginnote-role-8\" name=\"marginnote-role-8\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/no_traps.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>no_traps</a></span></p>", "a[href=\"contents/rl.html#what-else-is-there\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.14. </span>What else is there?<a class=\"headerlink\" href=\"#what-else-is-there\" title=\"Link to this heading\">#</a></h2><p>The methods treated here will not be enough to train a world-class\nchess engine or a fully autonomous robot.\nThe spectacular advances in reinforcement learning that we have seen\nin recent years become only possible if the basic ideas presented here\nare combined with the power of deep neural networks.</p><p>In <em>deep <span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning</em>, for instance, a deep neural network learns to\napproximate a <span class=\"math notranslate nohighlight\">\\(q\\)</span>-function.<label class=\"margin-toggle\" for=\"sidenote-role-9\"><span id=\"id9\">\n<sup>9</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-9\" name=\"sidenote-role-9\" type=\"checkbox\"/><span class=\"sidenote\"><sup>9</sup><em>Playing Atari with Deep Reinforcement Learning</em>,\nV. Mnih, K. Kavukcuoglu, D. Silver, D. Wierstra, A. Graves\nand I. Antonoglou (2013), <a class=\"reference external\" href=\"https://arxiv.org/abs/1312.5602\">arXiv:1312.5602</a></span>\n<em>Actor-critic</em> algorithms extend this: two neural networks\nare trained. One, the critic, learns to predict <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values by observing\nthe other neural network, the actor; while the actor constantly tries\nto improve a policy by exploiting the approximated <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values of the critic.</p>", "a[href=\"contents/intro.html#training-and-errors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.5. </span>Training and errors<a class=\"headerlink\" href=\"#training-and-errors\" title=\"Link to this heading\">#</a></h2><p>So far, we have tried to minimise the training error. With the zero-one-loss, for instance,\nwe tried to minimise the misclassification rate on the training set when fitting a logistic regression.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-10\"></label><input class=\"margin-toggle\" id=\"marginnote-role-10\" name=\"marginnote-role-10\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/errors.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>errors</a></span></p>", "a[href=\"contents/convex.html#analysis-of-sgd\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.7. </span>Analysis of SGD<a class=\"headerlink\" href=\"#analysis-of-sgd\" title=\"Link to this heading\">#</a></h2><p>If the loss function is convex then SGD converges, at least in expectation,\ntowards the global minimum, provided some additional mild conditions are satisfied.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id4\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>Based on <em>The convergence of the Stochastic Gradient Descent (SGD) : a self-contained proof</em>\nby G. Turinci, <a class=\"reference external\" href=\"https://arxiv.org/pdf/2103.14350.pdf\">arXiv:2103.14350</a></span>\nThere are a number of such convergence proofs, each with their own set of additional\nconditions. We assume here strong convexity.</p><p>Let <span class=\"math notranslate nohighlight\">\\(S\\)</span> be a training set, and let</p>", "a[href=\"contents/loss.html#trade-off-between-true-positive-and-false-positive-rate\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.5. </span>Trade-off between true positive and false positive rate<a class=\"headerlink\" href=\"#trade-off-between-true-positive-and-false-positive-rate\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-4\"></label><input class=\"margin-toggle\" id=\"marginnote-role-4\" name=\"marginnote-role-4\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/losses/lendingclub.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>lendingclub</a></span>\nFor binary classification, we can express a different importance of the classes as\ndiffering preferences for true positive and negative rate. That is, we may specify\na target true negative rate of 90% and then optimise the true positive rate as much as possible.</p>", "a[href=\"contents/intro.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1. </span>Predictors, classification and losses<a class=\"headerlink\" href=\"#predictors-classification-and-losses\" title=\"Link to this heading\">#</a></h1><p>Does a given picture show a dog or a cat?\nShould the applicant be granted a credit? Is the mail\nspam or ham? These are all examples of <em>classification</em> tasks, one of the\nprincipal domains in machine learning.</p><p>A well-known classification tasks involves the MNIST data set.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-1\"></label><input class=\"margin-toggle\" id=\"marginnote-role-1\" name=\"marginnote-role-1\" type=\"checkbox\"/><span class=\"marginnote\"> MNIST is so well known that it has a <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/MNIST_database\">wikipedia</a> entry.</span>\nThe MNIST data set contains 70000 handwritten digits as images of 28<span class=\"math notranslate nohighlight\">\\(\\times\\)</span> 28 pixels. The task is\nto decide whether a given image <span class=\"math notranslate nohighlight\">\\(x\\)</span> shows a 0, 1, or perhaps a 7. In machine learning\nthis tasks is solved by letting an algorithm <em>learn</em> how to accomplish the tasks by\ngiving it access to a large number of examples, the <em>training set</em>,\ntogether with the true classification.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-2\"></label><input class=\"margin-toggle\" id=\"marginnote-role-2\" name=\"marginnote-role-2\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/MNIST.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>MNIST</a><br/>\n(everywhere you see this icon, there\u2019s a link that takes you directly to colab)</span>\nIn the MNIST tasks that means that the algorithm not only\nreceives perhaps 60000 images, each containing a handwritten digit, but also for each image\nthe information which <em>class</em> it is, ie, which of the digits 0,1,\u2026,9 is shown; see <a class=\"reference internal\" href=\"#mnistfig\"><span class=\"std std-numref\">Fig. 1.1</span></a>.</p>", "a[href=\"contents/convex.html#gradient-descent\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.5. </span>Gradient descent<a class=\"headerlink\" href=\"#gradient-descent\" title=\"Link to this heading\">#</a></h2><p>Some of the objective functions in machine learning are convex.\nHow can we minimise them? With <em>stochastic gradient descent</em> \u2013 it is this\nalgorithm (or one of its variants) that powers most of machine learning. Let\u2019s understand\nsimple <em>gradient descent</em> first.</p>", "a[href=\"contents/netsprops.html#the-saw-tooth-function\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.3. </span>The saw tooth function<a class=\"headerlink\" href=\"#the-saw-tooth-function\" title=\"Link to this heading\">#</a></h2><p>Deeper neural networks are more powerful than shallow networks with the same number\nof parameters. An easy example of this is the <em>saw tooth function</em>: it can be\ncomputed by a deep network with only a few neurons; any shallow network, however,\nwill need a very large number of neurons.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id3\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>I am following here <a class=\"reference external\" href=\"https://mjt.cs.illinois.edu/dlt/\">lecture notes</a> of Telgarsky, who also proved\nthe main result in this section.</span></p><p>What is the saw tooth function? It is the iteration of the simple function</p>", "a[href=\"contents/loss.html#loss-functions-in-classification\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.1. </span>Loss functions in classification<a class=\"headerlink\" href=\"#loss-functions-in-classification\" title=\"Link to this heading\">#</a></h2><p>A key metric of interest in classification is the misclassification rate, ie,\nzero-one loss. Directly minimising zero-one loss, however, is usually\ncomputationally infeasible. Instead, we minimise <em>surrogate</em> losses, losses\nthat are better behaved. What are common losses?</p><p>In a <em>binary classification</em> problem, with classes -1 and 1, we often compute\na classifier of the type <span class=\"math notranslate nohighlight\">\\(h:x\\in\\mathcal X\\mapsto \\sgn\\circ f(x)\\)</span>, where\n<span class=\"math notranslate nohighlight\">\\(f:\\mathcal X\\to\\mathbb R\\)</span> is some function. That is, if <span class=\"math notranslate nohighlight\">\\(f(x)&lt;0\\)</span> then\nwe classify <span class=\"math notranslate nohighlight\">\\(x\\)</span> as class -1, and if <span class=\"math notranslate nohighlight\">\\(f(x)\\geq 0\\)</span> then <span class=\"math notranslate nohighlight\">\\(x\\)</span> is predicted\nto have class 1. In a neural network, or in logistic regression,\n<span class=\"math notranslate nohighlight\">\\(f\\)</span> is passed further through the logistic function, so that the classifier\nultimately computes a probability estimate in <span class=\"math notranslate nohighlight\">\\([0,1]\\)</span>, how likely it is that the sample is class 1.</p>", "a[href=\"contents/intro.html#linear-predictors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.3. </span>Linear predictors<a class=\"headerlink\" href=\"#linear-predictors\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s look at possible the simplest sort of classification algorithm, a <em>linear classifier</em>.</p><p>We consider a binary classification task with domain set <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\subseteq \\mathbb R^d\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\mathcal Y=\\{-1,1\\}\\)</span>.\nA very simple classifier tries to separate the data points into different halfspaces. That is, we look for a halfspace\n<span class=\"math notranslate nohighlight\">\\(H=\\{x\\in\\mathbb R^d: \\trsp wx\\geq 0\\}\\)</span> and then classify all points in <span class=\"math notranslate nohighlight\">\\(H\\)</span> as <span class=\"math notranslate nohighlight\">\\(+1\\)</span>, say, and all points\nin <span class=\"math notranslate nohighlight\">\\(\\mathbb R^d\\setminus H\\)</span> as <span class=\"math notranslate nohighlight\">\\(-1\\)</span>. More generally, we could use an affine hyperplane defined by <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^d\\)</span>\nand <span class=\"math notranslate nohighlight\">\\(b\\in\\mathbb R\\)</span> and then classify as follows</p>", "a[href=\"contents/rl.html#reinforcement-learning-and-llms\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.13. </span>Reinforcement learning and LLMs<a class=\"headerlink\" href=\"#reinforcement-learning-and-llms\" title=\"Link to this heading\">#</a></h2><p>After training on a large text corpus,\nan AI chatbot such as ChatGPT, Gemini or Claude needs to be finetuned\nto ensure that it produces helpful, harmless und honest output.\nThis is a challenge because it is hard to come by high quality training data:\nIn contrast to pre-training the language model, where basically a good part of the internet is ingested,\ngood and helpful answers to prompts would need to be written by humans. This is burdensome, costly and\ntime consuming. As a result any such dataset of prompts and model answers will be\nsmall.</p><p>Because of the dearth of good training datasets, current AI chatbots\nare finetuned in a different way. Later versions of ChatGPT, for example, are finetuned\nwith <em>reinforcement learning from human feedback</em>,<label class=\"margin-toggle\" for=\"sidenote-role-8\"><span id=\"id8\">\n<sup>8</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-8\" name=\"sidenote-role-8\" type=\"checkbox\"/><span class=\"sidenote\"><sup>8</sup><em>Training language models to follow instructions\nwith human feedback</em>, Ouyang et al. (2022), <a class=\"reference external\" href=\"https://arxiv.org/abs/2203.02155\">arXiv:2203.02155</a></span>\nwhich we\u2019ll briefly describe here.</p>", "a[href=\"contents/mips.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8. </span>Maximum inner product search<a class=\"headerlink\" href=\"#maximum-inner-product-search\" title=\"Link to this heading\">#</a></h1><p><em>Vector databases</em> are becoming more and more important.\nWhat\u2019s a vector database? A system to store a large number of vectors\n<span class=\"math notranslate nohighlight\">\\(x^{(1)},\\ldots, x^{(n)}\\in\\mathbb R^d\\)</span>\nin such a way that a (approximate) nearest neighbour search can be performed efficiently.<br/>\nA recommender system, for instance, might\nstore the preferences of the users encoded as vectors; for a new user the five most similar\nknown users could be computed in order to recommend the products or services they prefered.\nAnother application comes from word or document embeddings: A number of vector representation\nof documents are stored in the database; a user may then formulate a query (\u201cwhich Tom Stoppard play\nfeatures Hamlet as a side character?\u201d) that is transformed into a vector; the documents with\nmost similar vector representation are then returned.</p><p>What <em>most similar</em> means will differ from application to application. Often it may\nsimply mean: the largest scalar product. That is, given a query <span class=\"math notranslate nohighlight\">\\(q\\in\\mathbb R^d\\)</span> we look for the <span class=\"math notranslate nohighlight\">\\(x^{(i)}\\)</span>\nwith largest <span class=\"math notranslate nohighlight\">\\(\\trsp{q}x^{(i)}\\)</span>. In that case, the problem is known as <em>maximum inner product search</em> (or MIPS).</p>", "a[href=\"contents/nets.html#back-propagation\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.1. </span>Back propagation<a class=\"headerlink\" href=\"#back-propagation\" title=\"Link to this heading\">#</a></h2><p>To train the neural network, we need to specify a loss function. Historically this was often the\n<em>square loss</em>. That is, given a training set <span class=\"math notranslate nohighlight\">\\(S\\subseteq \\mathcal X\\times\\mathcal Y\\)</span>\nthe empirical risk was taken to be</p>", "a[href=\"contents/rl.html#policy-improvement-theorem\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.7. </span>Policy improvement theorem<a class=\"headerlink\" href=\"#policy-improvement-theorem\" title=\"Link to this heading\">#</a></h2><p>Assume an agent is following a policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span> but is given the opportunity to change a single action in a state <span class=\"math notranslate nohighlight\">\\(s\\)</span>.\nWhich one should she choose? To answer this question, we introduce <em>state-action values</em>,\nor shorter <em><span class=\"math notranslate nohighlight\">\\(q\\)</span>-values</em>. Given a state <span class=\"math notranslate nohighlight\">\\(s\\)</span> and an action <span class=\"math notranslate nohighlight\">\\(a\\)</span>, the value <span class=\"math notranslate nohighlight\">\\(q_\\pi(s,a)\\)</span>\ngives the expected discounted return that is obtained by\nchoosing action <span class=\"math notranslate nohighlight\">\\(a\\)</span> in state <span class=\"math notranslate nohighlight\">\\(s\\)</span> and then following policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>:</p>", "a[href=\"contents/intro.html#logistic-regression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.4. </span>Logistic regression<a class=\"headerlink\" href=\"#logistic-regression\" title=\"Link to this heading\">#</a></h2><p>How can we train a linear predictor? Ideally, we would minimise the training error directly.\nLet\u2019s assume\na homogeneous training set, which means that it suffices to\nlearn a linear classifer of the form <span class=\"math notranslate nohighlight\">\\(h_w\\)</span>, ie, one without a bias term.\nAnd let\u2019s assume that the loss function is zero-one loss.\nThen, minimising the training error means solving the following optimisation problem:<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-7\"></label><input class=\"margin-toggle\" id=\"marginnote-role-7\" name=\"marginnote-role-7\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/logreg.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>logreg</a></span></p>", "a[href=\"contents/test.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9. </span>TEST<a class=\"headerlink\" href=\"#test\" title=\"Link to this heading\">#</a></h1><p>aha this is <span class=\"math notranslate nohighlight\">\\(\\dobs\\)</span>. or something Blubb</p>", "a[href=\"contents/intro.html#regression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.11. </span>Regression<a class=\"headerlink\" href=\"#regression\" title=\"Link to this heading\">#</a></h2><p>Regression is a more general problem than classification.\nIn both cases, we aim to find a predictor <span class=\"math notranslate nohighlight\">\\(h:\\mathcal X\\to\\mathcal Y\\)</span>. In classification, the target set\n<span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span> is finite: we are interested in knowing whether the email is spam or not, or\nwhether the image shows a cat, a dog or a hamster. In regression, in contrast, <span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span> is usually\ncontinuous, and normally equal to an interval <span class=\"math notranslate nohighlight\">\\([a,b]\\)</span>, or perhaps to a multidimensional analog\nsuch as <span class=\"math notranslate nohighlight\">\\([a,b]^n\\)</span>.\nIn principle, regression can be seen as a <em>function approximation</em> task. There is some unknown function <span class=\"math notranslate nohighlight\">\\(f\\)</span>\nthat we want to approximate with our predictor <span class=\"math notranslate nohighlight\">\\(h:\\mathcal X\\to\\mathcal Y\\)</span>.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-17\"></label><input class=\"margin-toggle\" id=\"marginnote-role-17\" name=\"marginnote-role-17\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/california.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>regression</a></span></p><p>The theory for classification, however, is cleaner than for regression, and this is the main reason\nwhy I will focus on classification tasks.\nArguably, regression is more powerful.\nI am probably more interested in how large the expected return on investment\nfor some stock portfolio is (a regression task) than whether there is a positive or negative ROI\n(a classification task). Let\u2019s do a quick digression on regression.</p>", "a[href=\"contents/nets.html#the-loss-function\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.2. </span>The loss function<a class=\"headerlink\" href=\"#the-loss-function\" title=\"Link to this heading\">#</a></h2><p>Historically neural networks were trained with the square loss. Indeed, in a lot of applications\nwe are interested in the mean square error, so why not use it for neural networks, too?\nIt turns out that square loss might lead to slow learning \u2013 in classification.</p><p>Recall that the square loss is defined as</p>", "a[href=\"contents/intro.html#neural-networks\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.9. </span>Neural networks<a class=\"headerlink\" href=\"#neural-networks\" title=\"Link to this heading\">#</a></h2><p>We\u2019ve already talked about a number of classifying algorithms. I omitted, however, the most important one of\nthem all, the neural network. Let\u2019s remedy this omission.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-16\"></label><input class=\"margin-toggle\" id=\"marginnote-role-16\" name=\"marginnote-role-16\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/tfintro.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>tfintro</a></span></p><p>Neural networks are old: Their origins go back to the 1940s. Modelled after (a crude simplification of) neurons\nin the brain they consists of single units, the neurons, that collect the inputs of other units (or of the input)\nand then compute an output value that might be fed into other neurons.</p>", "a[href=\"contents/rl.html#baselines\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.11. </span>Baselines<a class=\"headerlink\" href=\"#baselines\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-6\"></label><input class=\"margin-toggle\" id=\"marginnote-role-6\" name=\"marginnote-role-6\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/sreinforcement_learning/policy_grad.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>policy grad</a></span>\nHow can we improve the policy gradient method? Here is what seems nonsensical about the\nmethod: No matter how good the trajectory <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is, we try to improve the likelihood\nof the trajectory by pushing the weights in the direction of it. The return <span class=\"math notranslate nohighlight\">\\(G(\\tau)\\)</span> of the\ntrajectory only influences how hard we push. If <span class=\"math notranslate nohighlight\">\\(G(\\tau)\\)</span> is large, the change <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> will\nbe large, if <span class=\"math notranslate nohighlight\">\\(H(\\tau)\\)</span> is small, <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> will be smaller \u2013 however, we always push to reinforce <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span>.</p><p>What would seem more promising: If <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is a trajectory that is exceptionally good, then we should\nmake it more likely, but if <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is worse than average, we should make it less likely, ie, push in\nthe opposite direction. That is, if <span class=\"math notranslate nohighlight\">\\(b\\)</span> is the average return of a trajectory then the sign of <span class=\"math notranslate nohighlight\">\\(G(\\tau)-b\\)</span>\ntells us whether <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is better or worse than an average trajectory.\n(How can we compute <span class=\"math notranslate nohighlight\">\\(b\\)</span>? Easy: We sample a number of trajectories and take the average of the returns.)</p>", "a[href=\"contents/intro.html#tasks-in-ml\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.1. </span>Tasks in ML<a class=\"headerlink\" href=\"#tasks-in-ml\" title=\"Link to this heading\">#</a></h2><p>Some common tasks in machine learning are:</p>", "a[href=\"contents/rl.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7. </span>Reinforcement learning<a class=\"headerlink\" href=\"#reinforcement-learning\" title=\"Link to this heading\">#</a></h1><p>What is reinforcement learning?<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup>The material in this chapter is based on <em>Reinforcement Learning</em>, R.S. Sutton and A.G. Barto, MIT Press (2018)\nand <em>Spinning Up in Deep RL</em>, J. Achiam, <a class=\"reference external\" href=\"https://spinningup.openai.com/en/latest/index.html\">link</a></span>\nIn reinforcement learning an autonomous agent interacts with a possibly unknown\nenvironment. Each action the agent takes results in a state change and a reward or penalty.\nThe agent strives to maximise the total reward.</p>", "a[href=\"contents/rl.html#value-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.6. </span>Value functions<a class=\"headerlink\" href=\"#value-functions\" title=\"Link to this heading\">#</a></h2><p>In many reinforcement learning tasks, a reward will only be awarded after many steps. A chess game has no immediate rewards;\nit is only won or lost at the end. In most tasks actions may have consequences that only become apparent many steps later.\nIn an inventory control problem, it may  be beneficial in the short run to not order anything, as no costs for purchases\nor storage are incurred; in the long run, however, we will pay dearly when we cannot satisfy customer demands.</p><p>Immediate rewards, therefore, are not a good basis for the next action. It\u2019s more important to estimate the long term\nconsequences of actions. This is where the <em>value function</em> comes in: It estimates the long term returns we can reap\nin a given state.\nHow we value a state depends on the setting of the task, whether we optimise total returns or discounted returns.\nHowever, as discounted returns default to total returns if the discounting factor <span class=\"math notranslate nohighlight\">\\(\\gamma\\)</span> is set to 1, we\ncan treat both settings in the same way.\nGiven a  policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>, we define for every state <span class=\"math notranslate nohighlight\">\\(s_0\\)</span> the value function as</p>", "a[href=\"contents/intro.html#decision-trees\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.8. </span>Decision trees<a class=\"headerlink\" href=\"#decision-trees\" title=\"Link to this heading\">#</a></h2><p>Let <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\subseteq \\mathbb R^d\\)</span>, and let <span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span> be a (finite) set of classes.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-13\"></label><input class=\"margin-toggle\" id=\"marginnote-role-13\" name=\"marginnote-role-13\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/dectree.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>dectree</a></span>\nA \\defi{decision tree} <span class=\"math notranslate nohighlight\">\\(T\\)</span> for <span class=\"math notranslate nohighlight\">\\(\\mathcal X,\\mathcal Y\\)</span> consists of a rooted tree, where each\nnon-leaf is associated with a decision rule. That is, <span class=\"math notranslate nohighlight\">\\(T\\)</span> has a root <span class=\"math notranslate nohighlight\">\\(r\\)</span>, every vertex <span class=\"math notranslate nohighlight\">\\(v\\)</span> is either\na \\ndefi{leaf} or not; if <span class=\"math notranslate nohighlight\">\\(v\\)</span> is not a leaf then it has precisely two children <span class=\"math notranslate nohighlight\">\\(v_L,v_R\\)</span>; every\nleaf <span class=\"math notranslate nohighlight\">\\(v\\)</span> is labelled with a class <span class=\"math notranslate nohighlight\">\\(c(v)\\in\\mathcal Y\\)</span>; every non-leaf <span class=\"math notranslate nohighlight\">\\(v\\)</span> has a decision rule,\na tuple <span class=\"math notranslate nohighlight\">\\((i,t)\\)</span>, where <span class=\"math notranslate nohighlight\">\\(i\\in[d]\\)</span> is a feature and <span class=\"math notranslate nohighlight\">\\(t\\in\\mathbb R\\)</span>\nis a threshold. A decision tree defines a classifier in the following sense:</p>", "a[href=\"contents/loss.html#common-loss-functions-in-regression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.3. </span>Common loss functions in regression<a class=\"headerlink\" href=\"#common-loss-functions-in-regression\" title=\"Link to this heading\">#</a></h2><p>In classification we ultimately aim to minimise the expected number of misclassified samples.\nBecause it is not computationally feasible to\ndirectly minimise zero-one loss, we use surrogate loss functions.</p><p>In regression, we normally do not need to deal with computationally infeasible loss functions.\nOften we minimise directly the metric that we are interested in. In contrast, however,\nit is not obvious anymore what the key metric is. Depending on the data and on the task at\nhand, different metrics may be appropriate to measure the quality of the regressor.</p>", "a[href=\"contents/nets.html#why-cross-entropy-loss\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.4. </span>Why cross-entropy loss?<a class=\"headerlink\" href=\"#why-cross-entropy-loss\" title=\"Link to this heading\">#</a></h2><p>Cross-entropy loss seems to make sense and certainly increases the more bad predictions are made \u2013 still\nit may appear a bit arbitrary. It\u2019s not. We take a step back and figure out where cross-entropy loss is coming from.</p><p><em>Cross-entropy</em> and entropy more generally is a concept from information theory. Cross-entropy, in information\ntheory, compares two probability distributions <span class=\"math notranslate nohighlight\">\\(p,q\\)</span>:</p>", "a[href=\"contents/netsprops.html#universal-approximators\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.2. </span>Universal approximators<a class=\"headerlink\" href=\"#universal-approximators\" title=\"Link to this heading\">#</a></h2><p>How expressive are neural networks? We have already seen above that every Boolean function\ncan be realised as a ReLU network. What about more complicated functions? Since ReLU networks\nencode piecewise affine functions such a network cannot reproduce any function that is not\npiecewise affine \u2013 but can every piecewise affine function be realised? Yes!</p><p>Let\u2019s define the <em>depth of a neural network</em> as the number of its layers not counting the input\nlayer. For example, a shallow neural network with input layer, one hidden layer and output layer\nhas depth 2. Let\u2019s say that the neural network has <em>width</em> at most <span class=\"math notranslate nohighlight\">\\(d\\)</span> if no layer,\nexcept possibly for the input layer,\nhas more than <span class=\"math notranslate nohighlight\">\\(d\\)</span> neurons.<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup><em>Universal function approximation by deep neural nets with bounded width\nand ReLU activation</em>, B. Hanin (2017), <a class=\"reference external\" href=\"https://arxiv.org/abs/1708.02691\">arXiv:1708.02691</a></span></p>", "a[href=\"contents/loss.html#bayes-consistency-and-arbitrary-classification-losses\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.7. </span>Bayes consistency and arbitrary classification losses<a class=\"headerlink\" href=\"#bayes-consistency-and-arbitrary-classification-losses\" title=\"Link to this heading\">#</a></h2><p>Surrogate loss functions are computationally feasible loss function that can be minimised\ninstead of zero-one loss. If the loss function is Bayes-consistent, then the minimiser\nof the surrogate loss function will be a Bayes-classifier. How do we need to adapt\nthe surrogate losses if we aim for a different loss than zero-one loss?</p><p>Let <span class=\"math notranslate nohighlight\">\\(\\ell^*:\\mathcal Y\\times\\mathcal Y\\to\\mathbb R_+\\)</span> be a loss function.\nA (surrogate) loss function <span class=\"math notranslate nohighlight\">\\(\\ell:\\mathcal Y\\times\\mathcal Y\\to\\mathbb R_+\\)</span>\nis <em>Bayes-consistent for <span class=\"math notranslate nohighlight\">\\(\\ell^*\\)</span></em> if for every distribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span> on\n<span class=\"math notranslate nohighlight\">\\(\\mathcal X\\times\\mathcal Y\\)</span> and\nfor every sequence\n<span class=\"math notranslate nohighlight\">\\(h_1,h_2,\\ldots:\\mathcal X\\to\\mathcal Y\\)</span> of classifiers with</p>", "a[href=\"contents/rl.html#policies\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.5. </span>Policies<a class=\"headerlink\" href=\"#policies\" title=\"Link to this heading\">#</a></h2><p>It\u2019s time to turn to the agent. Once fully trained, how should the agent act? What the agent\nknows about the environment is encapsulated in the state. Based on that state\nthe agent has to decide which action to take. That is, an agent is described by a function\nfrom states to actions. Such a function</p>", "a[href=\"contents/rl.html#what-is-known-about-the-environment\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.3. </span>What is known about the environment?<a class=\"headerlink\" href=\"#what-is-known-about-the-environment\" title=\"Link to this heading\">#</a></h2><p>When we train a reinforcement learning system, what do we usually know about the environment?\nIn particular, do we have access to the transition probability and the reward function?\nSometimes yes, sometimes no. This depends on the scenario.</p><p>In toy problems we often know everything about the environment. What about a real-life scenario?\nWhen we try to train a control algorithm for a gas turbine, or a Go playing system,\nwe cannot directly train in the real environment: That would take too long (wait for 1000000 people\nto play against your at the beginning amusingly bad Go playing system), be too costly (pay the\n1000000 people), or result in catastrophic failure (gas turbine explodes).\nInstead, we may train\u2026</p>", "a[href=\"contents/nets.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4. </span>Neural networks<a class=\"headerlink\" href=\"#neural-networks\" title=\"Link to this heading\">#</a></h1><p>Let\u2019s train a neural network.<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup>A very good source for this \u2013 and other aspects of neural networks! \u2013 is Michael Nielsen\u2019s <a class=\"reference external\" href=\"http://neuralnetworksanddeeplearning.com\">website</a></span>\nNormally, SGD or one of its more advanced cousins is used\nto train neural networks. The main issue is how to compute the gradient as efficiently as\npossible. But let\u2019s not get ahead of ourselves.</p><p>The neural network we consider has <span class=\"math notranslate nohighlight\">\\(K\\)</span> layers, each layer <span class=\"math notranslate nohighlight\">\\(k\\)</span> consisting of <span class=\"math notranslate nohighlight\">\\(n_k\\)</span> nodes.\nThe last layer may either consist of a single node (for binary classification) or\nof several nodes. Normally, the activation function of any hidden layer\nis ReLU (<span class=\"math notranslate nohighlight\">\\(x\\mapsto \\max(0,x)\\)</span>) or leaky ReLU  <span class=\"math notranslate nohighlight\">\\((x\\mapsto \\max(\\alpha x,x)\\)</span> for <span class=\"math notranslate nohighlight\">\\(\\alpha\\in (0,1)\\)</span>),\nwhile the activation layer for the output layer is either the logistic function\n(for a single output node) or softmax (for several output nodes). We will simply\nwrite <span class=\"math notranslate nohighlight\">\\(\\sigma_k\\)</span> for the activation function of layer <span class=\"math notranslate nohighlight\">\\(k\\)</span>, and I will pretend\nthat <span class=\"math notranslate nohighlight\">\\(\\sigma_k\\)</span> is differentiable \u2013 even though this is clearly a lie. Let me discuss\nthis issue later.</p>", "a[href=\"contents/intro.html#loss-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.10. </span>Loss functions<a class=\"headerlink\" href=\"#loss-functions\" title=\"Link to this heading\">#</a></h2><p>So far, we have only considered zero-one loss.\nIn some situations, other loss functions will be needed. For instance, if we want to classify\nemails as <em>spam or ham</em>, that is, as spam emails or non-spam, then it is far more serious to\nmisclassify a ham email than a spam email: Indeed, if we miss an important email because it was put\nin the spam folder (or perhaps deleted) then we will be quite cross, the occasional Viagra ad in our\ninbox, however, will merely annoy us somewhat.\nA loss function then might look like this:</p>", "a[href=\"contents/loss.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6. </span>Loss functions<a class=\"headerlink\" href=\"#loss-functions\" title=\"Link to this heading\">#</a></h1><p>Loss functions play a major role in classification as well as in regression.\nThat role, however, is not the same. Let us first examine how loss functions\ndetermine the quality of a classifier. At the end of this section, we\nconsider different loss functions for regression.</p>", "a[href=\"contents/loss.html#class-weights-in-losses\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.6. </span>Class weights in losses<a class=\"headerlink\" href=\"#class-weights-in-losses\" title=\"Link to this heading\">#</a></h2><p>We return to, potentially, multi-class classification.\nOften, the key metric in classification is simply <em>zero-one loss</em>, ie</p>", "a[href=\"contents/pac.html#empirical-risk-minimisation\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">2.1. </span>Empirical risk minimisation<a class=\"headerlink\" href=\"#empirical-risk-minimisation\" title=\"Link to this heading\">#</a></h2><p>How do we train a classifier?\nGiven a domain set <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\)</span> and a set of classes <span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span>, and\na (hidden) distribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span> on <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\times\\mathcal Y\\)</span>, we\ndraw a training set <span class=\"math notranslate nohighlight\">\\(S\\)</span> of size <span class=\"math notranslate nohighlight\">\\(m\\)</span> from the distribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span>.\nWe write <span class=\"math notranslate nohighlight\">\\(S\\sim\\mathcal D^m\\)</span> to denote that we draw <span class=\"math notranslate nohighlight\">\\(m\\)</span> samples from <span class=\"math notranslate nohighlight\">\\(S\\)</span>\nindepedently of each other.  Observe that it might happen that we draw a given point twice or even more\noften.\nHow now should we choose a classifier <span class=\"math notranslate nohighlight\">\\(h:\\mathcal X\\to\\mathcal Y\\)</span>? It should have low training error.\nOften, in this context, the training error is also called \\defi{empirical risk}:</p>", "a[href=\"contents/netsprops.html#neural-networks-are-sometimes-overconfident\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.4. </span>Neural networks are sometimes overconfident<a class=\"headerlink\" href=\"#neural-networks-are-sometimes-overconfident\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-5\"></label><input class=\"margin-toggle\" id=\"marginnote-role-5\" name=\"marginnote-role-5\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/fool.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>fool</a></span>\nIn practice it is sometimes observed that neural networks confidently\nclassify some image data, with confidence levels approaching 100%,\neven though the input data is just white noise. That is, in MNIST for\nexample, it is possible to generate noise pictures that a neural network\nwill happily claim show a \u20187\u2019, and that with 99.8% certainty.</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
