selector_to_html = {"a[href=\"https://en.wikipedia.org/wiki/Low-rank_approximation\"]": "<p>In mathematics, <b>low-rank approximation</b> refers to the process of approximating a given matrix by a matrix of lower rank. More precisely, it is a minimization problem, in which the cost function measures the fit between a given matrix and an approximating matrix, subject to a constraint that the approximating matrix has reduced rank. The problem is used for mathematical modeling and data compression. The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure.</p>", "a[href^=\"https://en.wikipedia.org/wiki/Low-rank_approximation#\"]": "<p>In mathematics, <b>low-rank approximation</b> refers to the process of approximating a given matrix by a matrix of lower rank. More precisely, it is a minimization problem, in which the cost function measures the fit between a given matrix and an approximating matrix, subject to a constraint that the approximating matrix has reduced rank. The problem is used for mathematical modeling and data compression. The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure.</p>", "a[href=\"https://en.wikipedia.org/wiki/Normal_distribution\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/320px-Normal_Distribution_PDF.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In probability theory and statistics, a <b>normal distribution</b> or <b>Gaussian distribution</b> is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n<span class=\"mwe-math-element\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4ac6c71a4a3df62eeaf7e052e27ce356793102f5\" class=\"mwe-math-fallback-image-display mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.838ex;width:24.077ex;height:7.176ex\" /></span></p>", "a[href^=\"https://en.wikipedia.org/wiki/Normal_distribution#\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/320px-Normal_Distribution_PDF.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In probability theory and statistics, a <b>normal distribution</b> or <b>Gaussian distribution</b> is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n<span class=\"mwe-math-element\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4ac6c71a4a3df62eeaf7e052e27ce356793102f5\" class=\"mwe-math-fallback-image-display mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.838ex;width:24.077ex;height:7.176ex\" /></span></p>", "a[href=\"#equation-thetastar\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-thetastar\">\n\\[\\begin{split}\\begin{align}\n\\theta^* &amp; = \\argmax_{\\theta} \\prod_{i=1}^Np_\\theta(x^{(i)}) \\notag \\\\\n\\Leftrightarrow\\quad\\theta^* &amp; = \\argmax_\\theta \\sum_{i=1}^N\\log\\left(p_\\theta(x^{(i)})\\right) \n\\end{align}\\end{split}\\]</div>", "a[href=\"#reconstruction-loss\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.4. </span>Reconstruction loss<a class=\"headerlink\" href=\"#reconstruction-loss\" title=\"Link to this heading\">#</a></h2><p>To employ stochastic gradient descent on the reconstruction loss we need to compute\ngradients <span class=\"math notranslate nohighlight\">\\(\\nabla_\\theta\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_\\phi\\)</span> with respect to the parameters <span class=\"math notranslate nohighlight\">\\(\\theta,\\phi\\)</span>.\nThe gradient with respect to <span class=\"math notranslate nohighlight\">\\(\\theta\\)</span> is relatively uncomplicated. In particular, the expectation and <span class=\"math notranslate nohighlight\">\\(\\nabla_\\theta\\)</span>\ncommute as the stochastic process <span class=\"math notranslate nohighlight\">\\(z\\sim q_\\phi(z|x) \\)</span> is independent of <span class=\"math notranslate nohighlight\">\\(\\theta\\)</span>:</p>", "a[href=\"#principle-component-analysis\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.1. </span>Principle component analysis<a class=\"headerlink\" href=\"#principle-component-analysis\" title=\"Link to this heading\">#</a></h2><p>Principle component analysis is a common method to project some data\nfrom a high-dimensional space <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n\\)</span> to a low-dimensional space <span class=\"math notranslate nohighlight\">\\(\\mathbb R^k\\)</span>,\nwhile keeping as much information on the data as possible.\nLet\u2019s briefly\nrecap what PCA does and how it works.</p><p>PCA is based on <em>singular value decomposition</em>, or <em>SVD</em>.\nFor this consider a matrix <span class=\"math notranslate nohighlight\">\\(X\\in\\mathbb R^{N\\times n}\\)</span>, which in our context will\narise from stacking the data <span class=\"math notranslate nohighlight\">\\(x^{(1)},\\ldots,x^{(N)}\\in\\mathbb R^n\\)</span>  as row vectors on top of each other, ie,\neach row corresponds to a data point. (The definition of the SVD works for any matrix, though.)\nIt\u2019s best to assume <span class=\"math notranslate nohighlight\">\\(N\\gg n\\)</span>.</p>", "a[href=\"#KLnormalthm\"]": "<div class=\"proof theorem admonition\" id=\"KLnormalthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 7.2 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<div class=\"math notranslate nohighlight\">\n\\[\n\\KL(\\mathcal N(\\mu,s)||\\mathcal N(0,1)) = \\tfrac{1}{2}\\sum_{i=1}^k\\left(s_i+\\mu_i^2-1-\\ln s_i\\right)\n\\]</div>\n<p>for any <span class=\"math notranslate nohighlight\">\\(\\mu\\in\\mathbb R^k\\)</span> and <span class=\"math notranslate nohighlight\">\\(s\\in\\mathbb R_{&gt;0}^k\\)</span>.</p>\n</section>\n</div>", "a[href=\"#equation-elbomax\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-elbomax\">\n\\[\\max_{\\theta,\\phi} \\sum_{i=1}^M\\text{ELBO}_{\\theta,\\phi}\\big(x^{(i)}\\big)\\]</div>", "a[href=\"nets.html#gibbsineq\"]": "<div class=\"proof theorem admonition\" id=\"gibbsineq\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 4.1 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(p,q\\)</span> be two (discrete) probability distributions. Then\nit holds that <span class=\"math notranslate nohighlight\">\\(\\KL(p||q)\\geq 0\\)</span>, and\n<span class=\"math notranslate nohighlight\">\\(\\KL(p||q)=0\\)</span> if and only if <span class=\"math notranslate nohighlight\">\\(p=q\\)</span>.</p>\n</section>\n</div>", "a[href=\"#pca-and-autoencoder\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.2. </span>PCA and autoencoder<a class=\"headerlink\" href=\"#pca-and-autoencoder\" title=\"Link to this heading\">#</a></h2><p>We consider the simplest autoencoder possible: an encoder that consists of a neural network <em>without</em> hidden layers, and with\nlinear activation function, and the same holds for the decoder. On top of that, we set the bias to 0 everywhere.\nThen, the encoder does nothing more than a matrix-vector mulitplication: <span class=\"math notranslate nohighlight\">\\(e:x\\mapsto Ex\\)</span> for some matrix <span class=\"math notranslate nohighlight\">\\(E\\in\\mathbb R^{k\\times n}\\)</span>.\nThe same goes for the decoder: <span class=\"math notranslate nohighlight\">\\(d:z\\mapsto Dz\\)</span> for some <span class=\"math notranslate nohighlight\">\\(D\\in\\mathbb R^{n\\times k}\\)</span>.\nIn total, the autoencoder computes <span class=\"math notranslate nohighlight\">\\(x\\mapsto DEx\\)</span>.</p><p>If we collect the (training) data as above in a matrix <span class=\"math notranslate nohighlight\">\\(X\\in\\mathbb R^{N\\times n}\\)</span> (with each data point as a row)\nthen the autoencoder computes <span class=\"math notranslate nohighlight\">\\(\\hat X = X\\trsp E\\trsp D\\)</span>. Note that the matrix <span class=\"math notranslate nohighlight\">\\(C=\\trsp E\\trsp D\\)</span> has rank at most <span class=\"math notranslate nohighlight\">\\(k\\)</span>,\nas each of <span class=\"math notranslate nohighlight\">\\(D,E\\)</span> has one dimension equal to <span class=\"math notranslate nohighlight\">\\(k\\)</span>.</p>", "a[href=\"#autoencoderfig\"]": "<figure class=\"align-default\" id=\"autoencoderfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/autoencoder.png\"><img alt=\"../_images/autoencoder.png\" src=\"../_images/autoencoder.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 7.1 </span><span class=\"caption-text\">An autoencoder.</span><a class=\"headerlink\" href=\"#autoencoderfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#autoencoders\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7. </span>Autoencoders<a class=\"headerlink\" href=\"#autoencoders\" title=\"Link to this heading\">#</a></h1><p>Labelled training data is often scarce. Labelling is typically costly and time consuming. Often, however,\nunlabelled data is much more readily available (think all the images on the internet, or all of wikipedia).\n<em>Autoencoders</em> are a way to leverage unlabelled data\nin tasks that in principle would require labelled data. So what\u2019s an autoencoder? An autoencoder\nconsists of two parts: an <em>encoder</em> neural network <span class=\"math notranslate nohighlight\">\\(e:\\mathbb R^n\\to\\mathbb R^k\\)</span> and\na <em>decoder</em> neural network <span class=\"math notranslate nohighlight\">\\(d:\\mathbb R^k\\to\\mathbb R^n\\)</span>. Here, <span class=\"math notranslate nohighlight\">\\(n\\)</span> is the input dimension, while\n<span class=\"math notranslate nohighlight\">\\(k\\)</span> is the dimension of the <em>latent space</em>, ie, of the <em>latent representation</em> <span class=\"math notranslate nohighlight\">\\(z=e(x)\\)</span>.\nThe idea of an autoencoder is that it learns to replicate the input. That is, that on input <span class=\"math notranslate nohighlight\">\\(x\\)</span> it computes</p>", "a[href=\"#variational-autoencoders\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.3. </span>Variational autoencoders<a class=\"headerlink\" href=\"#variational-autoencoders\" title=\"Link to this heading\">#</a></h2><p>The latent representation in autoencoders makes it possible to generate new samples.\nGenerative AI such as midjourney, DALL-E and stable diffusion\nshow that this may be incredibly useful. (These systems, however, are based on a different method.)\nThere is just a problem \u2013 how would we sample\nfrom the latent space? We do not have any control over the latent space, and we have no idea\nhow the latent representations of the samples are distributed.</p><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-2\"></label><input class=\"margin-toggle\" id=\"marginnote-role-2\" name=\"marginnote-role-2\" type=\"checkbox\"/><span class=\"marginnote\"> <em>An Introduction to Variational Autoencoders</em>, D.P. Kingma and M. Welling (2019),\n<a class=\"reference external\" href=\"https://arxiv.org/abs/1906.02691\">arXiv:1906.02691</a></span>\n<em>Variational autoencoders</em> (VAEs) aim to impose tight control on the latent space.\nIn fact in a variational autoencoder, the latent vectors are forced to conform\nto a fixed probability distribution, normally to a multivariate normal distribution.\nThen, it\u2019s easy to sample from the latent space: We simply draw <span class=\"math notranslate nohighlight\">\\(z\\sim \\mathcal N(0,1)\\)</span>,\nand then use the decoder to compute a new sample <span class=\"math notranslate nohighlight\">\\(d(z)\\)</span>.</p>", "a[href=\"#equation-elbomax2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-elbomax2\">\n\\[\\begin{split}\\min_{\\theta,\\phi}\\Big(-&amp;\\text{ELBO}_{\\theta,\\phi}(x)\\Big)\\notag \\\\  \n&amp; = \\min_{\\theta,\\phi} \\KL(q_\\phi(z|x)||\\mathcal N(0,1)) -\\expec_{z\\sim q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right] \\end{split}\\]</div>", "a[href=\"#multiKLlem\"]": "<div class=\"proof lemma admonition\" id=\"multiKLlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 7.1 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(p,q\\)</span> be multivariate probability density functions with independent components, ie,</p>\n<div class=\"math notranslate nohighlight\">\n\\[\np(x)=\\prod_{i=1}^k p_i(x_i)\\quad\\text{ and }\\quad q(x)=\\prod_{i=1}^kq_i(x_i),\n\\]</div>\n<p>where <span class=\"math notranslate nohighlight\">\\(p_i,q_i\\)</span> are univariate probability density functions.\nThen</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\KL(p||q)=\\sum_{i=1}^k\\KL(p_i||q_i)\n\\]</div>\n</section>\n</div>", "a[href=\"nets.html#klsec\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.5. </span>Kullback-Leibler divergence<a class=\"headerlink\" href=\"#kullback-leibler-divergence\" title=\"Link to this heading\">#</a></h2><p>The <em>Kullback-Leibler divergence</em> measures how different two probability distributions are.\nFor the definition let us consider two discrete probability distributions <span class=\"math notranslate nohighlight\">\\(p\\)</span> and <span class=\"math notranslate nohighlight\">\\(q\\)</span>\n(the general case is similar but involves integrals and measure-theoretic caveats). Then\nthe  Kullback-Leibler divergence between <span class=\"math notranslate nohighlight\">\\(p\\)</span> and <span class=\"math notranslate nohighlight\">\\(q\\)</span> is defined as</p>", "a[href=\"#the-kullback-leibler-loss\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.5. </span>The Kullback-Leibler loss<a class=\"headerlink\" href=\"#the-kullback-leibler-loss\" title=\"Link to this heading\">#</a></h2><p>What about the Kullback-Leibler loss <span class=\"math notranslate nohighlight\">\\(\\KL(q_\\phi(z|x)||\\mathcal N(0,1))\\)</span>?\nWe had fixed the distribution <span class=\"math notranslate nohighlight\">\\(q_\\phi(z|x)\\)</span> to be a normal distribution. Indeed, it is simply <span class=\"math notranslate nohighlight\">\\(\\mathcal N(\\mu^{(2)},s^{(2)})\\)</span>,\nwhere <span class=\"math notranslate nohighlight\">\\(\\mu^{(2)}\\)</span> and <span class=\"math notranslate nohighlight\">\\(s^{(2)}\\)</span> are computed by the encoder network. Fortunately, for two\nnormal distributions there is an explizit formula for the Kullback-Leibler divergence\n(and in our case, one is even the standard normal distribution).</p>", "a[href=\"#vaefig\"]": "<figure class=\"align-default\" id=\"vaefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/VAE2.png\"><img alt=\"../_images/VAE2.png\" src=\"../_images/VAE2.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 7.3 </span><span class=\"caption-text\">A variational autoencoder.</span><a class=\"headerlink\" href=\"#vaefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#eymthm\"]": "<div class=\"proof theorem admonition\" id=\"eymthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 7.1 </span> (Eckhart-Young-Mirsky)</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(X\\in\\mathbb R^{n\\times N}\\)</span> be a matrix, and let <span class=\"math notranslate nohighlight\">\\(k\\leq\\min(n,N)\\)</span> be an integer.\nLet <span class=\"math notranslate nohighlight\">\\(\\hat X= U_k\\text{diag}(\\sigma_1\\ldots\\sigma_k)\\trsp{V_k}\\)</span>, where <span class=\"math notranslate nohighlight\">\\(X=U\\Sigma \\trsp V\\)</span> is the SVD of <span class=\"math notranslate nohighlight\">\\(X\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\sigma\\)</span>\nis the vector of singular values.\nThen</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n||\\hat X -X||_F \\leq ||A-X||_F,\n\\]</div>\n<p>for every matrix <span class=\"math notranslate nohighlight\">\\(A\\in\\mathbb R^{n\\times N}\\)</span> with <span class=\"math notranslate nohighlight\">\\(\\rank(A)\\leq k\\)</span>.</p>\n</section>\n</div>", "a[href=\"#vaexfig\"]": "<figure class=\"align-default\" id=\"vaexfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/VAE1.png\"><img alt=\"../_images/VAE1.png\" src=\"../_images/VAE1.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 7.2 </span><span class=\"caption-text\">From latent space to sample.</span><a class=\"headerlink\" href=\"#vaexfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
