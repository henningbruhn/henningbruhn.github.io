selector_to_html = {"a[href=\"https://en.wikipedia.org/wiki/Logistic_function\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>A <b>logistic function</b> or <b>logistic curve</b> is a common S-shaped curve with the equation</p><dl><dd><span class=\"mwe-math-element\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/6f42e36c949c94189976ae00853af9a1b618e099\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.338ex;width:21.076ex;height:5.676ex\" /></span></dd></dl>", "a[href^=\"https://en.wikipedia.org/wiki/Logistic_function#\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>A <b>logistic function</b> or <b>logistic curve</b> is a common S-shaped curve with the equation</p><dl><dd><span class=\"mwe-math-element\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/6f42e36c949c94189976ae00853af9a1b618e099\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.338ex;width:21.076ex;height:5.676ex\" /></span></dd></dl>", "a[href=\"https://en.wikipedia.org/wiki/GPT-3\"]": "<p><b>Generative Pre-trained Transformer 3</b> (<b>GPT-3</b>) is a large language model released by OpenAI in 2020.</p>", "a[href^=\"https://en.wikipedia.org/wiki/GPT-3#\"]": "<p><b>Generative Pre-trained Transformer 3</b> (<b>GPT-3</b>) is a large language model released by OpenAI in 2020.</p>", "a[href=\"https://en.wikipedia.org/wiki/MNIST_database\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/MNIST_dataset_example.png/320px-MNIST_dataset_example.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>The <b>MNIST database</b> is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.</p>", "a[href^=\"https://en.wikipedia.org/wiki/MNIST_database#\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/MNIST_dataset_example.png/320px-MNIST_dataset_example.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>The <b>MNIST database</b> is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.</p>", "a[href=\"#heightsfig\"]": "<figure class=\"align-default\" id=\"heightsfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/heights_notes.png\"><img alt=\"../_images/heights_notes.png\" src=\"../_images/heights_notes.png\" style=\"width: 8cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.13 </span><span class=\"caption-text\">Height distribution of US-Americans in 2007 / 2008 of age 40 to 49, fitted to Gaussians. Source: U.S.\\ National Center for Health Statistics</span><a class=\"headerlink\" href=\"#heightsfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-linzo\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-linzo\">\n\\[\\begin{split}\\min_{w} L_S(h_w) &amp;= \\min_w\\frac{1}{|S|}\\sum_{(x,y)\\in S}\\ell_{0-1}(y,\\sgn\\circ h_w(x)) \\notag \\\\\n &amp; =\\min_{w}\\frac{1}{|S|}\\sum_{(x,y)\\in S}\\tfrac{1}{2}(1-y\\sgn\\trsp wx)\\end{split}\\]</div>", "a[href=\"#polynomial-predictors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.6. </span>Polynomial predictors<a class=\"headerlink\" href=\"#polynomial-predictors\" title=\"Link to this heading\">#</a></h2><p>Linear predictors are very simple and will often have a large training error.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-11\"></label><input class=\"margin-toggle\" id=\"marginnote-role-11\" name=\"marginnote-role-11\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/quadpred.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>quadpred</a></span>\nA classifier based on quadratic functions, or even higher polynomials,\nwill certainly be more powerful and will often have smaller training error.\nFor a quadratic classifier,\nwe look for weights <span class=\"math notranslate nohighlight\">\\(u\\in\\mathbb R^{d\\times d}\\)</span>, <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^d\\)</span> and <span class=\"math notranslate nohighlight\">\\(b\\in\\mathbb R\\)</span>\nsuch that</p>", "a[href=\"#decision-trees\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.8. </span>Decision trees<a class=\"headerlink\" href=\"#decision-trees\" title=\"Link to this heading\">#</a></h2><p>Let <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\subseteq \\mathbb R^d\\)</span>, and let <span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span> be a (finite) set of classes.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-13\"></label><input class=\"margin-toggle\" id=\"marginnote-role-13\" name=\"marginnote-role-13\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/dectree.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>dectree</a></span>\nA \\defi{decision tree} <span class=\"math notranslate nohighlight\">\\(T\\)</span> for <span class=\"math notranslate nohighlight\">\\(\\mathcal X,\\mathcal Y\\)</span> consists of a rooted tree, where each\nnon-leaf is associated with a decision rule. That is, <span class=\"math notranslate nohighlight\">\\(T\\)</span> has a root <span class=\"math notranslate nohighlight\">\\(r\\)</span>, every vertex <span class=\"math notranslate nohighlight\">\\(v\\)</span> is either\na \\ndefi{leaf} or not; if <span class=\"math notranslate nohighlight\">\\(v\\)</span> is not a leaf then it has precisely two children <span class=\"math notranslate nohighlight\">\\(v_L,v_R\\)</span>; every\nleaf <span class=\"math notranslate nohighlight\">\\(v\\)</span> is labelled with a class <span class=\"math notranslate nohighlight\">\\(c(v)\\in\\mathcal Y\\)</span>; every non-leaf <span class=\"math notranslate nohighlight\">\\(v\\)</span> has a decision rule,\na tuple <span class=\"math notranslate nohighlight\">\\((i,t)\\)</span>, where <span class=\"math notranslate nohighlight\">\\(i\\in[d]\\)</span> is a feature and <span class=\"math notranslate nohighlight\">\\(t\\in\\mathbb R\\)</span>\nis a threshold. A decision tree defines a classifier in the following sense:</p>", "a[href=\"#a-statistical-model\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.12. </span>A statistical model<a class=\"headerlink\" href=\"#a-statistical-model\" title=\"Link to this heading\">#</a></h2><p>On the face of it, what machine learning algorithms accomplish seems implausible.\nWe train the predictors on one dataset, the training set, and then expect\nthe predictors to perform well on a completely different dataset, ie, on new data, or\non the test set. How can that work at all?</p><p>That works \u2013 if training data and new data come from the same source. Mathematically,\nwe model this by a probability distribution  <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span> on <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\times\\mathcal Y\\)</span>.\nThat is, we assume that there is distribution that produces pairs <span class=\"math notranslate nohighlight\">\\((x,y)\\in\\mathcal X\\times\\mathcal Y\\)</span>\nof datapoints <span class=\"math notranslate nohighlight\">\\(x\\)</span> and classes <span class=\"math notranslate nohighlight\">\\(y\\)</span>, and we assume that the training set as well as\nany new data (or the test set) is drawn from this distribution.</p>", "a[href=\"#logistic-regression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.4. </span>Logistic regression<a class=\"headerlink\" href=\"#logistic-regression\" title=\"Link to this heading\">#</a></h2><p>How can we train a linear predictor? Ideally, we would minimise the training error directly.\nLet\u2019s assume\na homogeneous training set, which means that it suffices to\nlearn a linear classifer of the form <span class=\"math notranslate nohighlight\">\\(h_w\\)</span>, ie, one without a bias term.\nAnd let\u2019s assume that the loss function is zero-one loss.\nThen, minimising the training error means solving the following optimisation problem:<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-7\"></label><input class=\"margin-toggle\" id=\"marginnote-role-7\" name=\"marginnote-role-7\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/logreg.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>logreg</a></span></p>", "a[href=\"#mnistfig\"]": "<figure class=\"align-default\" id=\"mnistfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/mnist_row.png\"><img alt=\"../_images/mnist_row.png\" src=\"../_images/mnist_row.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.1 </span><span class=\"caption-text\">A small sample from the MNIST data set</span><a class=\"headerlink\" href=\"#mnistfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#linpredfig\"]": "<figure class=\"align-default\" id=\"linpredfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/sep_and_non_sep.png\"><img alt=\"../_images/sep_and_non_sep.png\" src=\"../_images/sep_and_non_sep.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.3 </span><span class=\"caption-text\">The left dataset can be fit perfectly by a linear classifier, the one on the right cannot.</span><a class=\"headerlink\" href=\"#linpredfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#nearest-neighbour\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.7. </span>Nearest neighbour<a class=\"headerlink\" href=\"#nearest-neighbour\" title=\"Link to this heading\">#</a></h2><p>A classic and very simple algorithm is <em>nearest neighbour</em>.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-12\"></label><input class=\"margin-toggle\" id=\"marginnote-role-12\" name=\"marginnote-role-12\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/nearest.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>nearest</a></span>\nDuring training\nthe algorithm simply memorises all training data points. When it is tasked to predict the\nclass of a new data point it determines the closest training data point and outputs the class\nof the training data point. The idea is that two data points that have similar features are\nlikely to have the same class.\\movtip{neighbour}%</p><p>As described the algorithm is very sensitive towards noise in the training set.\nA single erroneously classified data point in the training set, for instance, may lead to\nmany bad predictions of new data. Because of that, a more robust variant is often used,\n<em><span class=\"math notranslate nohighlight\">\\(k\\)</span>-nearest neighbour</em>: for each new data point the <span class=\"math notranslate nohighlight\">\\(k\\)</span> closest data points of the\ntraining sets are determined, and the output is then most common class among these\n<span class=\"math notranslate nohighlight\">\\(k\\)</span> data points. (Ties may be split randomly.)</p>", "a[href=\"#gdtimefig\"]": "<figure class=\"align-default\" id=\"gdtimefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/learning.png\"><img alt=\"../_images/learning.png\" src=\"../_images/learning.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.2 </span><span class=\"caption-text\">Learning an algorithm, the predictor</span><a class=\"headerlink\" href=\"#gdtimefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-zorisk\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-zorisk\">\n\\[L_{\\mathcal D}(h)=\\proba_{(x,y)\\sim\\mathcal D}[h(x)\\neq y]\\]</div>", "a[href=\"#equation-bayclass\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-bayclass\">\n\\[\\bayes(x)=\\argmax_{y\\in \\mathcal Y}\\proba[y|x] \\]</div>", "a[href=\"#ginifig\"]": "<figure class=\"align-default\" id=\"ginifig\">\n<a class=\"reference internal image-reference\" href=\"../_images/gini.png\"><img alt=\"../_images/gini.png\" src=\"../_images/gini.png\" style=\"width: 12cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.9 </span><span class=\"caption-text\">Train error, Gini impurity and entropy for two classes; <span class=\"math notranslate nohighlight\">\\(x\\)</span>-axis shows fraction of first class in sample.</span><a class=\"headerlink\" href=\"#ginifig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#logisfig\"]": "<figure class=\"align-default\" id=\"logisfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/logistic.png\"><img alt=\"../_images/logistic.png\" src=\"../_images/logistic.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.4 </span><span class=\"caption-text\">The logistic function</span><a class=\"headerlink\" href=\"#logisfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#vocabulary\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.2. </span>Vocabulary<a class=\"headerlink\" href=\"#vocabulary\" title=\"Link to this heading\">#</a></h2><p>In a classification task, we aim to train a classifier that assigns labels or a class to\ngiven input data.\nIn the MNIST digit recognition task, the input consists of grey-scale images of size\n28<span class=\"math notranslate nohighlight\">\\(\\times\\)</span>28 pixels. This is the <em>domain set</em>,\nthe set <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\)</span> that contains all (possible) data points for the task at hand.\nNormally <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\)</span> is a finite-dimensional vector space such as <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n\\)</span>, or at least a\nsubset of <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n\\)</span>. In the MNIST task, we could set <span class=\"math notranslate nohighlight\">\\(\\mathcal X=\\{0,\\ldots, 255\\}^{28\\times 28}\\)</span>,\nif we assume that each pixel has a grey value in 0,\u2026,255.</p><p>The entries of a data point <span class=\"math notranslate nohighlight\">\\(x\\in\\mathcal X\\)</span> are\nthe <em>features</em> or <em>attributes</em> of <span class=\"math notranslate nohighlight\">\\(x\\)</span>. This could be\nthe grey value of a pixel or the income of a customer.</p>", "a[href=\"#loss-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.10. </span>Loss functions<a class=\"headerlink\" href=\"#loss-functions\" title=\"Link to this heading\">#</a></h2><p>So far, we have only considered zero-one loss.\nIn some situations, other loss functions will be needed. For instance, if we want to classify\nemails as <em>spam or ham</em>, that is, as spam emails or non-spam, then it is far more serious to\nmisclassify a ham email than a spam email: Indeed, if we miss an important email because it was put\nin the spam folder (or perhaps deleted) then we will be quite cross, the occasional Viagra ad in our\ninbox, however, will merely annoy us somewhat.\nA loss function then might look like this:</p>", "a[href=\"#predictors-classification-and-losses\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1. </span>Predictors, classification and losses<a class=\"headerlink\" href=\"#predictors-classification-and-losses\" title=\"Link to this heading\">#</a></h1><p>Does a given picture show a dog or a cat?\nShould the applicant be granted a credit? Is the mail\nspam or ham? These are all examples of <em>classification</em> tasks, one of the\nprincipal domains in machine learning.</p><p>A well-known classification tasks involves the MNIST data set.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-1\"></label><input class=\"margin-toggle\" id=\"marginnote-role-1\" name=\"marginnote-role-1\" type=\"checkbox\"/><span class=\"marginnote\"> MNIST is so well known that it has a <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/MNIST_database\">wikipedia</a> entry.</span>\nThe MNIST data set contains 70000 handwritten digits as images of <span class=\"math notranslate nohighlight\">\\(28\\times 28\\)</span> pixels. The task is\nto decide whether a given image <span class=\"math notranslate nohighlight\">\\(x\\)</span> shows a <span class=\"math notranslate nohighlight\">\\(0\\)</span>, <span class=\"math notranslate nohighlight\">\\(1\\)</span>, or perhaps a <span class=\"math notranslate nohighlight\">\\(7\\)</span>. In machine learning\nthis tasks is solved by letting an algorithm <em>learn</em> how to accomplish the tasks by\ngiving it access to a large number of examples, the <em>training set</em>,\ntogether with the true classification.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-2\"></label><input class=\"margin-toggle\" id=\"marginnote-role-2\" name=\"marginnote-role-2\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/MNIST.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>MNIST</a></span>\nIn the MNIST tasks that means that the algorithm not only\nreceives perhaps 60000 images, each containing a handwritten digit, but also for each image\nthe information which <em>class</em> it is, ie, which of the digits 0,1,\u2026,9 is shown; see <a class=\"reference internal\" href=\"#mnistfig\"><span class=\"std std-numref\">Fig. 1.1</span></a>.</p>", "a[href=\"#knnfig\"]": "<figure class=\"sphinx-subfigure align-default\" id=\"knnfig\">\n<div class=\"sphinx-subfigure-grid ss-layout-default-AB ss-layout-sm-A_B\" style=\"display: grid; gap: 10px; grid-gap: 10px;\">\n<div class=\"sphinx-subfigure-area\" style=\"display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;\">\n<a class=\"reference internal image-reference\" href=\"../_images/one_nn.png\"><img alt=\"one\" src=\"../_images/one_nn.png\" style=\"width: 6cm;\"/>\n</a>\n</div>\n<div class=\"sphinx-subfigure-area\" style=\"display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: B;\">\n<a class=\"reference internal image-reference\" href=\"../_images/twenty_nn.png\"><img alt=\"twenty\" src=\"../_images/twenty_nn.png\" style=\"width: 6cm;\"/>\n</a>\n</div>\n</div>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.7 </span><span class=\"caption-text\">Algorithm <span class=\"math notranslate nohighlight\">\\(k\\)</span>-nearest neighbour for <span class=\"math notranslate nohighlight\">\\(k=1\\)</span> on the left and <span class=\"math notranslate nohighlight\">\\(k=20\\)</span> on the right</span><a class=\"headerlink\" href=\"#knnfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#neural-networks\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.9. </span>Neural networks<a class=\"headerlink\" href=\"#neural-networks\" title=\"Link to this heading\">#</a></h2><p>We\u2019ve already talked about a number of classifying algorithms. I omitted, however, the most important one of\nthem all, the neural network. Let\u2019s remedy this omission.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-16\"></label><input class=\"margin-toggle\" id=\"marginnote-role-16\" name=\"marginnote-role-16\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/tfintro.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>tfintro</a></span></p><p>Neural networks are old: Their origins go back to the 1940s. Modelled after (a crude simplification of) neurons\nin the brain they consists of single units, the neurons, that collect the inputs of other units (or of the input)\nand then compute an output value that might be fed into other neurons.</p>", "a[href=\"#regression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.11. </span>Regression<a class=\"headerlink\" href=\"#regression\" title=\"Link to this heading\">#</a></h2><p>Regression is a more general problem than classification.\nIn both cases, we aim to find a predictor <span class=\"math notranslate nohighlight\">\\(h:\\mathcal X\\to\\mathcal Y\\)</span>. In classification, the target set\n<span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span> is finite: we are interested in knowing whether the email is spam or not, or\nwhether the image shows a cat, a dog or a hamster. In regression, in contrast, <span class=\"math notranslate nohighlight\">\\(\\mathcal Y\\)</span> is usually\ncontinuous, and normally equal to an interval <span class=\"math notranslate nohighlight\">\\([a,b]\\)</span>, or perhaps to a multidimensional analog\nsuch as <span class=\"math notranslate nohighlight\">\\([a,b]^n\\)</span>.\nIn principle, regression can be seen as a <em>function approximation</em> task. There is some unknown function <span class=\"math notranslate nohighlight\">\\(f\\)</span>\nthat we want to approximate with our predictor <span class=\"math notranslate nohighlight\">\\(h:\\mathcal X\\to\\mathcal Y\\)</span>.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-17\"></label><input class=\"margin-toggle\" id=\"marginnote-role-17\" name=\"marginnote-role-17\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/california.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>regression</a></span></p><p>The theory for classification, however, is cleaner than for regression, and this is the main reason\nwhy I will focus on classification tasks.\nArguably, regression is more powerful.\nI am probably more interested in how large the expected return on investment\nfor some stock portfolio is (a regression task) than whether there is a positive or negative ROI\n(a classification task). Let\u2019s do a quick digression on regression.</p>", "a[href=\"#quadpredfig\"]": "<figure class=\"align-default\" id=\"quadpredfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/quadpred.png\"><img alt=\"../_images/quadpred.png\" src=\"../_images/quadpred.png\" style=\"width: 8cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.6 </span><span class=\"caption-text\">A quadratic classifier</span><a class=\"headerlink\" href=\"#quadpredfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#testerrfig\"]": "<figure class=\"align-default\" id=\"testerrfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/testerr.png\"><img alt=\"../_images/testerr.png\" src=\"../_images/testerr.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.5 </span><span class=\"caption-text\">Training, validation and test set.</span><a class=\"headerlink\" href=\"#testerrfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#neuronfig\"]": "<figure class=\"align-default\" id=\"neuronfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/neuron.png\"><img alt=\"../_images/neuron.png\" src=\"../_images/neuron.png\" style=\"width: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.10 </span><span class=\"caption-text\">A single neuron</span><a class=\"headerlink\" href=\"#neuronfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#actfig\"]": "<figure class=\"align-default\" id=\"actfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/activations.png\"><img alt=\"../_images/activations.png\" src=\"../_images/activations.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.11 </span><span class=\"caption-text\">Common activation functions</span><a class=\"headerlink\" href=\"#actfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-logloss\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-logloss\">\n\\[\\min_{w} \\frac{1}{|S|}\\sum_{(x,y)\\in S}-\\log_2\\left(\\sigm(y\\trsp wx)\\right)\\]</div>", "a[href=\"#iristreefig\"]": "<figure class=\"align-default\" id=\"iristreefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/iris.png\"><img alt=\"../_images/iris.png\" src=\"../_images/iris.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.8 </span><span class=\"caption-text\">A decision tree for the <em>iris</em> data set</span><a class=\"headerlink\" href=\"#iristreefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#classnetfig\"]": "<figure class=\"align-default\" id=\"classnetfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/classnet.png\"><img alt=\"../_images/classnet.png\" src=\"../_images/classnet.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1.12 </span><span class=\"caption-text\">A simple classification ReLU-neural network</span><a class=\"headerlink\" href=\"#classnetfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#linear-predictors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.3. </span>Linear predictors<a class=\"headerlink\" href=\"#linear-predictors\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s look at possible the simplest sort of classification algorithm, a <em>linear classifier</em>.</p><p>We consider a binary classification task with domain set <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\subseteq \\mathbb R^d\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\mathcal Y=\\{-1,1\\}\\)</span>.\nA very simple classifier tries to separate the data points into different halfspaces. That is, we look for a halfspace\n<span class=\"math notranslate nohighlight\">\\(H=\\{x\\in\\mathbb R^d: \\trsp wx\\geq 0\\}\\)</span> and then classify all points in <span class=\"math notranslate nohighlight\">\\(H\\)</span> as <span class=\"math notranslate nohighlight\">\\(+1\\)</span>, say, and all points\nin <span class=\"math notranslate nohighlight\">\\(\\mathbb R^d\\setminus H\\)</span> as <span class=\"math notranslate nohighlight\">\\(-1\\)</span>. More generally, we could use an affine hyperplane defined by <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^d\\)</span>\nand <span class=\"math notranslate nohighlight\">\\(b\\in\\mathbb R\\)</span> and then classify as follows</p>", "a[href=\"#bayes-error\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.13. </span>Bayes error<a class=\"headerlink\" href=\"#bayes-error\" title=\"Link to this heading\">#</a></h2><p>Because\nof the inherent uncertainty in the distribution, the smallest generalisation error that can be achieved\nmight be larger than zero.</p><p>Let\u2019s assume, for example, that we want to predict the sex of a person based only on the height of the\nperson. A person of height 1.8m is likely male \u2013 but not necessarily so. Obviously, there are also\nwomen of 1.8m, but they are fewer than men of 1.8m. Height does not determine sex, and thus any\nclassifier based only on height will never be perfect. This residual error that any classifier\nis bound to make is called <em>Bayes error</em>. It is defined as</p>", "a[href=\"#equation-impurity\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-impurity\">\n\\[\\left(\\frac{|S_L|}{|S_v|}\\gamma(S_L)+\\frac{|S_R|}{|S_v|}\\gamma(S_R)\\right),\\]</div>", "a[href=\"#tasks-in-ml\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.1. </span>Tasks in ML<a class=\"headerlink\" href=\"#tasks-in-ml\" title=\"Link to this heading\">#</a></h2><p>Some common tasks in machine learning are:</p>", "a[href=\"#training-and-errors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">1.5. </span>Training and errors<a class=\"headerlink\" href=\"#training-and-errors\" title=\"Link to this heading\">#</a></h2><p>So far, we have tried to minimise the training error. With the zero-one-loss, for instance,\nwe tried to minimise the misclassification rate on the training set when fitting a logistic regression.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-10\"></label><input class=\"margin-toggle\" id=\"marginnote-role-10\" name=\"marginnote-role-10\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/basic_algorithms_and_concepts/errors.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>errors</a></span></p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
