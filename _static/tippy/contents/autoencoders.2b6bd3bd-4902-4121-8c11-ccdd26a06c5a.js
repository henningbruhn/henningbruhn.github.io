selector_to_html = {"a[href=\"https://en.wikipedia.org/wiki/Low-rank_approximation\"]": "<p>In mathematics, <b>low-rank approximation</b> refers to the process of approximating a given matrix by a matrix of lower rank. More precisely, it is a minimization problem, in which the cost function measures the fit between a given matrix and an approximating matrix, subject to a constraint that the approximating matrix has reduced rank. The problem is used for mathematical modeling and data compression. The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure.</p>", "a[href^=\"https://en.wikipedia.org/wiki/Low-rank_approximation#\"]": "<p>In mathematics, <b>low-rank approximation</b> refers to the process of approximating a given matrix by a matrix of lower rank. More precisely, it is a minimization problem, in which the cost function measures the fit between a given matrix and an approximating matrix, subject to a constraint that the approximating matrix has reduced rank. The problem is used for mathematical modeling and data compression. The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure.</p>", "a[href=\"#eymthm\"]": "<div class=\"proof theorem admonition\" id=\"eymthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 7.1 </span> (Eckhart-Young-Mirsky)</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(X\\in\\mathbb R^{n\\times N}\\)</span> be a matrix, and let <span class=\"math notranslate nohighlight\">\\(k\\leq\\min(n,N)\\)</span> be an integer.\nLet <span class=\"math notranslate nohighlight\">\\(\\hat X= U_k\\text{diag}(\\sigma_1\\ldots\\sigma_k)\\trsp{V_k}\\)</span>, where <span class=\"math notranslate nohighlight\">\\(X=U\\Sigma \\trsp V\\)</span> is the SVD of <span class=\"math notranslate nohighlight\">\\(X\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\sigma\\)</span>\nis the vector of singular values.\nThen</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n||\\hat X -X||_F \\leq ||A-X||_F,\n\\]</div>\n<p>for every matrix <span class=\"math notranslate nohighlight\">\\(A\\in\\mathbb R^{n\\times N}\\)</span> with <span class=\"math notranslate nohighlight\">\\(\\rank(A)\\leq k\\)</span>.</p>\n</section>\n</div>", "a[href=\"#principle-component-analysis\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.1. </span>Principle component analysis<a class=\"headerlink\" href=\"#principle-component-analysis\" title=\"Link to this heading\">#</a></h2><p>Principle component analysis is a common method to project some data\nfrom a high-dimensional space <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n\\)</span> to a low-dimensional space <span class=\"math notranslate nohighlight\">\\(\\mathbb R^k\\)</span>,\nwhile keeping as much information on the data as possible.\nLet\u2019s briefly\nrecap what PCA does and how it works.</p><p>PCA is based on <em>singular value decomposition</em>, or <em>SVD</em>.\nFor this consider a matrix <span class=\"math notranslate nohighlight\">\\(X\\in\\mathbb R^{N\\times n}\\)</span>, which in our context will\narise from stacking the data <span class=\"math notranslate nohighlight\">\\(x^{(1)},\\ldots,x^{(N)}\\in\\mathbb R^n\\)</span>  as row vectors on top of each other, ie,\neach row corresponds to a data point. (The definition of the SVD works for any matrix, though.)\nIt\u2019s best to assume <span class=\"math notranslate nohighlight\">\\(N\\gg n\\)</span>.</p>", "a[href=\"#autoencoderfig\"]": "<figure class=\"align-default\" id=\"autoencoderfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/autoencoder.png\"><img alt=\"../_images/autoencoder.png\" src=\"../_images/autoencoder.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 7.1 </span><span class=\"caption-text\">An autoencoder.</span><a class=\"headerlink\" href=\"#autoencoderfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#variational-autoencoders\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.3. </span>Variational autoencoders<a class=\"headerlink\" href=\"#variational-autoencoders\" title=\"Link to this heading\">#</a></h2><p>The latent representation in autoencoders makes it possible to generate new samples.\nGenerative AI such as midjourney, DALL-E and stable diffusion\nshow that this may be incredibly useful. (These systems, however, are based on a different method.)\nThere is just a problem \u2013 how would we sample\nfrom the latent space? We do not have any control over the latent space, and we have no idea\nhow the latent representations of the samples are distributed.</p><p><em>Variational autoencoders</em> (VAEs) aim to impose tight control on the latent space.\nIn fact in a variational autoencoder, the latent vectors are forced to conform\nto a fixed probability distribution, normally to a multivariate normal distribution.\nThen, it\u2019s easy to sample from the latent space: We simply draw <span class=\"math notranslate nohighlight\">\\(z\\sim \\mathcal N(0,1)\\)</span>,\nand then use the decoder to compute a new sample <span class=\"math notranslate nohighlight\">\\(d(z)\\)</span>.</p>", "a[href=\"#vaexfig\"]": "<figure class=\"align-default\" id=\"vaexfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/VAE1.png\"><img alt=\"../_images/VAE1.png\" src=\"../_images/VAE1.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 7.2 </span><span class=\"caption-text\">From latent space to sample.</span><a class=\"headerlink\" href=\"#vaexfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#pca-and-autoencoder\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7.2. </span>PCA and autoencoder<a class=\"headerlink\" href=\"#pca-and-autoencoder\" title=\"Link to this heading\">#</a></h2><p>We consider the simplest autoencoder possible: an encoder that consists of a neural network <em>without</em> hidden layers, and with\nlinear activation function, and the same holds for the decoder. On top of that, we set the bias to 0 everywhere.\nThen, the encoder does nothing more than a matrix-vector mulitplication: <span class=\"math notranslate nohighlight\">\\(e:x\\mapsto Ex\\)</span> for some matrix <span class=\"math notranslate nohighlight\">\\(E\\in\\mathbb R^{k\\times n}\\)</span>.\nThe same goes for the decoder: <span class=\"math notranslate nohighlight\">\\(d:z\\mapsto Dz\\)</span> for some <span class=\"math notranslate nohighlight\">\\(D\\in\\mathbb R^{n\\times k}\\)</span>.\nIn total, the autoencoder computes <span class=\"math notranslate nohighlight\">\\(x\\mapsto DEx\\)</span>.</p><p>If we collect the (training) data as above in a matrix <span class=\"math notranslate nohighlight\">\\(X\\in\\mathbb R^{N\\times n}\\)</span> (with each data point as a row)\nthen the autoencoder computes <span class=\"math notranslate nohighlight\">\\(\\hat X = X\\trsp E\\trsp D\\)</span>. Note that the matrix <span class=\"math notranslate nohighlight\">\\(C=\\trsp E\\trsp D\\)</span> has rank at most~<span class=\"math notranslate nohighlight\">\\(k\\)</span>,\nas each of <span class=\"math notranslate nohighlight\">\\(D,E\\)</span> has one dimension equal to <span class=\"math notranslate nohighlight\">\\(k\\)</span>.</p>", "a[href=\"#autoencoders\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">7. </span>Autoencoders<a class=\"headerlink\" href=\"#autoencoders\" title=\"Link to this heading\">#</a></h1><p>Labelled training data is often scarce. Labelling is typically costly and time consuming. In many situations, however,\nthere a large number of unlabelled data is available. <em>Autoencoders</em> are a way to leverage unlabelled data\nin task that in principle would require labelled data. So what\u2019s an autoencoder? An autoencoder\nconsists of two parts: an <em>encoder</em> neural network <span class=\"math notranslate nohighlight\">\\(e:\\mathbb R^n\\to\\mathbb R^k\\)</span> and\na <em>decoder</em> neural network <span class=\"math notranslate nohighlight\">\\(d:\\mathbb R^k\\to\\mathbb R^n\\)</span>. Here, <span class=\"math notranslate nohighlight\">\\(n\\)</span> is the input dimension, while\n<span class=\"math notranslate nohighlight\">\\(k\\)</span> is the dimension of the <em>latent space</em>, ie, of the <em>latent representation</em> <span class=\"math notranslate nohighlight\">\\(z=e(x)\\)</span>.\nThe idea of an autoencoder is that it learns to replicate the input. That is, that on input <span class=\"math notranslate nohighlight\">\\(x\\)</span> it computes</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
