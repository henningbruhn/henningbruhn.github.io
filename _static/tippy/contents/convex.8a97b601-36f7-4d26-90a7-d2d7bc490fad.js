selector_to_html = {"a[href=\"#discussion-of-sgd\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.8. </span>Discussion of SGD<a class=\"headerlink\" href=\"#discussion-of-sgd\" title=\"Link to this heading\">#</a></h2><p>Descent methods are old, simple and have apparently been observed\nto be ``slow and unreliable\u2019\u2019.<label class=\"margin-toggle\" for=\"sidenote-role-4\"><span id=\"id5\">\n<sup>4</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-4\" name=\"sidenote-role-4\" type=\"checkbox\"/><span class=\"sidenote\"><sup>4</sup><em>Deep learning</em>, p. 148</span> Moreover,\nin convex optimisation, when both algorithms are known to converge,\nSGD has even worse convergence rates than vanilla gradient descent.\nIn fact, while the error <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>, the difference <span class=\"math notranslate nohighlight\">\\(f(x^{(t)})-f(x^*)\\)</span>\nis known to drop exponentially for gradient descent, ie</p>", "a[href=\"#convsetfig\"]": "<figure class=\"align-default\" id=\"convsetfig\" style=\"width: 15cm\">\n<img alt=\"../_images/convexsets.png\" src=\"../_images/convexsets.png\"/>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 3.1 </span><span class=\"caption-text\">A convex set in  (a) and (b); the set in (c) is not convex</span><a class=\"headerlink\" href=\"#convsetfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#sgdlem3\"]": "<div class=\"proof lemma admonition\" id=\"sgdlem3\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 3.11 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<div class=\"math notranslate nohighlight\">\n\\[\n\\epsilon_{t+1}\\leq\\epsilon_t(1-\\eta_t\\mu)+\\eta_t^2B\n\\]</div>\n</section>\n</div>", "a[href=\"#gradlem\"]": "<div class=\"proof lemma admonition\" id=\"gradlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 3.1 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f:C\\to\\mathbb R\\)</span> be a differentiable function on an open convex set <span class=\"math notranslate nohighlight\">\\(C\\subseteq \\mathbb R^n\\)</span>.\nThen <span class=\"math notranslate nohighlight\">\\(f\\)</span> is convex if and only if</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nf(y)\\geq f(x)+\\trsp{\\nabla f(x)}(y-x)\\text{ for all }x,y\\in C.\n\\]</div>\n</section>\n</div>", "a[href=\"#gradient-descent\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.5. </span>Gradient descent<a class=\"headerlink\" href=\"#gradient-descent\" title=\"Link to this heading\">#</a></h2><p>Some of the objective functions in machine learning are convex.\nHow can we minimise them? With <em>stochastic gradient descent</em> \u2013 it is this\nalgorithm (or one of its variants) that powers most of machine learning. Let\u2019s understand\nsimple <em>gradient descent</em> first.</p>", "a[href=\"#setafig\"]": "<figure class=\"align-default\" id=\"setafig\">\n<a class=\"reference internal image-reference\" href=\"../_images/sgd_three_runs.png\"><img alt=\"../_images/sgd_three_runs.png\" src=\"../_images/sgd_three_runs.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 3.4 </span><span class=\"caption-text\">Three runs of SGD with same constant learning rate. The function to be minimised\nis <span class=\"math notranslate nohighlight\">\\((x,y)\\mapsto \\tfrac{1}{2}(x^2+10y^2)\\)</span>, which is not of the type that is typical in machine learning.\nTo simulate SGD a normally distributed error is added to each gradient.</span><a class=\"headerlink\" href=\"#setafig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#analysis-of-sgd\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.7. </span>Analysis of SGD<a class=\"headerlink\" href=\"#analysis-of-sgd\" title=\"Link to this heading\">#</a></h2><p>If the loss function is convex then SGD converges, at least in expectation,\ntowards the global minimum, provided some additional mild conditions are satisfied.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id4\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>Based on <em>The convergence of the Stochastic Gradient Descent (SGD) : a self-contained proof</em>\nby G. Turinci, <a class=\"reference external\" href=\"https://arxiv.org/pdf/2103.14350.pdf\">arXiv:2103.14350</a></span>\nThere are a number of such convergence proofs, each with their own set of additional\nconditions. We assume here strong convexity.</p><p>Let <span class=\"math notranslate nohighlight\">\\(S\\)</span> be a training set, and let</p>", "a[href=\"#etafig\"]": "<figure class=\"align-default\" id=\"etafig\">\n<a class=\"reference internal image-reference\" href=\"../_images/gd_etas.png\"><img alt=\"../_images/gd_etas.png\" src=\"../_images/gd_etas.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 3.3 </span><span class=\"caption-text\">Gradient descent with constant learning rates of different values.\nThe function to be minimised is\n<span class=\"math notranslate nohighlight\">\\((x,y)\\mapsto \\tfrac{1}{2}(x^2+10y^2)\\)</span>. Middle: small learning\nrate leads to slow convergence. Right: learning rate is too large, no\nconvergence.</span><a class=\"headerlink\" href=\"#etafig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#strongdifflem\"]": "<div class=\"proof lemma admonition\" id=\"strongdifflem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 3.9 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f:K\\to\\mathbb R\\)</span> be a differentiable\nfunction on an open convex set <span class=\"math notranslate nohighlight\">\\(K\\subseteq\\mathbb R^d\\)</span>.\nThen <span class=\"math notranslate nohighlight\">\\(f\\)</span> is <span class=\"math notranslate nohighlight\">\\(\\mu\\)</span>-strongly convex if and only if for all <span class=\"math notranslate nohighlight\">\\(x,y\\in K\\)</span></p>\n<div class=\"math notranslate nohighlight\">\n\\[\nf(y)\\geq f(x)+\\nabla \\trsp{f(x)} (y-x)+\\frac{\\mu}{2}||y-x||^2\n\\]</div>\n</section>\n</div>", "a[href=\"#sumlem\"]": "<div class=\"proof lemma admonition\" id=\"sumlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 3.4 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(C\\subseteq\\mathbb R^n\\)</span> be a convex set, let <span class=\"math notranslate nohighlight\">\\(w_1,\\ldots, w_m\\geq 0\\)</span>,\nand\nlet <span class=\"math notranslate nohighlight\">\\(f_1,\\ldots,f_m:C\\to\\mathbb R\\)</span> be convex functions. Then <span class=\"math notranslate nohighlight\">\\(f=\\sum_{i=1}^mw_if_i\\)</span>\nis a convex function.</p>\n</section>\n</div>", "a[href=\"#sgdthm\"]": "<div class=\"proof theorem admonition\" id=\"sgdthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 3.2 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(S\\)</span> be a training set, and let</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nL:w \\mapsto\\frac{1}{|S|}\\sum_{(x,y)\\in S}L_{(x,y)}(w)\n\\]</div>\n<p>be a differentiable <span class=\"math notranslate nohighlight\">\\(\\mu\\)</span>-strongly convex loss function such that\nthere is a constant <span class=\"math notranslate nohighlight\">\\(B\\)</span> with\n<span class=\"math notranslate nohighlight\">\\(\\sup_{w}||\\nabla L_{(x,y)}(w)||^2\\leq B\\)</span> for all <span class=\"math notranslate nohighlight\">\\((x,y)\\in S\\)</span>.\nIf <span class=\"math notranslate nohighlight\">\\(\\eta_1\\geq \\eta_2\\geq\\ldots\\)</span> are\nsuch that\n<span class=\"math notranslate nohighlight\">\\(\\sum_{t=1}^\\infty\\eta_t=\\infty\\)</span>  but <span class=\"math notranslate nohighlight\">\\(\\sum_{t=1}^\\infty\\eta_t^2&lt;\\infty\\)</span>\nthen</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\expec_{1..t}\\left[||w^{(t+1)}-w^*||^2\\right]\\to 0\\text{ as }t\\to\\infty,\n\\]</div>\n<p>where <span class=\"math notranslate nohighlight\">\\(w^*\\)</span> is a global minimum of <span class=\"math notranslate nohighlight\">\\(L\\)</span>.</p>\n</section>\n</div>", "a[href=\"#gdtimefig\"]": "<figure class=\"align-default\" id=\"gdtimefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/sgd_time.png\"><img alt=\"../_images/sgd_time.png\" src=\"../_images/sgd_time.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 3.5 </span><span class=\"caption-text\">Comparison of  logistic loss per running time for batch gradient descent,\nand online and minibatch gradient descent.\nAll algorithms were applied to an artificial logistic regression problem. Minibatch size was equal\nto 20, total sample size was 5000. Online and minibatch SGD converge much faster than\nbatch gradient descent. Note the different scales of the axes.</span><a class=\"headerlink\" href=\"#gdtimefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#convex-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.3. </span>Convex functions<a class=\"headerlink\" href=\"#convex-functions\" title=\"Link to this heading\">#</a></h2><p>Which functions are convex?\nNorms are convex. Indeed, the function <span class=\"math notranslate nohighlight\">\\(x\\mapsto ||x||\\)</span> is convex as for every <span class=\"math notranslate nohighlight\">\\(\\lambda\\in [0,1]\\)</span>\nthe triangle inequality implies:</p>", "a[href=\"#sgdsec\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.6. </span>Stochastic gradient descent<a class=\"headerlink\" href=\"#sgdsec\" title=\"Link to this heading\">#</a></h2><p>Gradient descent is a quite efficient algorithm. Under mild assumptions and with the right\n(adaptable) learning rate it can be shown that the error <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>, the difference <span class=\"math notranslate nohighlight\">\\(f(\\overline x)-f(x^*)\\)</span>,\ndecreases exponentially with the number of iterations, i.e. that</p>", "a[href=\"#sgdlem4\"]": "<div class=\"proof lemma admonition\" id=\"sgdlem4\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 3.10 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<div class=\"math notranslate nohighlight\">\n\\[\n\\epsilon_{T'+k+1}\\leq \\epsilon_{T'}\\prod_{t=T'}^{T'+k}(1-\\eta_t\\mu)+\\sum_{t=T'}^{T'+k}\\eta_t^2B\n\\]</div>\n<p>as long as <span class=\"math notranslate nohighlight\">\\(1-\\eta_t\\mu\\geq 0\\)</span> for all <span class=\"math notranslate nohighlight\">\\(t\\in\\{T',\\ldots, T'+k\\}\\)</span>.</p>\n</section>\n</div>", "a[href=\"#equation-convdef\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-convdef\">\n\\[\\text{epi}(f)=\\{(x,y) : x\\in C,y\\geq f(x)\\}\\]</div>", "a[href=\"#equation-blubb1\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-blubb1\">\n\\[B\\sum_{t=T'}^\\infty \\eta_t^2\\leq\\tfrac{1}{2}\\delta\\]</div>", "a[href=\"#convex-optimisation-problems\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.2. </span>Convex optimisation problems<a class=\"headerlink\" href=\"#convex-optimisation-problems\" title=\"Link to this heading\">#</a></h2><p>A <em>convex optimisation problem</em> is any problem of the form</p>", "a[href=\"#strong-convexity\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.4. </span>Strong convexity<a class=\"headerlink\" href=\"#strong-convexity\" title=\"Link to this heading\">#</a></h2><p>Many of the functions we encounter in machine learning are at least locally convex,\nand usually these even exhibit a stronger notion of convexity that is called,\nwell, <em>strong</em> convexity. The difference between convexity and strong convexity\nis basically the difference between an affine function such as <span class=\"math notranslate nohighlight\">\\(x\\mapsto x\\)</span> and a\nquadratic function such as <span class=\"math notranslate nohighlight\">\\(x\\mapsto x^2\\)</span>. Affine functions are convex but barely so:\nthey satisfy the defining inequality of convexity <a class=\"reference internal\" href=\"#equation-convdef\">(3.1)</a> with equality. For\na strongly convex function this will never be the case.</p><p>A function <span class=\"math notranslate nohighlight\">\\(f:K\\to\\mathbb R\\)</span> on a convex set <span class=\"math notranslate nohighlight\">\\(K\\subseteq\\mathbb R^d\\)</span> is\n<em><span class=\"math notranslate nohighlight\">\\(\\mu\\)</span>-strongly convex</em> for <span class=\"math notranslate nohighlight\">\\(\\mu&gt;0\\)</span> if for all <span class=\"math notranslate nohighlight\">\\(\\lambda\\in [0,1]\\)</span>\nand <span class=\"math notranslate nohighlight\">\\(x,y\\in K\\)</span> it holds that</p>", "a[href=\"#equation-sgd1\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-sgd1\">\n\\[\\epsilon_{t+1} \\leq \\epsilon_t -2\\eta_t\\expec_{1..t}\\left[\\trsp{(w^{(t)}-w^*)}\\nabla^{(t)}\\right]+\\eta_t^2 B\\]</div>", "a[href=\"#gdalg\"]": "<div class=\"proof algorithm admonition\" id=\"gdalg\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Algorithm 3.1 </span> (gradient descent)</p>\n<section class=\"algorithm-content\" id=\"proof-content\">\n<p><strong>Instance</strong> A differentiable function <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R\\)</span>, a first point <span class=\"math notranslate nohighlight\">\\(x^{(1)}\\)</span>.<br/>\n<strong>Output</strong> A point <span class=\"math notranslate nohighlight\">\\(x\\)</span>.</p>\n<ol class=\"arabic simple\">\n<li><p>Set <span class=\"math notranslate nohighlight\">\\(t=1\\)</span>.</p></li>\n<li><p><strong>while</strong> stopping criterion not satisfied:</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute <span class=\"math notranslate nohighlight\">\\(\\nabla f(x^{(t)})\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute learning rate <span class=\"math notranslate nohighlight\">\\(\\eta_t\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set <span class=\"math notranslate nohighlight\">\\(x^{(t+1)}=x^{(t)}-\\eta_t\\nabla f(x^{(t)})\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set <span class=\"math notranslate nohighlight\">\\(t=t+1\\)</span>.</p></li>\n<li><p><strong>output</strong> <span class=\"math notranslate nohighlight\">\\(x^{(t)}\\)</span>, or best of <span class=\"math notranslate nohighlight\">\\(x^{(1)},\\ldots, x^{(t)}\\)</span>, or average.</p></li>\n</ol>\n</section>\n</div>", "a[href=\"#stochastic-gradient-descent\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3. </span>Stochastic gradient descent<a class=\"headerlink\" href=\"#stochastic-gradient-descent\" title=\"Link to this heading\">#</a></h1><p>How is a neural network trained? How can we minimise logistic loss in order\nto learn the parameters of a logistic regression? Both cases reduce to an\noptimisation problem that requires a numerical optimisation algorithm, often a variant\nof a gradient descent technique. In the nicest and simplest setting, a convex optimisation\nproblem, these are even guaranteed to find an optimal solution.</p>", "a[href=\"#equation-convopt\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-convopt\">\n\\[\\inf f(x),\\quad x\\in K\\]</div>", "a[href=\"#equation-strongmin2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-strongmin2\">\n\\[f(y)-f(x)\\geq \\frac{\\mu}{2}||y-x||^2\\]</div>", "a[href=\"#convexity\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.1. </span>Convexity<a class=\"headerlink\" href=\"#convexity\" title=\"Link to this heading\">#</a></h2><p>A set <span class=\"math notranslate nohighlight\">\\(C\\subseteq\\mathbb R^n\\)</span> is <em>convex</em> if the connecting segment\nbetween any two points in <span class=\"math notranslate nohighlight\">\\(C\\)</span> is also contained in <span class=\"math notranslate nohighlight\">\\(C\\)</span>:</p>", "a[href=\"#convfunfig\"]": "<figure class=\"align-default\" id=\"convfunfig\" style=\"width: 15cm\">\n<img alt=\"../_images/convexconcave.png\" src=\"../_images/convexconcave.png\"/>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 3.2 </span><span class=\"caption-text\">A convex function, a concave function (negative of a convex function), and a function that is neither convex nor concave.</span><a class=\"headerlink\" href=\"#convfunfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-blubb2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-blubb2\">\n\\[\\text{exp}\\left(\\sum_{t=T'}^{T'+k_0}\\eta_t\\mu\\right)\\geq \\frac{2\\epsilon_{T'}}{\\delta}\\]</div>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
