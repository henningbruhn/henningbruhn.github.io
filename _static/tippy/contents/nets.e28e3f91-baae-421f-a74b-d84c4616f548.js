selector_to_html = {"a[href=\"#id12\"]": "<figure class=\"align-left\" id=\"id12\">\n<a class=\"reference internal image-reference\" href=\"../_images/blurry_Leopold_Kronecker_1865.jpg\"><img alt=\"../_images/blurry_Leopold_Kronecker_1865.jpg\" src=\"../_images/blurry_Leopold_Kronecker_1865.jpg\" style=\"width: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.7 </span><span class=\"caption-text\">blurry Kronecker</span><a class=\"headerlink\" href=\"#id12\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#local-minima\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.6. </span>Local minima<a class=\"headerlink\" href=\"#local-minima\" title=\"Link to this heading\">#</a></h2><p>Neural networks are trained with SGD, or with a variant of SGD.\nWhy does that work at all? The loss functions is highly\nnon-convex. Indeed, because of the symmetries in the weights it is almost guaranteed that\nthere will be many local minima: permuting the nodes of a layer (together with their weights)\nwill turn one local minimum into another one.</p><p>Why doesn\u2019t SGD regularly become trapped in a local minimum that is far away from the\nglobal minimum?<br/>\nWhy are local minima not a problem?</p>", "a[href=\"#otimeslem\"]": "<div class=\"proof lemma admonition\" id=\"otimeslem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 4.1 </span> (Allman, Matias and Rhodes (2009))</p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(k,\\ell,N\\)</span> be integers with <span class=\"math notranslate nohighlight\">\\(k\\ell\\geq N\\)</span>. Then\nfor almost all <span class=\"math notranslate nohighlight\">\\((A,B)\\)</span> with\n<span class=\"math notranslate nohighlight\">\\(A\\in\\mathbb R^{k\\times N}\\)</span> and <span class=\"math notranslate nohighlight\">\\(B\\in\\mathbb R^{\\ell\\times N}\\)</span>\nit holds for the Khatri-Rao product that</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\rank(A\\circ B)=N\n\\]</div>\n</section>\n</div>", "a[href=\"#id11\"]": "<figure class=\"align-left\" id=\"id11\">\n<a class=\"reference internal image-reference\" href=\"../_images/information_small.png\"><img alt=\"../_images/information_small.png\" src=\"../_images/information_small.png\" style=\"width: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.4 </span><span class=\"caption-text\">midjourney on information</span><a class=\"headerlink\" href=\"#id11\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-updatedelta\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-updatedelta\">\n\\[\\delta^{(k)}_h = \\sum_{i=1}^{n_{k+1}} w^{(k+1)}_{ih}\\cdot \\delta^{(k+1)}_i\\cdot \\sigma'_k(g^{(k)}_h)\\]</div>", "a[href=\"#equation-netloss\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-netloss\">\n\\[L(a)=\\frac{1}{|S|}\\sum_{(x,y)\\in S}L_{(x,y)}(a)\\]</div>", "a[href=\"#trainvarfig\"]": "<figure class=\"align-default\" id=\"trainvarfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/locmin.png\"><img alt=\"../_images/locmin.png\" src=\"../_images/locmin.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.6 </span><span class=\"caption-text\">Distribution of the training error for 100 runs of SGD in one hidden layer neural networks of\ndifferent sizes. All neural networks were trained on Fashion MNIST with the same training set of size 5000.\nFor over-parameterised\nnetworks (100 hidden nodes) training error shows smaller variance, while the variance\nis larger for fewer hidden nodes. Models were trained until training loss no longer improved.</span><a class=\"headerlink\" href=\"#trainvarfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#no-traps\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.7. </span>No traps<a class=\"headerlink\" href=\"#no-traps\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s try again to argue that local minima are unlikely to trap\nSGD when training a neural network.\nIn the setting we will consider, there are more weights, more parameters of the\nneural network, than points in the training set.\nThis is not unusual. Alexnet, a well-known\nneural network for image recognition had about 60 million parameters but was trained\non just 1.2 million datapoints.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-8\"></label><input class=\"margin-toggle\" id=\"marginnote-role-8\" name=\"marginnote-role-8\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/no_traps.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>no_traps</a></span></p>", "a[href=\"#equation-mselss2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-mselss2\">\n\\[\\text{MSE} =\\frac{1}{N}\\sum_{n=1}^N (y^{(n)}-\\trsp{(a^{(n)})} Wx^{(n)} )^2\\]</div>", "a[href=\"#softmax-and-loss\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.3. </span>Softmax and loss<a class=\"headerlink\" href=\"#softmax-and-loss\" title=\"Link to this heading\">#</a></h2><p>The preceding discussion applied to binary classification and a single output\nnode. What do we do when we have more classes? First, we\u2019ll have more outputs, namely\none for each class. Second, we\u2019ll use a softmax layer at the output.</p><p>Assume we do image classification\nwith three classes: cat, pumpkin and bird. Then there will be three output neurons.\nIf a training sample <span class=\"math notranslate nohighlight\">\\((x,y)\\)</span> shows a cat, then the class vector <span class=\"math notranslate nohighlight\">\\(y\\)</span> will be <span class=\"math notranslate nohighlight\">\\(y=(1,0,0\\trsp )\\)</span>.\nIf the image shows a pumpkin, we\u2019ll have <span class=\"math notranslate nohighlight\">\\(y=(0,1,0\\trsp )\\)</span>, and if it\u2019s a bird then\n<span class=\"math notranslate nohighlight\">\\(y=(0,0,1\\trsp )\\)</span>. This way of encoding the classes even has a fancy name: <em>one-hot encoding</em>,\nbecause there is always just one bit that is <em>hot</em>, ie, equal to 1.</p>", "a[href=\"#equation-8c11a3ba-5b66-45ec-8aa4-aa158fc9c33c\"]": "<div class=\"amsmath math notranslate nohighlight\" id=\"equation-8c11a3ba-5b66-45ec-8aa4-aa158fc9c33c\">\n\\[\\begin{equation}\n\\delta^{(k)}=\\trsp{(W^{(k+1)})}\\delta^{(k+1)}\\odot \\sigma'_k(g^{(k)})\n\\end{equation}\\]</div>", "a[href=\"#slowfig\"]": "<figure class=\"align-default\" id=\"slowfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/slow.png\"><img alt=\"../_images/slow.png\" src=\"../_images/slow.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.2 </span><span class=\"caption-text\">Gradient <span class=\"math notranslate nohighlight\">\\(\\frac{\\partial L_{(x,y)}}{\\partial b^{(K)}_{1}} \\)</span> for square loss.\nWe assume that <span class=\"math notranslate nohighlight\">\\(y=0\\)</span>. Marked: gradient when the network is very wrong.</span><a class=\"headerlink\" href=\"#slowfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-gradb\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-gradb\">\n\\[\\frac{\\partial L_{(x,y)}}{\\partial b^{(k)}_{h}} = \n\\frac{\\partial L_{(x,y)}}{\\partial g^{(k)}_{h}}\\cdot \n\\frac{\\partial g^{(k)}_{h}}{\\partial b^{(k)}_{h}}\n= \\delta^{(k)}_h\\]</div>", "a[href=\"#klfig\"]": "<figure class=\"align-default\" id=\"klfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/KL.png\"><img alt=\"../_images/KL.png\" src=\"../_images/KL.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.5 </span><span class=\"caption-text\">Comparison of Kullback-Leibler divergence for varying values of coin toss probability <span class=\"math notranslate nohighlight\">\\(\\nu\\)</span>.\nFair coin toss denoted by <span class=\"math notranslate nohighlight\">\\(p\\)</span>, while under <span class=\"math notranslate nohighlight\">\\(q\\)</span> head occurs with probability <span class=\"math notranslate nohighlight\">\\(\\nu\\)</span>.</span><a class=\"headerlink\" href=\"#klfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-kldef\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-kldef\">\n\\[\\KL(p||q)=\\sum_{x} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right)\n= \\expec_p[\\log p(x)] - \\expec_p[\\log q(x)]\\]</div>", "a[href=\"#back-propagation\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.1. </span>Back propagation<a class=\"headerlink\" href=\"#back-propagation\" title=\"Link to this heading\">#</a></h2><p>To train the neural network, we need to specify a loss function. Historically this was often the\n<em>square loss</em>. That is, given a training set <span class=\"math notranslate nohighlight\">\\(S\\subseteq \\mathcal X\\times\\mathcal Y\\)</span>\nthe empirical risk was taken to be</p>", "a[href=\"#equation-delta\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-delta\">\n\\[\\delta^{(k)}=\\nabla_{g^{(k)}}L_{(x,y)} = \\trsp{\\left(\n\\frac{\\partial L_{(x,y)}}{\\partial g^{(k)}_1},\\ldots,\\frac{\\partial L_{(x,y)}}{\\partial g^{(k)}_{n_k}}\n\\right)}\\]</div>", "a[href=\"#equation-deltak\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-deltak\">\n\\[\\delta^{(K)}_1:=\\frac{\\partial L_{(x,y)}}{\\partial g^{(K)}_1} = \n\\frac{\\partial L_{(x,y)}}{\\partial f^{(K)}_{1}}\n\\cdot  \\frac{\\partial f^{(K)}_{1}}{\\partial g^{(K)}_1}\n= \\frac{\\partial L_{(x,y)}}{\\partial f^{(K)}_{1}} \\cdot \\sigma_K'(g^{(K)}_1)\\]</div>", "a[href=\"#why-cross-entropy-loss\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.4. </span>Why cross-entropy loss?<a class=\"headerlink\" href=\"#why-cross-entropy-loss\" title=\"Link to this heading\">#</a></h2><p>Cross-entropy loss seems to make sense and certainly increases the more bad predictions are made \u2013 still\nit may appear a bit arbitrary. It\u2019s not. We take a step back and figure out where cross-entropy loss is coming from.</p><p><em>Cross-entropy</em> and entropy more generally is a concept from information theory. Cross-entropy, in information\ntheory, compares two probability distributions <span class=\"math notranslate nohighlight\">\\(p,q\\)</span>:</p>", "a[href=\"#equation-f714f2bc-676d-46f9-b5ee-c24fee91466c\"]": "<div class=\"amsmath math notranslate nohighlight\" id=\"equation-f714f2bc-676d-46f9-b5ee-c24fee91466c\">\n\\[\\begin{equation}\\label{ceq}\nL=-\\frac{1}{|S|}\\sum_{(x,y)\\in S}\\log(q(y|x))\n\\end{equation}\\]</div>", "a[href=\"#neural-networks\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4. </span>Neural networks<a class=\"headerlink\" href=\"#neural-networks\" title=\"Link to this heading\">#</a></h1><p>Let\u2019s train a neural network.<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup>A very good source for this \u2013 and other aspects of neural networks! \u2013 is Michael Nielsen\u2019s <a class=\"reference external\" href=\"http://neuralnetworksanddeeplearning.com\">website</a></span>\nNormally, SGD or one of its more advanced cousins is used\nto train neural networks. The main issue is how to compute the gradient as efficiently as\npossible. But let\u2019s not get ahead of ourselves.</p><p>The neural network we consider has <span class=\"math notranslate nohighlight\">\\(K\\)</span> layers, each layer <span class=\"math notranslate nohighlight\">\\(k\\)</span> consisting of <span class=\"math notranslate nohighlight\">\\(n_k\\)</span> nodes.\nThe last layer may either consist of a single node (for binary classification) or\nof several nodes. Normally, the activation function of any hidden layer\nis ReLU (<span class=\"math notranslate nohighlight\">\\(x\\mapsto \\max(0,x)\\)</span>) or leaky ReLU  <span class=\"math notranslate nohighlight\">\\((x\\mapsto \\max(\\alpha x,x)\\)</span> for <span class=\"math notranslate nohighlight\">\\(\\alpha\\in (0,1)\\)</span>),\nwhile the activation layer for the output layer is either the logistic function\n(for a single output node) or softmax (for several output nodes). We will simply\nwrite <span class=\"math notranslate nohighlight\">\\(\\sigma_k\\)</span> for the activation function of layer <span class=\"math notranslate nohighlight\">\\(k\\)</span>, and I will pretend\nthat <span class=\"math notranslate nohighlight\">\\(\\sigma_k\\)</span> is differentiable \u2013 even though this is clearly a lie. Let me discuss\nthis issue later.</p>", "a[href=\"#netfig\"]": "<figure class=\"align-default\" id=\"netfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/net.png\"><img alt=\"../_images/net.png\" src=\"../_images/net.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.1 </span><span class=\"caption-text\">A classfication ReLU-neural network</span><a class=\"headerlink\" href=\"#netfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-mselss\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-mselss\">\n\\[\\text{MSE}=\\frac{1}{N}\\sum_{n=1}^N (y^{(n)}-W^{(2)}\\diag(a^{(n)}) \\cdot W^{(1)}x^{(n)} )^2\\]</div>", "a[href=\"#equation-sqbw\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-sqbw\">\n\\[\\begin{split}\\frac{\\partial L_{(x,y)}}{\\partial w^{(K)}_{1i}}\n &amp; = 2( f^{(K)}_{1} -y )\\cdot f^{(K)}_1(1-f^{(K)}_1) \\cdot f^{(K-1)}_{i} \\\\\n\\frac{\\partial L_{(x,y)}}{\\partial b^{(K)}_{1}} &amp; = 2( f^{(K)}_{1} -y )\\cdot f^{(K)}_1(1-f^{(K)}_1) \\end{split}\\]</div>", "a[href=\"#kullback-leibler-divergence\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.5. </span>Kullback-Leibler divergence<a class=\"headerlink\" href=\"#kullback-leibler-divergence\" title=\"Link to this heading\">#</a></h2><p>The <em>Kullback-Leibler divergence</em> measures how different two probability distributions are.\nFor the definition let us consider two discrete probability distributions <span class=\"math notranslate nohighlight\">\\(p\\)</span> and <span class=\"math notranslate nohighlight\">\\(q\\)</span>\n(the general case is similar but involves integrals and measure-theoretic caveats). Then\nthe  Kullback-Leibler divergence between <span class=\"math notranslate nohighlight\">\\(p\\)</span> and <span class=\"math notranslate nohighlight\">\\(q\\)</span> is defined as</p>", "a[href=\"#softfig\"]": "<figure class=\"align-default\" id=\"softfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/soft.png\"><img alt=\"../_images/soft.png\" src=\"../_images/soft.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 4.3 </span><span class=\"caption-text\">How a softmax output layer works</span><a class=\"headerlink\" href=\"#softfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#the-loss-function\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">4.2. </span>The loss function<a class=\"headerlink\" href=\"#the-loss-function\" title=\"Link to this heading\">#</a></h2><p>Historically neural networks were trained with the square loss. Indeed, in a lot of applications\nwe are interested in the mean square error, so why not use it for neural networks, too?\nIt turns out that square loss might lead to slow learning \u2013 in classification.</p><p>Recall that the square loss is defined as</p>", "a[href=\"#equation-gradw\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-gradw\">\n\\[\\frac{\\partial L_{(x,y)}}{\\partial w^{(k)}_{hi}} = \n\\frac{\\partial L_{(x,y)}}{\\partial g^{(k)}_{h}}\\cdot \n\\frac{\\partial g^{(k)}_{h}}{\\partial w^{(k)}_{hi}}\n= \\delta^{(k)}_h\\cdot f^{(k-1)}_i\\]</div>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,

            });
        };
    };
    console.log("tippy tips loaded!");
};
