selector_to_html = {"a[href=\"#ensemble-learning\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8. </span>Ensemble learning<a class=\"headerlink\" href=\"#ensemble-learning\" title=\"Link to this heading\">#</a></h1><p>In <em>Who wants to be a millionaire?</em> contestants have the option to ask the audience\nfor help with a quiz question. Often this successful: the majority vote indicates the right answer.\nThe interesting feature\nhere is that obviously the audience of game shows does not usually consist of experts.\nOn the contrary, the typical audience member  is arguably more ignorant than the contestants,\nwho have already proved their merit by clearing the pre-selection process. Still,\ncollectively, the audience is relatively strong.\nThis phenomenon is called <em>wisdom of the crowd</em>.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-1\"></label><input class=\"margin-toggle\" id=\"marginnote-role-1\" name=\"marginnote-role-1\" type=\"checkbox\"/><span class=\"marginnote\"> There seems to have been written a lot about the supposed\nwisdom of the crowd, even a whole book. Sometimes, though, the crowd is dead-wrong:\nThe 80s, for example, were terrible and there\u2019s no reason to celebrate them.</span></p><p><em>Ensemble learning</em> combines several weak\npredictors to a strong predictor. Broadly, there are two ways to do that:</p>", "a[href=\"#wisdom-of-the-crowd\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.1. </span>Wisdom of the crowd<a class=\"headerlink\" href=\"#wisdom-of-the-crowd\" title=\"Link to this heading\">#</a></h2><p>An easy way to increase the performance in a classification task is\nto train several classifiers and then let them decide by majority voting.</p><p>To gain a first insight, consider a binary classification task and\nassume that we have access to <span class=\"math notranslate nohighlight\">\\(T\\)</span> classifiers\n<span class=\"math notranslate nohighlight\">\\(h_1,\\ldots, h_T\\)</span> that each have a probability of <span class=\"math notranslate nohighlight\">\\(p&gt;\\tfrac{1}{2}\\)</span>\nto classify a randomly drawn data point correctly (we assume here that\nthe class <span class=\"math notranslate nohighlight\">\\(y\\)</span> is completely determined by <span class=\"math notranslate nohighlight\">\\(x\\)</span>). Assume, furthermore,\nthat the classifiers are stochastically independent. (Clearly,\nthis is an unrealistic assumption.) Then the probability that\nthe majority vote decides wrongly is</p>", "a[href=\"#dependent-classifiers\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.3. </span>Dependent classifiers<a class=\"headerlink\" href=\"#dependent-classifiers\" title=\"Link to this heading\">#</a></h2><p>After this short digression, we come back to a collective of interdependent classifiers.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id3\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>Based on <em>The Condorcet Jury Theorem, Free Speech, and Correlated Votes</em>, K.K. Ladha (1992)</span>\nAssume that we have trained <span class=\"math notranslate nohighlight\">\\(n\\)</span> classifiers <span class=\"math notranslate nohighlight\">\\(h_i:\\mathcal X\\to\\mathcal Y\\)</span>, and we let <span class=\"math notranslate nohighlight\">\\(h\\)</span>\nbe the majority classifier</p>", "a[href=\"#short-stochastic-digression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.2. </span>Short stochastic digression<a class=\"headerlink\" href=\"#short-stochastic-digression\" title=\"Link to this heading\">#</a></h2><p>To cope with a collective of classifiers with stochastic interdependencies\nwe need a stochastic tool, a sort of  one-sided\nChebyshev\u2019s inequality.</p><p>For comparison, let\u2019s recall Chebyshev\u2019s inequality.\nFor this let <span class=\"math notranslate nohighlight\">\\(X\\)</span> be a random variable, and recall the definition\nof the <em>variance</em> of a random variable:</p>", "a[href=\"#equation-canteq\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-canteq\">\n\\[ \\begin{align}\\begin{aligned}\\proba[\\bar X&gt;\\tfrac{1}{2}]  \n= 1-\\proba[\\bar X\\leq \\bar p-\\delta] \n \\geq 1-\\frac{\\sigma^2}{\\sigma^2+\\delta^2} = \\frac{\\delta^2}{\\sigma^2+\\delta^2}\n``'\\\\\\begin{split}So, let's compute the variance $\\sigma^2$ of $\\bar X$. \n\\begin{align*}\n\\sigma^2 &amp; = \\expec[\\bar X^2]-\\expec[\\bar X]^2 \n=\\frac{1}{n^2}\\sum_{i,j=1}^n\\expec[X_iX_j]-\\bar p^2\\\\\n&amp; = \\frac{1}{n^2}\\sum_{i=1}^n\\expec[X_i]+\\frac{1}{n^2}\\sum_{i\\neq j}\\expec[X_iX_j]-\\bar p^2\\\\\n&amp; = \\frac{\\bar p}{n} -\\bar p^2 + \\frac{1}{n^2}\\sum_{i\\neq j}\\expec[X_iX_j]\n\\end{align*}\\end{split}\\\\The thing to note there is that the term  $\\expec[X_iX_j]$ makes up an essential part\nof the \\defi{correlation coefficient} of the random variables $X_i,X_j$:\\\\$$\n\\text{corr}(X_i,X_j)=\\frac{\\expec[X_iX_j]-\\expec[X_i]\\expec[X_j]}{\\sqrt{\\vari[X_i]\\vari[X_j]}}\n$$\\\\The correlation coefficient takes values in $[-1,1]$ and \n is a measure of how much $X_i$ and $X_j$ influence each other. \nIn particular, stochastically independent variables have correlation coefficient 0.\n(However, correlation 0 does not necessarily imply stochastic independence.)\\\\In our setting, we can expect positive correlation. Indeed, if one classifier \ndetects the true class, then another one seems more likely to do so, too -- perhaps \nbecause they both decide because of the same feature. \n%%% this is correct: note that \\expec[X_iX_j]=\\proba[X_i=1\\text{ and }X_j=1]\n%%% thus we need to see whether it's more likely that both are correct\n%%% than would happen if both were independent\\\\\nTo simplify a little bit, let us assume that all $X_i$ have the same expectation, \nwhich is then equal to $\\bar p$. Then $\\proba[X_i=1]=\\bar p$ \nand $\\vari[X_i]=\\bar p(1-\\bar p)$, and thus \\\\$$\n\\sqrt{\\vari[X_i]\\vari[X_j]} = \\bar p(1-\\bar p)\n$$\\\\We also \n write $c$ for the average correlation coefficient\n$$\nc=\\frac{1}{n(n-1)}\\sum_{i\\neq j}\\text{corr}(X_i,X_j)\n$$\\\\\\begin{split}We get from above:\n\\begin{align*}\n\\sigma^2 &amp; = \\frac{\\bar p}{n} -\\bar p^2 + \n\\frac{1}{n^2}\\sum_{i\\neq j}\\left(\\expec[X_iX_j]-\\bar p^2+\\bar p^2\\right) \\\\\n&amp; = \\frac{\\bar p}{n} -\\bar p^2 +  \\frac{n-1}{n}c\\bar p(1-\\bar p)+\\frac{n(n-1)}{n^2}\\bar p^2\\\\\n&amp; = \\frac{\\bar p-\\bar p^2}{n}+\\frac{n-1}{n}c\\bar p(1-\\bar p)\n\\end{align*}\nLet me point out here that we now have obtained a complicated proof of Condorcet's\njury theorem. Indeed, if $c=0$ then $\\sigma^2\\to 0$ for $n\\to\\infty$, which together\nwith {eq}`canteq` yields $\\proba[\\bar X&gt;\\tfrac{1}{2}]\\to 1$. \\end{split}\\\\If $c&gt;0$, however, it no longer follows that a correct majority decision tends to \ncertainty with increasing $n$, and indeed, this is sometimes not the case. (Recall\nthe clone collective.)\\end{aligned}\\end{align} \\]</div>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
