selector_to_html = {"a[href=\"#univ2thm\"]": "<div class=\"proof theorem admonition\" id=\"univ2thm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 5.7 </span> (Lu et al.)</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R\\)</span> be a Lebesque-measurable function and let <span class=\"math notranslate nohighlight\">\\(\\epsilon&gt;0\\)</span>. Then there is a\nfully connected ReLU network of maximum width at most <span class=\"math notranslate nohighlight\">\\(n+4\\)</span> (but possibly large depth) such that\nthe function <span class=\"math notranslate nohighlight\">\\(N\\)</span> represented by the network achieves</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\int_{\\mathbb R^n}|f(x)-N(x)|dx\\leq \\epsilon\n\\]</div>\n</section>\n</div>", "a[href=\"#nnpiecesthm\"]": "<div class=\"proof theorem admonition\" id=\"nnpiecesthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 5.2 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(\\mathcal N\\)</span> be a (leaky) ReLU network with one input, one output node and <span class=\"math notranslate nohighlight\">\\(L-1\\)</span> hidden layers such that\nlayer <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span> has <span class=\"math notranslate nohighlight\">\\(n_\\ell\\)</span> neurons. Set <span class=\"math notranslate nohighlight\">\\(\\overline n=\\sum_{\\ell=1}^Ln_\\ell\\)</span>.\nThen <span class=\"math notranslate nohighlight\">\\(\\mathcal N\\)</span> compute a piecewise affine function with\npiece-number at most</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n2^L\\prod_{\\ell=1}^Ln_\\ell \\leq \\left(\\frac{2\\overline n}{L}\\right)^L\n\\]</div>\n</section>\n</div>", "a[href=\"#universal-approximators\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.2. </span>Universal approximators<a class=\"headerlink\" href=\"#universal-approximators\" title=\"Link to this heading\">#</a></h2><p>How expressive are neural networks? We have already seen above that every Boolean function\ncan be realised as a ReLU network. What about more complicated functions? Since ReLU networks\nencode piecewise affine functions such a network cannot reproduce any function that is not\npiecewise affine \u2013 but can every piecewise affine function be realised? Yes!</p><p>Let\u2019s define the <em>depth of a neural network</em> as the number of its layers not counting the input\nlayer. For example, a shallow neural network with input layer, one hidden layer and output layer\nhas depth 2. Let\u2019s say that the neural network has <em>width</em> at most <span class=\"math notranslate nohighlight\">\\(d\\)</span> if no layer,\nexcept possibly for the input layer,\nhas more than <span class=\"math notranslate nohighlight\">\\(d\\)</span> neurons.<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup><em>Universal function approximation by deep neural nets with bounded width\nand ReLU activation</em>, B. Hanin (2017), <a class=\"reference external\" href=\"https://arxiv.org/abs/1708.02691\">arXiv:1708.02691</a></span></p>", "a[href=\"#equation-numtrigs\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-numtrigs\">\n\\[|\\mathcal T|=2^\\ell-1=2^{L^2+4}-1&gt;2^{L^2+3}\\]</div>", "a[href=\"#smallmaxlem\"]": "<div class=\"proof lemma admonition\" id=\"smallmaxlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.5 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>The function</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\mathbb R^2\\to\\mathbb R,\\,\n\\twovec{x_1}{x_2}\\mapsto \\max(x_1,x_2)\n\\]</div>\n<p>can be computed by a ReLU network of depth <span class=\"math notranslate nohighlight\">\\(2\\)</span> and width <span class=\"math notranslate nohighlight\">\\(3\\)</span>.</p>\n</section>\n</div>", "a[href=\"#relu-networks-and-piecewise-affine-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.1. </span>ReLU networks and piecewise affine functions<a class=\"headerlink\" href=\"#relu-networks-and-piecewise-affine-functions\" title=\"Link to this heading\">#</a></h2><p>We\u2019ve trained and studied neural networks \u2013 but what kind of functions do they actually encode?\nIt turns out, ReLU networks encode quite simple functions, namely piecewise affine functions.</p><p>A function <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R^m\\)</span> is <em>piecewise affine</em> if there\nare finitely many polyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\subseteq\\mathbb R^n\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n=\\bigcup_{i=1}^s\\ph Q_i\\)</span> and <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span>\nis an affine function for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots, s\\)</span>. The\npolyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_i\\)</span> are the <em>pieces</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.\nThe smallest number of pieces <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span> is affine for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots,s\\)</span>\nis the <em>piece-number</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.</p>", "a[href=\"#NNmaxlem\"]": "<div class=\"proof lemma admonition\" id=\"NNmaxlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.6 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(g_1,\\ldots,g_N:\\mathbb R^n\\to\\mathbb R\\)</span> be  functions that can be computed by\nlinear/ReLU networks of depth <span class=\"math notranslate nohighlight\">\\(k\\)</span> and width <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span>. Then <span class=\"math notranslate nohighlight\">\\(\\max_{i=1,\\ldots, N} g_i(x)\\)</span> can be computed\nby a linear/ReLU network of depth <span class=\"math notranslate nohighlight\">\\(N(k+2)\\)</span> and width <span class=\"math notranslate nohighlight\">\\(\\max\\{n+3,n+\\ell+2\\}\\)</span>.</p>\n</section>\n</div>", "a[href=\"#Haninthm\"]": "<div class=\"proof theorem admonition\" id=\"Haninthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 5.4 </span> (Hanin (2017))</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R\\)</span> be a continuous piecewise affine function.\nIf</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nf(x)=\\max_{i=1,\\ldots,N}g_i(x)-\\max_{j=1,\\ldots,N}h_j(x)\\text{ for all }x\\in\\mathbb R^n,\n\\]</div>\n<p>where the <span class=\"math notranslate nohighlight\">\\(g_i\\)</span> and <span class=\"math notranslate nohighlight\">\\(h_j\\)</span> are affine functions,\nthen exists a ReLU neural network <span class=\"math notranslate nohighlight\">\\(F\\)</span> with linear output layer of depth at most <span class=\"math notranslate nohighlight\">\\(3N\\)</span>\nand width <span class=\"math notranslate nohighlight\">\\(4n+12\\)</span> such that <span class=\"math notranslate nohighlight\">\\(F(x)=f(x)\\)</span> for all <span class=\"math notranslate nohighlight\">\\(x\\in\\mathbb R^n\\)</span>.</p>\n</section>\n</div>", "a[href=\"#sawfig\"]": "<figure class=\"align-default\" id=\"sawfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/sawtooth.png\"><img alt=\"../_images/sawtooth.png\" src=\"../_images/sawtooth.png\" style=\"width: 12cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 5.2 </span><span class=\"caption-text\"><span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span>, on the left. On the right: <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> iterated three times.</span><a class=\"headerlink\" href=\"#sawfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-sawrelu\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-sawrelu\">\n\\[\\Delta(x) = 2(\\text{ReLU}(x)-2\\text{ReLU}(x-\\tfrac{1}{2}) + \\text{ReLU}(x-1))\\]</div>", "a[href=\"#fig-sawtrigs\"]": "<figure class=\"align-default\" id=\"fig-sawtrigs\">\n<a class=\"reference internal image-reference\" href=\"../_images/sawtooth3.png\"><img alt=\"../_images/sawtooth3.png\" src=\"../_images/sawtooth3.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 5.3 </span><span class=\"caption-text\">Triangles missed by second affine function in grey. Function realised by <span class=\"math notranslate nohighlight\">\\(\\mathcal N\\)</span> in red.</span><a class=\"headerlink\" href=\"#fig-sawtrigs\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#univthm\"]": "<div class=\"proof theorem admonition\" id=\"univthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 5.5 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R\\)</span> be a continuous function, and let <span class=\"math notranslate nohighlight\">\\(C\\subseteq\\mathbb R^n\\)</span> be compact.\nThen, for every <span class=\"math notranslate nohighlight\">\\(\\epsilon&gt;0\\)</span>, there is a ReLU neural network with linear output layer that computes\na function <span class=\"math notranslate nohighlight\">\\(F:\\mathbb R^n\\to\\mathbb R\\)</span> such that</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\max_{x\\in C}|f(x)-F(x)|&lt;\\epsilon\n\\]</div>\n</section>\n</div>", "a[href=\"#foolthm\"]": "<div class=\"proof theorem admonition\" id=\"foolthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 5.9 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(N\\)</span> be  a ReLU network with  softmax output layer, and\nlet <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R^K\\)</span> be the piecewise affine function\nsuch that <span class=\"math notranslate nohighlight\">\\(N\\)</span> computes the function <span class=\"math notranslate nohighlight\">\\(\\softmax\\circ f\\)</span>, where <span class=\"math notranslate nohighlight\">\\(\\softmax\\)</span> is the\nsoftmax function.\nLet <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots, \\ph Q_s\\)</span> be the pieces of <span class=\"math notranslate nohighlight\">\\(f\\)</span>, and let\n<span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_t}\\)</span> be described by the affine function <span class=\"math notranslate nohighlight\">\\(x\\mapsto A^{(t)}x+b^{(t)}\\)</span>.\nIf, for all <span class=\"math notranslate nohighlight\">\\(t=1,\\ldots, s\\)</span>, the matrix <span class=\"math notranslate nohighlight\">\\(A^{(t)}\\)</span> does not have identical\nrows then for almost every <span class=\"math notranslate nohighlight\">\\(x\\in\\mathbb R^n\\)</span> there exists\na class <span class=\"math notranslate nohighlight\">\\(k\\in\\{1,\\ldots,K\\}\\)</span> such that\nthe confidence of <span class=\"math notranslate nohighlight\">\\(N\\)</span> that <span class=\"math notranslate nohighlight\">\\(\\alpha x\\)</span> lies in <span class=\"math notranslate nohighlight\">\\(k\\)</span> tends to 1\nas <span class=\"math notranslate nohighlight\">\\(\\alpha\\to\\infty\\)</span>, ie</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\lim_{\\alpha\\to\\infty}\\frac{e^{f_k(\\alpha x)}}{\\sum_{\\ell=1}^Ke^{f_\\ell(\\alpha x)}} = 1\n\\]</div>\n</section>\n</div>", "a[href=\"#properties-of-neural-networks\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5. </span>Properties of neural networks<a class=\"headerlink\" href=\"#properties-of-neural-networks\" title=\"Link to this heading\">#</a></h1><h2><span class=\"section-number\">5.1. </span>ReLU networks and piecewise affine functions<a class=\"headerlink\" href=\"#relu-networks-and-piecewise-affine-functions\" title=\"Link to this heading\">#</a></h2><p>We\u2019ve trained and studied neural networks \u2013 but what kind of functions do they actually encode?\nIt turns out, ReLU networks encode quite simple functions, namely piecewise affine functions.</p><p>A function <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R^m\\)</span> is <em>piecewise affine</em> if there\nare finitely many polyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\subseteq\\mathbb R^n\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(\\mathbb R^n=\\bigcup_{i=1}^s\\ph Q_i\\)</span> and <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span>\nis an affine function for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots, s\\)</span>. The\npolyhedra <span class=\"math notranslate nohighlight\">\\(\\ph Q_i\\)</span> are the <em>pieces</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.\nThe smallest number of pieces <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots,\\ph Q_s\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(f|_{\\ph Q_i}\\)</span> is affine for every <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots,s\\)</span>\nis the <em>piece-number</em> of <span class=\"math notranslate nohighlight\">\\(f\\)</span>.</p>", "a[href=\"#raylem\"]": "<div class=\"proof lemma admonition\" id=\"raylem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.10 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let  <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R^m\\)</span> be a  piecewise affine function\nwith pieces <span class=\"math notranslate nohighlight\">\\(\\ph Q_1,\\ldots, \\ph Q_s\\)</span>.\nFor every <span class=\"math notranslate nohighlight\">\\(x\\in\\mathbb R^n\\)</span> there is an <span class=\"math notranslate nohighlight\">\\(\\alpha\\geq 1\\)</span> and a\npiece <span class=\"math notranslate nohighlight\">\\(\\ph Q_t\\)</span>\nsuch that <span class=\"math notranslate nohighlight\">\\(\\beta x\\in\\ph Q_t\\)</span> for all <span class=\"math notranslate nohighlight\">\\(\\beta\\geq\\alpha\\)</span>.</p>\n</section>\n</div>", "a[href=\"#NNcomplem\"]": "<div class=\"proof lemma admonition\" id=\"NNcomplem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.4 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f_1:\\mathbb R^n\\to\\mathbb R^m\\)</span> be a function that can be computed by a linear/ReLU\nnetwork of depth <span class=\"math notranslate nohighlight\">\\(k_1\\)</span> and width <span class=\"math notranslate nohighlight\">\\(\\ell_1\\)</span>, and\nlet <span class=\"math notranslate nohighlight\">\\(f_2:\\mathbb R^m\\to\\mathbb R^d\\)</span> be a function that can be computed by a linear/ReLU\nnetwork of depth <span class=\"math notranslate nohighlight\">\\(k_2\\)</span> and width <span class=\"math notranslate nohighlight\">\\(\\ell_2\\)</span>. Then <span class=\"math notranslate nohighlight\">\\(f_2\\circ f_1\\)</span> can be computed by a linear/ReLU\nnetwork of depth <span class=\"math notranslate nohighlight\">\\(k_1+k_2\\)</span> and width <span class=\"math notranslate nohighlight\">\\(\\max\\{\\ell_1,\\ell_2\\}\\)</span>.</p>\n</section>\n</div>", "a[href=\"#pwafffig\"]": "<figure class=\"align-default\" id=\"pwafffig\">\n<a class=\"reference internal image-reference\" href=\"../_images/pw_aff.png\"><img alt=\"../_images/pw_aff.png\" src=\"../_images/pw_aff.png\" style=\"width: 12cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 5.1 </span><span class=\"caption-text\">An illustration of <a class=\"reference internal\" href=\"#CPWLthm\">Theorem 5.3</a>: The piecewise affine function <span class=\"math notranslate nohighlight\">\\(f\\)</span>\ncan be expressed as the sum of the concave function <span class=\"math notranslate nohighlight\">\\(g\\)</span> and the convex function <span class=\"math notranslate nohighlight\">\\(h\\)</span>. The\nconvex function <span class=\"math notranslate nohighlight\">\\(h\\)</span> is the pointwise max of two affine functions (dashed), while the concave\nfunction <span class=\"math notranslate nohighlight\">\\(h\\)</span> is the min of two affine functions (dashed). Note that <span class=\"math notranslate nohighlight\">\\(-g\\)</span> is a convex\nfunction and that <span class=\"math notranslate nohighlight\">\\(-g\\)</span> can then be seen as the max of two affine functions.</span><a class=\"headerlink\" href=\"#pwafffig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#the-saw-tooth-function\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.3. </span>The saw tooth function<a class=\"headerlink\" href=\"#the-saw-tooth-function\" title=\"Link to this heading\">#</a></h2><p>Deeper neural networks are more powerful than shallow networks with the same number\nof parameters. An easy example of this is the <em>saw tooth function</em>: it can be\ncomputed by a deep network with only a few neurons; any shallow network, however,\nwill need a very large number of neurons.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id3\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>I am following here <a class=\"reference external\" href=\"https://mjt.cs.illinois.edu/dlt/\">lecture notes</a> of Telgarsky, who also proved\nthe main result in this section.</span></p><p>What is the saw tooth function? It is the iteration of the simple function</p>", "a[href=\"#replacelem\"]": "<div class=\"proof lemma admonition\" id=\"replacelem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.2 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(\\mathcal N\\)</span> be a linear/ReLU network of depth <span class=\"math notranslate nohighlight\">\\(d\\)</span> and width <span class=\"math notranslate nohighlight\">\\(w\\)</span>.\nThen there is a ReLU network <span class=\"math notranslate nohighlight\">\\(\\mathcal N'\\)</span> of depth <span class=\"math notranslate nohighlight\">\\(d\\)</span> and width at most <span class=\"math notranslate nohighlight\">\\(2w\\)</span>\nthat computes the same function.</p>\n</section>\n</div>", "a[href=\"#foolfig\"]": "<figure class=\"align-default\" id=\"foolfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/fool.png\"><img alt=\"../_images/fool.png\" src=\"../_images/fool.png\" style=\"width: 12cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 5.4 </span><span class=\"caption-text\">A neural network was trained on the MNIST data set. It recognises the \u20183\u2019 on the left correctly,\nand with high confidence. It also recognises noise and the picture of a shoe as a \u20183\u2019, again with high confidence.</span><a class=\"headerlink\" href=\"#foolfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#pacomplem\"]": "<div class=\"proof lemma admonition\" id=\"pacomplem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.1 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f,g:\\mathbb R\\to\\mathbb R\\)</span> be piecewise affine functions, and let\n<span class=\"math notranslate nohighlight\">\\(f\\)</span> have piece-number <span class=\"math notranslate nohighlight\">\\(k\\)</span>, and let <span class=\"math notranslate nohighlight\">\\(g\\)</span> have piece-number <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span>. Then</p>\n<ol class=\"arabic simple\">\n<li><p><span class=\"math notranslate nohighlight\">\\(f+h\\)</span> has piece-number at most <span class=\"math notranslate nohighlight\">\\(k+\\ell\\)</span>.</p></li>\n<li><p><span class=\"math notranslate nohighlight\">\\(f\\circ g\\)</span> has piece-number at most <span class=\"math notranslate nohighlight\">\\(k\\ell\\)</span>.</p></li>\n</ol>\n</section>\n</div>", "a[href=\"#CPWLthm\"]": "<div class=\"proof theorem admonition\" id=\"CPWLthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 5.3 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>A function <span class=\"math notranslate nohighlight\">\\(f:\\mathbb R^n\\to\\mathbb R\\)</span> is continuous piecewise affine if and only\nif there are affine functions <span class=\"math notranslate nohighlight\">\\(g_i,h_i\\)</span>, <span class=\"math notranslate nohighlight\">\\(i=1,\\ldots,N\\)</span> such that</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nf(x)=\\max_{i=1,\\ldots,N}g_i(x)-\\max_{j=1,\\ldots,N}h_j(x)\\text{ for all }x\\in\\mathbb R^n\n\\]</div>\n</section>\n</div>", "a[href=\"#neural-networks-are-sometimes-overconfident\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">5.4. </span>Neural networks are sometimes overconfident<a class=\"headerlink\" href=\"#neural-networks-are-sometimes-overconfident\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-5\"></label><input class=\"margin-toggle\" id=\"marginnote-role-5\" name=\"marginnote-role-5\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/fool.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>fool</a></span>\nIn practice it is sometimes observed that neural networks confidently\nclassify some image data, with confidence levels approaching 100%,\neven though the input data is just white noise. That is, in MNIST for\nexample, it is possible to generate noise pictures that a neural network\nwill happily claim show a \u20187\u2019, and that with 99.8% certainty.</p>", "a[href=\"convex.html#suplem\"]": "<div class=\"proof lemma admonition\" id=\"suplem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 3.6 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(I\\)</span> be some index set, let <span class=\"math notranslate nohighlight\">\\(C\\)</span> be a convex set.\nLet <span class=\"math notranslate nohighlight\">\\(f_i:C\\to\\mathbb R\\)</span>, <span class=\"math notranslate nohighlight\">\\(i\\in I\\)</span>, be a family of convex functions. Then\n<span class=\"math notranslate nohighlight\">\\(f:x\\mapsto\\sup_{i\\in I}f_i(x)\\)</span> is a convex function.</p>\n</section>\n</div>", "a[href=\"#NNaddlem\"]": "<div class=\"proof lemma admonition\" id=\"NNaddlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 5.3 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(f_1,f_2:\\mathbb R^n\\to\\mathbb R^m\\)</span> be two functions that can be computed by\nlinear/ReLU networks of depth <span class=\"math notranslate nohighlight\">\\(k\\)</span> and width <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span>. Then <span class=\"math notranslate nohighlight\">\\(f_1+f_2\\)</span> can be computed\nby a linear/ReLU network of depth <span class=\"math notranslate nohighlight\">\\(k\\)</span> and width <span class=\"math notranslate nohighlight\">\\(2\\ell\\)</span>.</p>\n</section>\n</div>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
