selector_to_html = {"a[href=\"#adafig\"]": "<figure class=\"align-default\" id=\"adafig\">\n<a class=\"reference internal image-reference\" href=\"../_images/ada.png\"><img alt=\"../_images/ada.png\" src=\"../_images/ada.png\" style=\"width: 12cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 8.4 </span><span class=\"caption-text\">The effect of boosting: we iteratively combine six simple classifiers,\neach one an axis-parallel half-plane. In each panel the decision boundary of the combined\nclassifier is shown, as well as the training set (two classes), with misclassified\nsamples shown with larger markers. The dashed line shows the decision boundary\nof the new classifier in each step.\nObserve that the decision boundary becomes increasingly\nmore complicated with increased number of classifiers. Also, previously correctly\nclassified samples may become misclassified.</span><a class=\"headerlink\" href=\"#adafig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#adaalg\"]": "<div class=\"proof algorithm admonition\" id=\"adaalg\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Algorithm 8.1 </span> (AdaBoost)</p>\n<section class=\"algorithm-content\" id=\"proof-content\">\n<p><strong>Instance</strong> Training set <span class=\"math notranslate nohighlight\">\\(S\\)</span>, number of iterations <span class=\"math notranslate nohighlight\">\\(T\\)</span>.<br/>\n<strong>Output</strong> A classifier.</p>\n<ol class=\"arabic simple\">\n<li><p>Put <span class=\"math notranslate nohighlight\">\\(p^{(1)}_{(x,y)}=\\frac{1}{|S|}\\)</span> for every <span class=\"math notranslate nohighlight\">\\((x,y)\\in S\\)</span>.</p></li>\n<li><p><strong>for</strong> <span class=\"math notranslate nohighlight\">\\(t=1,\\ldots, T\\)</span></p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Minimise <a class=\"reference internal\" href=\"#equation-wloss\">(8.2)</a> over <span class=\"math notranslate nohighlight\">\\(\\mathcal W\\)</span>, and let <span class=\"math notranslate nohighlight\">\\(h_t\\)</span> be a minimiser.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute the error</p></li>\n</ol>\n<div class=\"math notranslate nohighlight\">\n\\[\\epsilon_t=\\sum_{(x,y)\\in S}p^{(t)}_{(x,y)}\\ell_{0-1}(y,h_t(x))\\]</div>\n<ol class=\"arabic simple\" start=\"5\">\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Put <span class=\"math notranslate nohighlight\">\\(\\alpha_t=\\tfrac{1}{2}\\log\\left(\\frac{1}{\\epsilon_t}-1\\right)\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Put</p></li>\n</ol>\n<div class=\"math notranslate nohighlight\">\n\\[p_{(x,y)}^{(t+1)}=\\frac{p^{(t)}_{(x,y)}e^{-\\alpha_tyh_t(x)}}\n{\\sum_{(x',y')\\in S}p^{(t)}_{(x',y')}e^{-\\alpha_ty'h_t(x')}}\n\\quad\\text{for every }(x,y)\\in S\\]</div>\n<ol class=\"arabic simple\" start=\"7\">\n<li><p><strong>output</strong> <span class=\"math notranslate nohighlight\">\\(\\sgn\\sum_{t=1}^T\\alpha_th_t\\)</span>.</p></li>\n</ol>\n</section>\n</div>", "a[href=\"#dependent-classifiers\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.3. </span>Dependent classifiers<a class=\"headerlink\" href=\"#dependent-classifiers\" title=\"Link to this heading\">#</a></h2><p>After this short digression, we come back to a collective of interdependent classifiers.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id3\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>Based on <em>The Condorcet Jury Theorem, Free Speech, and Correlated Votes</em>, K.K. Ladha (1992)</span>\nAssume that we have trained <span class=\"math notranslate nohighlight\">\\(n\\)</span> classifiers <span class=\"math notranslate nohighlight\">\\(h_i:\\mathcal X\\to\\mathcal Y\\)</span>, and we let <span class=\"math notranslate nohighlight\">\\(h\\)</span>\nbe the majority classifier</p>", "a[href=\"#equation-enough\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-enough\">\n\\[\\frac{Z_{t+1}}{Z_t}\\leq e^{-2\\gamma^2}\\]</div>", "a[href=\"#random-forest\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.4. </span>Random Forest<a class=\"headerlink\" href=\"#random-forest\" title=\"Link to this heading\">#</a></h2><p>One of the most popular ensemble machine learning algorithms is\n<em>random forest</em>.\nRandom forest trains an ensemble of decision trees, and then\nclassifies by majority voting (or alternatively, by voting weighted\nby confidence levels).</p><p>As we have seen in <a class=\"reference internal\" href=\"#sec-depclass\"><span class=\"std std-numref\">Section 8.3</span></a>, there are two design goals\nfor a majority voting classifier:</p>", "a[href=\"#sec-depclass\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.3. </span>Dependent classifiers<a class=\"headerlink\" href=\"#dependent-classifiers\" title=\"Link to this heading\">#</a></h2><p>After this short digression, we come back to a collective of interdependent classifiers.<label class=\"margin-toggle\" for=\"sidenote-role-3\"><span id=\"id3\">\n<sup>3</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-3\" name=\"sidenote-role-3\" type=\"checkbox\"/><span class=\"sidenote\"><sup>3</sup>Based on <em>The Condorcet Jury Theorem, Free Speech, and Correlated Votes</em>, K.K. Ladha (1992)</span>\nAssume that we have trained <span class=\"math notranslate nohighlight\">\\(n\\)</span> classifiers <span class=\"math notranslate nohighlight\">\\(h_i:\\mathcal X\\to\\mathcal Y\\)</span>, and we let <span class=\"math notranslate nohighlight\">\\(h\\)</span>\nbe the majority classifier</p>", "a[href=\"#wisdom-of-the-crowd\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.1. </span>Wisdom of the crowd<a class=\"headerlink\" href=\"#wisdom-of-the-crowd\" title=\"Link to this heading\">#</a></h2><p>An easy way to increase the performance in a classification task is\nto train several classifiers and then let them decide by majority voting.</p><p>To gain a first insight, consider a binary classification task and\nassume that we have access to <span class=\"math notranslate nohighlight\">\\(T\\)</span> classifiers\n<span class=\"math notranslate nohighlight\">\\(h_1,\\ldots, h_T\\)</span> that each have a probability of <span class=\"math notranslate nohighlight\">\\(p&gt;\\tfrac{1}{2}\\)</span>\nto classify a randomly drawn data point correctly (we assume here that\nthe class <span class=\"math notranslate nohighlight\">\\(y\\)</span> is completely determined by <span class=\"math notranslate nohighlight\">\\(x\\)</span>). Assume, furthermore,\nthat the classifiers are stochastically independent. (Clearly,\nthis is an unrealistic assumption.) Then the probability that\nthe majority vote decides wrongly is</p>", "a[href=\"#corrfig\"]": "<figure class=\"align-default\" id=\"corrfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/corr.png\"><img alt=\"../_images/corr.png\" src=\"../_images/corr.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 8.1 </span><span class=\"caption-text\"><span class=\"math notranslate nohighlight\">\\(\\bar p\\mapsto \\frac{\\delta^2}{c\\bar p(1-\\bar p)+\\delta^2}\\)</span> for different values of the average\ncorrelation <span class=\"math notranslate nohighlight\">\\(c\\)</span>.</span><a class=\"headerlink\" href=\"#corrfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-wloss\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-wloss\">\n\\[\\argmin_{h\\in\\mathcal W} L_{p,S}(h) = \\argmin_{h\\in\\mathcal W}\n\\sum_{(x,y)\\in S}p_{(x,y)}\\ell_{0-1}(y,h(x))\\]</div>", "a[href=\"#moretreesfig\"]": "<figure class=\"align-default\" id=\"moretreesfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/rf_depths.png\"><img alt=\"../_images/rf_depths.png\" src=\"../_images/rf_depths.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 8.2 </span><span class=\"caption-text\">Random forest benefits from deeper and more trees. Test error vs number of\ndecision trees in the ensemble, with varying maximum depths.\nData set is on <a class=\"reference external\" href=\"https://www.openml.org/search?type=data&amp;amp;status=active&amp;amp;id=40497\">Thyroid function</a>.\nEach setting is\nrepeated 100 times, plot shows means and 95% confidence intervals.</span><a class=\"headerlink\" href=\"#moretreesfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#lossfig\"]": "<figure class=\"align-default\" id=\"lossfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/losses.png\"><img alt=\"../_images/losses.png\" src=\"../_images/losses.png\" style=\"width: 10cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 8.5 </span><span class=\"caption-text\">Three loss functions. The zero-one loss here is interpreted as\n<span class=\"math notranslate nohighlight\">\\(z\\mapsto \\ell_{0-1}(y\\sgn z,1)\\)</span>. The logistic loss function was defined\nin <a class=\"reference internal\" href=\"intro.html#logregsec\"><span class=\"std std-numref\">Section 1.4</span></a>.</span><a class=\"headerlink\" href=\"#lossfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-adab\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-adab\">\n\\[\\epsilon_t\\leq \\frac{1}{2}-\\gamma\\]</div>", "a[href=\"#xgboost\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.9. </span>XGBoost<a class=\"headerlink\" href=\"#xgboost\" title=\"Link to this heading\">#</a></h2><p>XGBoost<label class=\"margin-toggle\" for=\"sidenote-role-10\"><span id=\"id10\">\n<sup>10</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-10\" name=\"sidenote-role-10\" type=\"checkbox\"/><span class=\"sidenote\"><sup>10</sup><em>XGBoost: A Scalable Tree Boosting System</em>, T. Chen and C. Guestrin, <a class=\"reference external\" href=\"https://arxiv.org/abs/1603.02754\">arXiv:1603.02754</a></span>\ncombines gradient boosting and regularisation. It is a relatively simple but quite powerful learning algorithm\nthat was sucessfully used in a number of competitions.\nLet me just give a rough overview on XGBoost without going into too many details.</p><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-11\"></label><input class=\"margin-toggle\" id=\"marginnote-role-11\" name=\"marginnote-role-11\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/ensemble_classifiers/xgb.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>xgb</a></span>\nXGBoost introduces a penalty term <span class=\"math notranslate nohighlight\">\\(\\Omega(h_{t})\\)</span> on the trees <span class=\"math notranslate nohighlight\">\\(h_{t}\\)</span>. The term penalises\nlarge weights and also large trees. The loss function becomes</p>", "a[href=\"#equation-canteq\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-canteq\">\n\\[\\proba[\\bar X&gt;\\tfrac{1}{2}]  \n= 1-\\proba[\\bar X\\leq \\bar p-\\delta] \n \\geq 1-\\frac{\\sigma^2}{\\sigma^2+\\delta^2} = \\frac{\\delta^2}{\\sigma^2+\\delta^2}\\]</div>", "a[href=\"#equation-pt\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-pt\">\n\\[p^{(t+1)}_{(x,y)}=\\frac{e^{-yg_t(x)}}{\\sum_{(x',y')\\in S}e^{-y'g_t(x')}}\\]</div>", "a[href=\"#ensemble-learning\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8. </span>Ensemble learning<a class=\"headerlink\" href=\"#ensemble-learning\" title=\"Link to this heading\">#</a></h1><p>In <em>Who wants to be a millionaire?</em> contestants have the option to ask the audience\nfor help with a quiz question. Often this successful: the majority vote indicates the right answer.\nThe interesting feature\nhere is that obviously the audience of game shows does not usually consist of experts.\nOn the contrary, the typical audience member  is arguably more ignorant than the contestants,\nwho have already proved their merit by clearing the pre-selection process. Still,\ncollectively, the audience is relatively strong.\nThis phenomenon is called <em>wisdom of the crowd</em>.<label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-1\"></label><input class=\"margin-toggle\" id=\"marginnote-role-1\" name=\"marginnote-role-1\" type=\"checkbox\"/><span class=\"marginnote\"> There seems to have been written a lot about the supposed\nwisdom of the crowd, even a whole book. Sometimes, though, the crowd is dead-wrong:\nThe 80s, for example, were terrible and there\u2019s no reason to celebrate them.</span></p><p><em>Ensemble learning</em> combines several poor\npredictors to a better predictor. Broadly, there are two ways to do that:</p>", "a[href=\"#short-stochastic-digression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.2. </span>Short stochastic digression<a class=\"headerlink\" href=\"#short-stochastic-digression\" title=\"Link to this heading\">#</a></h2><p>To cope with a collective of classifiers with stochastic interdependencies\nwe need a stochastic tool, a sort of  one-sided\nChebyshev\u2019s inequality.</p><p>For comparison, let\u2019s recall Chebyshev\u2019s inequality.\nFor this let <span class=\"math notranslate nohighlight\">\\(X\\)</span> be a random variable, and recall the definition\nof the <em>variance</em> of a random variable:</p>", "a[href=\"#noise-features\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.5. </span>Noise features<a class=\"headerlink\" href=\"#noise-features\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-5\"></label><input class=\"margin-toggle\" id=\"marginnote-role-5\" name=\"marginnote-role-5\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/ensemble_classifiers/rf_noise_vars.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>rf_noise_vars</a></span>\nThe design of random forest gives us some insights when random forest is adequate\nand when not. Let\u2019s look at one set of circumstances when it is not.</p><p>In many classification tasks some of the inputs are just noise and are not in any way indicative\nof the class. This is typically the case, for instance, for boundary pixels in image recognition.\nIn the MNIST digit recognition task there are some pixels, the upper left pixel, for example,\nthat are white for every single sample. In a more realistic image recognition task the upper left\npixel will typically still be completely unrelated to the class but show different values\nfor different images \u2013 it\u2019s simply noise.</p>", "a[href=\"#rfnoisefig\"]": "<figure class=\"align-default\" id=\"rfnoisefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/rf_noise_vars.png\"><img alt=\"../_images/rf_noise_vars.png\" src=\"../_images/rf_noise_vars.png\" style=\"width: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 8.3 </span><span class=\"caption-text\">Accuracy of random forest suffers if there are many noisy features.\nIn a binary classification task with two features, different numbers of noise features where\nadded, each normally distributed independently of the class. Each setting was repeated 100 times;\nthe plot shows mean and 95% confidence intervals. While both, AdaBoost and random forest,\nperform worse with increased number of noise features, \\alg{random forest} is much more severely\naffected.</span><a class=\"headerlink\" href=\"#rfnoisefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#gradient-boosting\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.8. </span>Gradient boosting<a class=\"headerlink\" href=\"#gradient-boosting\" title=\"Link to this heading\">#</a></h2><p>AdaBoost, as presented above, does binary classification. What if we have a regression\nproblem? What if the loss function is somewhat unusual? It is not immediately\nclear how AdaBoost would need to be adapted. The framework\nof <em>gradient boosting</em> helps.</p><p>We consider a regression problem, with a training set <span class=\"math notranslate nohighlight\">\\(S\\)</span> of\npairs <span class=\"math notranslate nohighlight\">\\((x,y)\\)</span> with <span class=\"math notranslate nohighlight\">\\(x\\in\\mathbb R^n\\)</span> and <span class=\"math notranslate nohighlight\">\\(y\\in\\mathbb R\\)</span>, and a loss\nfunction <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span> that is sufficiently smooth (with perhaps a finite number\nof exceptions). It is not wrong to imagine that <span class=\"math notranslate nohighlight\">\\(\\ell\\)</span> is equal to the square loss,\nie</p>", "a[href=\"#boosting\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.6. </span>Boosting<a class=\"headerlink\" href=\"#boosting\" title=\"Link to this heading\">#</a></h2><p>A classifier may show bad performance due to two reasons:</p>", "a[href=\"#adaboost\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">8.7. </span>AdaBoost<a class=\"headerlink\" href=\"#adaboost\" title=\"Link to this heading\">#</a></h2><p>We consider a binary classification task. That is, as usual, we draw a training set\n<span class=\"math notranslate nohighlight\">\\(S\\)</span> from <span class=\"math notranslate nohighlight\">\\(\\mathcal X\\times \\mathcal Y\\)</span>, where <span class=\"math notranslate nohighlight\">\\(\\mathcal Y=\\{-1,1\\}\\)</span>. Moreover,\nwe aim to minimise the average zero-one loss</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
