selector_to_html = {"a[href=\"https://en.wikipedia.org/wiki/Elo_rating_system\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/en/f/f0/ArpadElo.jpg\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>The <b>Elo rating system</b> is a method for calculating the relative skill levels of players in zero-sum games such as chess or esports. It is named after its creator Arpad Elo, a Hungarian-American physics professor.</p>", "a[href^=\"https://en.wikipedia.org/wiki/Elo_rating_system#\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/en/f/f0/ArpadElo.jpg\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>The <b>Elo rating system</b> is a method for calculating the relative skill levels of players in zero-sum games such as chess or esports. It is named after its creator Arpad Elo, a Hungarian-American physics professor.</p>", "a[href=\"#on-and-off-policy\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.12. </span>On- and off-policy<a class=\"headerlink\" href=\"#on-and-off-policy\" title=\"Link to this heading\">#</a></h2><p>The <a class=\"reference internal\" href=\"#pgmalg\">Algorithm 9.3</a> (policy gradient method) is <em>on-policy</em>: the algorithm needs to generate\ntrajectories that follow the current policy (see line 4).\nAt first glance this looks to be the case for <a class=\"reference internal\" href=\"#qlearnalg\">Algorithm 9.2</a> (<span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning), too.\nThe next action is chosen <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>-greedily, and thus most of the time\naccording to the current <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values. The algorithm, however, does not need that.\nWhat it needs is that every state/action pair is visited arbitrarily often,\nie, that the conditions of <a class=\"reference internal\" href=\"#qlthm\">Theorem 9.6</a> are satisfied.\nIn fact, <span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning is an <em>off-policy</em> method: the\ntrajectories do not have to come from the current policy.</p><p>Off-policy methods have an advantage over on-policy methods. They allow\ntraining with historical data. Often historical data is easier to collect.\nImagine the situation that a climate control system should be run by a\nreinforcement learning algorihm. The current system is either controlled\nby humans or by simple rules. In either case, plenty of data of past\nperformance is likely available and could be used to at least start\ntraining with an off-policy method. In a setting like a climate control\nsystem, on-policy learning may be very slow (climate settings won\u2019t be changed\na thousand times every hour) and a  badly performing initial\npolicy might lead to unacceptable climate control over an extended\nperiod of time (too hot or too cold because the policy still\nneeds to improve).</p>", "a[href=\"#mazefig\"]": "<figure class=\"align-default\" id=\"mazefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/maze.png\"><img alt=\"../_images/maze.png\" src=\"../_images/maze.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 9.2 </span><span class=\"caption-text\">A maze: Find the exit, don\u2019t die.</span><a class=\"headerlink\" href=\"#mazefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#polesec\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.2. </span>Pole balancing<a class=\"headerlink\" href=\"#pole-balancing\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-3\"></label><input class=\"margin-toggle\" id=\"marginnote-role-3\" name=\"marginnote-role-3\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/reinforcement_learning/pole.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>pole</a></span>\nLet\u2019s have a look at a classic reinforcement learning problem. The task consists in balancing a pole on a cart.\nWhen the pole starts falling over, the cart can move to counterbalance the movement of the pole. The aim\nis to keep the pole from falling as long as possible. See <a class=\"reference internal\" href=\"#polefig\"><span class=\"std std-numref\">Fig. 9.4</span></a>.</p>", "a[href=\"#polgrad2thm\"]": "<div class=\"proof theorem admonition\" id=\"polgrad2thm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 9.8 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(b\\)</span> be a function on the states. Then</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\nabla_w v_{\\pi_w}(s)=\\expec_{\\tau\\sim\\pi_w}\\left[\n \\sum_{t=0}^T  \\left(G(\\tau)-b(S_t)\\right)\\frac{\\nabla_w\\pi_w(A_t|S_t)}{\\pi_w(A_t|S_t)}\n \\,\\Big|\\, S_0=s\n\\right]\n\\]</div>\n</section>\n</div>", "a[href=\"#equation-rl2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-rl2\">\n\\[\\nabla_w v_{\\pi_w}(s)= \n   \\sum_{\\tau} G(\\tau)\\proba[\\tau | w] \\nabla_w \\log(\\proba[\\tau | w])\\]</div>", "a[href=\"#returns\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.4. </span>Returns<a class=\"headerlink\" href=\"#returns\" title=\"Link to this heading\">#</a></h2><p>Once the agent in a reinforcement learning task starts it follows a\ntrajectory of states, actions, and rewards:</p>", "a[href=\"#equation-qupdate\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-qupdate\">\n\\[\\begin{split}\\begin{aligned}\n&amp;q_{t+1}(s_t,a_t):=\\\\\n&amp;\\qquad q_t(s_t,a_t)+\\eta_t(r(s_t,a_t,s_{t+1})+\\gamma \\max_{a'}q_t(s_{t+1},a')-q_t(s_t,a_t)),\n\\end{aligned}\\end{split}\\]</div>", "a[href=\"#pole-balancing\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.2. </span>Pole balancing<a class=\"headerlink\" href=\"#pole-balancing\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-3\"></label><input class=\"margin-toggle\" id=\"marginnote-role-3\" name=\"marginnote-role-3\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/reinforcement_learning/pole.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>pole</a></span>\nLet\u2019s have a look at a classic reinforcement learning problem. The task consists in balancing a pole on a cart.\nWhen the pole starts falling over, the cart can move to counterbalance the movement of the pole. The aim\nis to keep the pole from falling as long as possible. See <a class=\"reference internal\" href=\"#polefig\"><span class=\"std std-numref\">Fig. 9.4</span></a>.</p>", "a[href=\"#markov-decision-processes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.1. </span>Markov decision processes<a class=\"headerlink\" href=\"#markov-decision-processes\" title=\"Link to this heading\">#</a></h2><p>Let\u2019s consider a classical toy example, an example of a <em>maze</em>; see <a class=\"reference internal\" href=\"#mazefig\"><span class=\"std std-numref\">Fig. 9.2</span></a>.\nAn agent starts at the position marked by a robot and moves in each time step from one square to the next.\nThe agent can move up, down, left or right, as long as there are no walls.\nThe aim of the agent is to reach, in the shortest possible way, the exit marked by a flag, where a reward of +1\nis waiting. There are also three squares with deadly traps resulting in a penalty of -1.\nThe task is stopped if the agent enters any square with a trap or the exit.</p>", "a[href=\"#policy-improvement-theorem\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.7. </span>Policy improvement theorem<a class=\"headerlink\" href=\"#policy-improvement-theorem\" title=\"Link to this heading\">#</a></h2><p>Assume an agent is following a policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span> but is given the opportunity to change a single action in a state <span class=\"math notranslate nohighlight\">\\(s\\)</span>.\nWhich one should she choose? To answer this question, we introduce <em>state-action values</em>,\nor shorter <em><span class=\"math notranslate nohighlight\">\\(q\\)</span>-values</em>. Given a state <span class=\"math notranslate nohighlight\">\\(s\\)</span> and an action <span class=\"math notranslate nohighlight\">\\(a\\)</span>, the value <span class=\"math notranslate nohighlight\">\\(q_\\pi(s,a)\\)</span>\ngives the expected discounted return that is obtained by\nchoosing action <span class=\"math notranslate nohighlight\">\\(a\\)</span> in state <span class=\"math notranslate nohighlight\">\\(s\\)</span> and then following policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>:</p>", "a[href=\"#what-else-is-there\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.14. </span>What else is there?<a class=\"headerlink\" href=\"#what-else-is-there\" title=\"Link to this heading\">#</a></h2><p>The methods treated here will not be enough to train a world-class\nchess engine or a fully autonomous robot.\nThe spectacular advances in reinforcement learning that we have seen\nin recent years become only possible if the basic ideas presented here\nare combined with the power of deep neural networks.</p><p>In <em>deep <span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning</em>, for instance, a deep neural network learns to\napproximate a <span class=\"math notranslate nohighlight\">\\(q\\)</span>-function.<label class=\"margin-toggle\" for=\"sidenote-role-10\"><span id=\"id10\">\n<sup>10</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-10\" name=\"sidenote-role-10\" type=\"checkbox\"/><span class=\"sidenote\"><sup>10</sup><em>Playing Atari with Deep Reinforcement Learning</em>,\nV. Mnih, K. Kavukcuoglu, D. Silver, D. Wierstra, A. Graves\nand I. Antonoglou (2013), <a class=\"reference external\" href=\"https://arxiv.org/abs/1312.5602\">arXiv:1312.5602</a></span>\n<em>Actor-critic</em> algorithms extend this: two neural networks\nare trained. One, the critic, learns to predict <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values by observing\nthe other neural network, the actor; while the actor constantly tries\nto improve a policy by exploiting the approximated <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values of the critic.</p>", "a[href=\"#equation-trajprob\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-trajprob\">\n\\[\\proba[s_0,s_1,\\ldots]=\\prod_{t=0}^\\infty p(s_t,\\pi(s_t),s_{t+1})\\]</div>", "a[href=\"#polimpthm\"]": "<div class=\"proof theorem admonition\" id=\"polimpthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 9.1 </span> (policy improvement)</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\pi'\\)</span> be two deterministic policies such that</p>\n<div class=\"math notranslate nohighlight\" id=\"equation-polimpeq\">\n<span class=\"eqno\">(9.3)<a class=\"headerlink\" href=\"#equation-polimpeq\" title=\"Link to this equation\">#</a></span>\\[q_\\pi(s,\\pi'(s))\\geq v_\\pi(s)\\text{ for all }s\\in\\mathcal S.\\]</div>\n<p>Then</p>\n<div class=\"math notranslate nohighlight\" id=\"equation-polimpeq2\">\n<span class=\"eqno\">(9.4)<a class=\"headerlink\" href=\"#equation-polimpeq2\" title=\"Link to this equation\">#</a></span>\\[v_{\\pi'}(s)\\geq v_\\pi(s) \\text{ for all }s\\in\\mathcal S\\]</div>\n<p>Moreover, if some\ninequality in <a class=\"reference internal\" href=\"#equation-polimpeq\">(9.3)</a> is strict for some state <span class=\"math notranslate nohighlight\">\\(s\\)</span>, then also <a class=\"reference internal\" href=\"#equation-polimpeq2\">(9.4)</a>\nis strict for that state.</p>\n</section>\n</div>", "a[href=\"#blackjackfig\"]": "<figure class=\"align-default\" id=\"blackjackfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/blackjack.png\"><img alt=\"../_images/blackjack.png\" src=\"../_images/blackjack.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 9.6 </span><span class=\"caption-text\"><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning in a game of Blackjack. After 200000 iterations\nconvergence still has not been achieved: The boundary between <em>hitting</em> (player demands one more card) and\n<em>sticking</em> (player sticks to their hand) should be much more regular.</span><a class=\"headerlink\" href=\"#blackjackfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-polgradeq\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-polgradeq\">\n\\[\\expec_{\\tau\\sim p_w}\\left[\n\\nabla_w \\log\\pi_w(A_t|S_t)b(S_t)\n\\right]=0\\]</div>", "a[href=\"#equation-polimpeq3\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-polimpeq3\">\n\\[\\expec_{a\\sim \\pi'}[q_\\pi(s,a)]\\geq v_\\pi(s)\\text{ for all }s\\in\\mathcal S.\\]</div>", "a[href=\"#policies\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.5. </span>Policies<a class=\"headerlink\" href=\"#policies\" title=\"Link to this heading\">#</a></h2><p>It\u2019s time to turn to the agent. Once fully trained, how should the agent act? What the agent\nknows about the current situation is encapsulated in the state. Based on that state\nthe agent has to decide which action to take. That is, an agent is described by a function\nfrom states to actions. Such a function</p>", "a[href=\"#mazepolfig\"]": "<figure class=\"align-default\" id=\"mazepolfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/mazepol.png\"><img alt=\"../_images/mazepol.png\" src=\"../_images/mazepol.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 9.5 </span><span class=\"caption-text\">A deterministic policy in a maze.</span><a class=\"headerlink\" href=\"#mazepolfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-qlearn\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-qlearn\">\n\\[q(s,a) = \\sum_{s'}p(s,a,s')\\left(\nr(s,a,s')+\\gamma \\max_{a'}q(s',a')\\right)\\]</div>", "a[href=\"#qlthm\"]": "<div class=\"proof theorem admonition\" id=\"qlthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 9.6 </span> (Watkins 1982)</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let a finite MDP be given.\nIf</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\sum_{t=1}^\\infty\\eta_t(s,a)=\\infty\\quad\\text{ and }\\quad\\sum_{t=1}^\\infty \\eta^2_t(s,a)&lt;\\infty\n\\]</div>\n<p>for all pairs <span class=\"math notranslate nohighlight\">\\(s,a\\)</span> of a state and an action then the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values\n<span class=\"math notranslate nohighlight\">\\(q_t\\)</span> computed by <a class=\"reference internal\" href=\"#qlearnalg\">Algorithm 9.2</a>.\nconverge with probability 1 to the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values of an optimal policy.</p>\n</section>\n</div>", "a[href=\"#equation-polimpeq4\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-polimpeq4\">\n\\[v_{\\pi'}(s)\\geq v_\\pi(s) \\text{ for all }s\\in\\mathcal S\\]</div>", "a[href=\"#reinforcement-learning-and-llms\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.13. </span>Reinforcement learning and LLMs<a class=\"headerlink\" href=\"#reinforcement-learning-and-llms\" title=\"Link to this heading\">#</a></h2><p>After training on a large text corpus,\nan AI chatbot such as ChatGPT, Gemini or Claude needs to be finetuned\nto ensure that it produces helpful, harmless und honest output.\nThis is a challenge because it is hard to come by high quality training data:\nIn contrast to pre-training the language model, where basically a good part of the internet is ingested,\ngood and helpful answers to prompts would need to be written by humans. This is burdensome, costly and\ntime consuming. As a result any such dataset of prompts and model answers will be\nsmall.</p><p>Because of the dearth of good training datasets, current AI chatbots\nare finetuned in a different way. Later versions of ChatGPT, for example, are finetuned\nwith <em>reinforcement learning from human feedback</em>,<label class=\"margin-toggle\" for=\"sidenote-role-9\"><span id=\"id9\">\n<sup>9</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-9\" name=\"sidenote-role-9\" type=\"checkbox\"/><span class=\"sidenote\"><sup>9</sup><em>Training language models to follow instructions\nwith human feedback</em>, Ouyang et al. (2022), <a class=\"reference external\" href=\"https://arxiv.org/abs/2203.02155\">arXiv:2203.02155</a></span>\nwhich we\u2019ll briefly describe here.</p>", "a[href=\"#equation-valuedef\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-valuedef\">\n\\[\\begin{split}\\begin{aligned}\nv_\\pi(s_0) &amp; = \\expec_\\pi\\left[\\sum_{t=0}^\\infty\\gamma^{t}R_{t+1}|S_0=s_0\\right]\\\\\n&amp; = \\sum_{t=0}^\\infty\\gamma^t\n\\sum_{s,s'}\\sum_a\n\\proba_\\pi[S_t=s|S_0=s_0]\\,p(s,a,s')\\,\\pi(a|s)\\, r(s,a,s')\n\\end{aligned}\\end{split}\\]</div>", "a[href=\"#polgradthm\"]": "<div class=\"proof theorem admonition\" id=\"polgradthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 9.7 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<div class=\"math notranslate nohighlight\">\n\\[\n\\nabla_w v_{\\pi_w}(s)=\\expec_{\\tau\\sim\\pi_w}\\left[\nG(\\tau)\\sum_{t=0}^T  \\frac{\\nabla_w\\pi_w(A_t|S_t)}{\\pi_w(A_t|S_t)}\n \\,\\Big|\\, S_0=s\n\\right]\n\\]</div>\n</section>\n</div>", "a[href=\"#epsgreedyalg\"]": "<div class=\"proof algorithm admonition\" id=\"epsgreedyalg\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Algorithm 9.1 </span> (<span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>-greedy)</p>\n<section class=\"algorithm-content\" id=\"proof-content\">\n<p><strong>Instance</strong> A state <span class=\"math notranslate nohighlight\">\\(s\\)</span>, <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values, a constant <span class=\"math notranslate nohighlight\">\\(\\epsilon&gt;0\\)</span>.<br/>\n<strong>Output</strong> An action.</p>\n<ol class=\"arabic simple\">\n<li><p>Draw a random number <span class=\"math notranslate nohighlight\">\\(r\\)</span> uniformly from <span class=\"math notranslate nohighlight\">\\([0,1]\\)</span>.</p></li>\n<li><p>If <span class=\"math notranslate nohighlight\">\\(r\\leq \\epsilon\\)</span> then pick an action <span class=\"math notranslate nohighlight\">\\(a\\)</span> from <span class=\"math notranslate nohighlight\">\\(\\mathcal A(s)\\)</span> uniformly.</p></li>\n<li><p>If <span class=\"math notranslate nohighlight\">\\(r&gt;\\epsilon\\)</span> then set <span class=\"math notranslate nohighlight\">\\(a=\\argmax_{a'}q(s,a')\\)</span>.</p></li>\n<li><p><strong>output</strong> <span class=\"math notranslate nohighlight\">\\(a\\)</span>.</p></li>\n</ol>\n</section>\n</div>", "a[href=\"#polefig\"]": "<figure class=\"align-default\" id=\"polefig\">\n<a class=\"reference internal image-reference\" href=\"../_images/polefig.png\"><img alt=\"../_images/polefig.png\" src=\"../_images/polefig.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 9.4 </span><span class=\"caption-text\">Pole balancing</span><a class=\"headerlink\" href=\"#polefig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-polimpeq\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-polimpeq\">\n\\[q_\\pi(s,\\pi'(s))\\geq v_\\pi(s)\\text{ for all }s\\in\\mathcal S.\\]</div>", "a[href=\"#qoptthm\"]": "<div class=\"proof theorem admonition\" id=\"qoptthm\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 9.4 </span></p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>A deterministic policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span> is optimal if and only for every state <span class=\"math notranslate nohighlight\">\\(s\\in\\mathcal S\\)</span>\nit holds that</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\max_{a\\in\\mathcal A} q_{\\pi}(s,a) = q_{\\pi}(s,{\\pi}(s)).\n\\]</div>\n</section>\n</div>", "a[href=\"#eglplem\"]": "<div class=\"proof lemma admonition\" id=\"eglplem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 9.1 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(p_w\\)</span> be a parameterised probability distribution that depends on a vector <span class=\"math notranslate nohighlight\">\\(w\\)</span>.\nThen</p>\n<div class=\"math notranslate nohighlight\">\n\\[\n\\expec_{x\\sim p_w}\\left[\\nabla_w \\log p_w(x)\\right]=0\n\\]</div>\n</section>\n</div>", "a[href=\"#q-learning\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.8. </span><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning<a class=\"headerlink\" href=\"#q-learning\" title=\"Link to this heading\">#</a></h2><p>How can we learn an optimal policy? Above we sketched a procedure: Start with some policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>,\ncompute its <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values, check whether there is a\nstate <span class=\"math notranslate nohighlight\">\\(s\\)</span> and an action <span class=\"math notranslate nohighlight\">\\(a\\)</span> with <span class=\"math notranslate nohighlight\">\\(q_{\\pi}(s,{\\pi}(s))&lt;q_{\\pi}(s,a)\\)</span>,\nchange the policy <span class=\"math notranslate nohighlight\">\\(\\pi(s)=a\\)</span> and repeat. The procedure is very slow and suffers from a serious disadvantage:\nwe don\u2019t know how to compute the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.</p><p>Fortunately, there is a method, <em><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning</em>, that bypasses the policy improvement\nsteps and directly learns the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values <span class=\"math notranslate nohighlight\">\\(q^*\\)</span> of an optimal deterministic policy <span class=\"math notranslate nohighlight\">\\(\\pi^*\\)</span> \u2013 provided\nthe MDP is finite.\nHow then can we recover the policy from the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values?\nIt follows directly from <a class=\"reference internal\" href=\"#qoptthm\">Theorem 9.4</a> that</p>", "a[href=\"#baselines\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.11. </span>Baselines<a class=\"headerlink\" href=\"#baselines\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-7\"></label><input class=\"margin-toggle\" id=\"marginnote-role-7\" name=\"marginnote-role-7\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/sreinforcement_learning/policy_grad.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>policy grad</a></span>\nHow can we improve the policy gradient method? Here is what seems nonsensical about the\nmethod: No matter how good the trajectory <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is, we try to improve the likelihood\nof the trajectory by pushing the weights in the direction of it. The return <span class=\"math notranslate nohighlight\">\\(G(\\tau)\\)</span> of the\ntrajectory only influences how hard we push. If <span class=\"math notranslate nohighlight\">\\(G(\\tau)\\)</span> is large, the change <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> will\nbe large, if <span class=\"math notranslate nohighlight\">\\(H(\\tau)\\)</span> is small, <span class=\"math notranslate nohighlight\">\\(\\Delta\\)</span> will be smaller \u2013 however, we always push to reinforce <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span>.</p><p>What would seem more promising: If <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is a trajectory that is exceptionally good, then we should\nmake it more likely, but if <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is worse than average, we should make it less likely, ie, push in\nthe opposite direction. That is, if <span class=\"math notranslate nohighlight\">\\(b\\)</span> is the average return of a trajectory then the sign of <span class=\"math notranslate nohighlight\">\\(G(\\tau)-b\\)</span>\ntells us whether <span class=\"math notranslate nohighlight\">\\(\\tau\\)</span> is better or worse than an average trajectory.\n(How can we compute <span class=\"math notranslate nohighlight\">\\(b\\)</span>? Easy: We sample a number of trajectories and take the average of the returns.)</p>", "a[href=\"#equation-cartnablal\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-cartnablal\">\n\\[\\frac{\\nabla \\pi(\\text{left}|s)}{\\pi(\\text{left}|s)} = -\\pi(\\text{right}|s)s\\]</div>", "a[href=\"#parameterised-policies\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.9. </span>Parameterised policies<a class=\"headerlink\" href=\"#parameterised-policies\" title=\"Link to this heading\">#</a></h2><p><span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning is not always possible. If the set of possible states or the set of possible actions is\nlarge then it becomes infeasible to compute <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values. There are two ways out of this: We may\ntry to learn a predictor for the <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values; or we may try to learn a policy that is not based on <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.\nLet\u2019s concentrate on the latter method.</p><p>Recall that every state in the <a class=\"reference internal\" href=\"#polesec\"><span class=\"std std-ref\">pole balancing</span></a> task is characterised by four parameters: position, velocity,\nangle and angular velocity.\nA linear policy in this case would, based on a weight vector <span class=\"math notranslate nohighlight\">\\(w\\in\\mathbb R^4\\)</span>,\ndecide for a state <span class=\"math notranslate nohighlight\">\\(s\\in\\mathbb R^4\\)</span>  as follows:</p>", "a[href=\"#policy-gradient-method\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.10. </span>Policy gradient method<a class=\"headerlink\" href=\"#policy-gradient-method\" title=\"Link to this heading\">#</a></h2><p>Assume that the agent is positioned in state <span class=\"math notranslate nohighlight\">\\(s\\)</span>, and assume that some\nstochastic parameterised policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span> is given.<label class=\"margin-toggle\" for=\"sidenote-role-6\"><span id=\"id6\">\n<sup>6</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-6\" name=\"sidenote-role-6\" type=\"checkbox\"/><span class=\"sidenote\"><sup>6</sup>This section is largely based on material of <a class=\"reference external\" href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\">OpenAI</a></span>\nHere the subscript <span class=\"math notranslate nohighlight\">\\(w\\)</span> indicates the set of weights\nthat describes the policy. The return we can expect with policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span> in state <span class=\"math notranslate nohighlight\">\\(s\\)</span> is then</p>", "a[href=\"#value-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.6. </span>Value functions<a class=\"headerlink\" href=\"#value-functions\" title=\"Link to this heading\">#</a></h2><p>In many reinforcement learning tasks, a reward will only be awarded after many steps. A chess game has no immediate rewards;\nit is only won or lost at the end. In most tasks actions may have consequences that only become apparent many steps later.\nIn an inventory control problem, it may  be beneficial in the short run to not order anything, as no costs for purchases\nor storage are incurred; in the long run, however, we will pay dearly when we cannot satisfy customer demands.</p><p>Immediate rewards, therefore, are not a good basis for the next action. It\u2019s more important to estimate the long term\nconsequences of actions. This is where the <em>value function</em> comes in: It estimates the long term returns we can reap\nin a given state.\nHow we value a state depends on the setting of the task, whether we optimise total returns or discounted returns.\nHowever, as discounted returns default to total returns if the discounting factor <span class=\"math notranslate nohighlight\">\\(\\gamma\\)</span> is set to 1, we\ncan treat both settings in the same way.\nGiven a  policy <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span>, we define for every state <span class=\"math notranslate nohighlight\">\\(s_0\\)</span> the value function as</p>", "a[href=\"#qlearnalg\"]": "<div class=\"proof algorithm admonition\" id=\"qlearnalg\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Algorithm 9.2 </span> (<span class=\"math notranslate nohighlight\">\\(q\\)</span>-learning)</p>\n<section class=\"algorithm-content\" id=\"proof-content\">\n<p><strong>Instance</strong> A reinforcement learning task, an <span class=\"math notranslate nohighlight\">\\(\\epsilon&gt;0\\)</span>, learning rates <span class=\"math notranslate nohighlight\">\\(\\eta_t\\)</span>.<br/>\n<strong>Output</strong> <span class=\"math notranslate nohighlight\">\\(q\\)</span>-values.</p>\n<ol class=\"arabic simple\">\n<li><p>Initialise <span class=\"math notranslate nohighlight\">\\(q_0\\)</span>-values, set <span class=\"math notranslate nohighlight\">\\(t:=0\\)</span>.</p></li>\n<li><p><strong>while</strong> termination condition not sat\u2019d:</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Start new episode with random start state <span class=\"math notranslate nohighlight\">\\(s\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<strong>while</strong> <span class=\"math notranslate nohighlight\">\\(s\\)</span> not terminal state:</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Choose action <span class=\"math notranslate nohighlight\">\\(a\\)</span> with <a class=\"reference internal\" href=\"#epsgreedyalg\">Algorithm 9.1</a>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Take action <span class=\"math notranslate nohighlight\">\\(a\\)</span>, observe reward <span class=\"math notranslate nohighlight\">\\(r\\)</span> and new state <span class=\"math notranslate nohighlight\">\\(s'\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set</p></li>\n</ol>\n<div class=\"math notranslate nohighlight\">\n\\[q_{t+1}(s,a):=q_t(s,a)+\\eta_t(s,a)(r+\\gamma \\max_{a'}q_t(s',a')-q_t(s,a))\\]</div>\n<ol class=\"arabic simple\" start=\"6\">\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set <span class=\"math notranslate nohighlight\">\\(t:=t+1\\)</span>, and <span class=\"math notranslate nohighlight\">\\(s:=s'\\)</span>.</p></li>\n<li><p><strong>output</strong> <span class=\"math notranslate nohighlight\">\\(q_{t-1}\\)</span>.</p></li>\n</ol>\n</section>\n</div>", "a[href=\"#equation-polimpeq2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-polimpeq2\">\n\\[v_{\\pi'}(s)\\geq v_\\pi(s) \\text{ for all }s\\in\\mathcal S\\]</div>", "a[href=\"#maze2fig\"]": "<figure class=\"align-default\" id=\"maze2fig\">\n<a class=\"reference internal image-reference\" href=\"../_images/maze2.png\"><img alt=\"../_images/maze2.png\" src=\"../_images/maze2.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 9.3 </span><span class=\"caption-text\">Transition probabilities from state <span class=\"math notranslate nohighlight\">\\(s\\)</span> if action <span class=\"math notranslate nohighlight\">\\(\\uparrow\\)</span> (go up) is taken.</span><a class=\"headerlink\" href=\"#maze2fig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#what-is-known-about-the-environment\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9.3. </span>What is known about the environment?<a class=\"headerlink\" href=\"#what-is-known-about-the-environment\" title=\"Link to this heading\">#</a></h2><p>When we train a reinforcement learning system, what do we usually know about the environment?\nIn particular, do we have access to the transition probability and the reward function?\nSometimes yes, sometimes no. This depends on the scenario.</p><p>In toy problems we often know everything about the environment. What about a real-life scenario?\nWhen we try to train a control algorithm for a gas turbine, or a Go playing system,\nwe cannot directly train in the real environment: That would take too long (wait for 1000000 people\nto play against your at the beginning amusingly bad Go playing system), be too costly (pay the\n1000000 people), or result in catastrophic failure (gas turbine explodes).\nInstead, we may train\u2026</p>", "a[href=\"#equation-polgradeq2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-polgradeq2\">\n\\[\\begin{split}\\begin{align}\n\\expec_{\\tau\\sim p_w} &amp; \\left[\n\\nabla_w \\log\\pi_w(A_t|S_t)b(S_t)\n\\right] \\notag\\\\\n&amp; =\n\\expec_{1\\to T}\\left[\n\\nabla_w \\log\\pi_w(A_t|S_t)b(S_t)\n\\right] \\notag\\\\\n&amp; =\n\\expec_{1\\to t}\\left[\n\\expec_{t+1\\to T}\\left[\n\\nabla_w \\log\\pi_w(A_t|S_t)b(S_t)\n\\right]\n\\right] \\notag\\\\\n&amp; =\n\\expec_{1\\to t}\\left[\nb(S_t)\n\\expec_{t+1\\to T}\\left[\n\\nabla_w \\log\\pi_w(A_t|S_t) \n\\right]\n\\right]\n\\end{align}\\end{split}\\]</div>", "a[href=\"#polimpthm3\"]": "<div class=\"proof theorem admonition\" id=\"polimpthm3\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Theorem 9.3 </span> (policy improvement)</p>\n<section class=\"theorem-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(\\pi\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\pi'\\)</span> be two policies such that</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nv_{\\pi'}(s)\\geq \\expec_{a\\sim\\pi}[q_{\\pi'}(s,a)]\\text{ for all }s\\in\\mathcal S.\n\\]</div>\n<p>Then</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nv_{\\pi'}(s)\\geq v_\\pi(s) \\text{ for all }s\\in\\mathcal S\n\\]</div>\n</section>\n</div>", "a[href=\"#rlfig\"]": "<figure class=\"align-default\" id=\"rlfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/basicRL.png\"><img alt=\"../_images/basicRL.png\" src=\"../_images/basicRL.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 9.1 </span><span class=\"caption-text\">Basic setup in reinforcement learning</span><a class=\"headerlink\" href=\"#rlfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#equation-expdelta\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-expdelta\">\n\\[\\expec[\\delta_{t+1}]=0\\]</div>", "a[href=\"#reinforcement-learning\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">9. </span>Reinforcement learning<a class=\"headerlink\" href=\"#reinforcement-learning\" title=\"Link to this heading\">#</a></h1><p>What is reinforcement learning?<label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup>The material in this chapter is based on <em>Reinforcement Learning</em>, R.S. Sutton and A.G. Barto, MIT Press (2018)\nand <em>Spinning Up in Deep RL</em>, J. Achiam, <a class=\"reference external\" href=\"https://spinningup.openai.com/en/latest/index.html\">link</a></span>\nIn reinforcement learning an autonomous agent interacts with a possibly unknown\nenvironment. Each action the agent takes results in a state change and a reward or penalty.\nThe agent strives to maximise the total reward.</p>", "a[href=\"#equation-cartnablar\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-cartnablar\">\n\\[\\frac{\\nabla \\pi(\\text{right}|s)}{\\pi(\\text{right}|s)} = \\frac{\\sigm\\left(\\trsp ws\\right)(1-\\sigm(\\left(\\trsp ws\\right)) s}{\\pi(\\text{right}|s)}\n= \\pi(\\text{left}|s)s\\]</div>", "a[href=\"#pgmalg\"]": "<div class=\"proof algorithm admonition\" id=\"pgmalg\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Algorithm 9.3 </span> (Policy gradient method)</p>\n<section class=\"algorithm-content\" id=\"proof-content\">\n<p><strong>Instance</strong> A RL environment, a parameterised policy <span class=\"math notranslate nohighlight\">\\(w\\mapsto \\pi_w\\)</span>.<br/>\n<strong>Output</strong> A better parameterised policy <span class=\"math notranslate nohighlight\">\\(\\pi_w\\)</span>.</p>\n<ol class=\"arabic simple\">\n<li><p>Set <span class=\"math notranslate nohighlight\">\\(i=1\\)</span>.</p></li>\n<li><p>Initialise <span class=\"math notranslate nohighlight\">\\(w^{(1)}\\)</span> to some value.</p></li>\n<li><p><strong>while</strong> stopping criterion not sat\u2019d:</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Generate episode <span class=\"math notranslate nohighlight\">\\(s_0,s_1,\\ldots\\)</span>, <span class=\"math notranslate nohighlight\">\\(a_0,a_1,\\ldots\\)</span>, <span class=\"math notranslate nohighlight\">\\(r_1,r_2,\\ldots\\)</span> following <span class=\"math notranslate nohighlight\">\\(\\pi_{w^{(i)}}\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute returns <span class=\"math notranslate nohighlight\">\\(g=\\sum_{k=0}^T\\gamma^{k}r_{k+1}\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute <span class=\"math notranslate nohighlight\">\\(\\Delta=g\\sum_{t=0}^{T} \\frac{\\nabla \\pi_{w^{(t)}}(a_t|s_t)}{\\pi_{w^{(t)}}(a_t|s_t)}\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute learning rate <span class=\"math notranslate nohighlight\">\\(\\eta_i\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set <span class=\"math notranslate nohighlight\">\\(w^{(i+1)}=w^{(i)}+\\eta_i\\Delta\\)</span>.</p></li>\n<li><p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set <span class=\"math notranslate nohighlight\">\\(i=i+1\\)</span>.</p></li>\n<li><p><strong>output</strong> <span class=\"math notranslate nohighlight\">\\(w^{(i-1)}\\)</span>.</p></li>\n</ol>\n</section>\n</div>", "a[href=\"#equation-rl1\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-rl1\">\n\\[\\begin{split}\\begin{align}\n\\nabla_w v_{\\pi_w}(s)= &amp;\\nabla_w  \\expec_{\\tau\\sim\\pi_w}[G(\\tau)]\\notag \\\\\n= &amp; \\nabla_w \\sum_{\\tau} \\proba[\\tau | w] G(\\tau)\\notag \\\\\n= &amp;  \\sum_{\\tau} \\nabla_w\\proba[\\tau | w] G(\\tau)  \\\\\n= &amp;  \\sum_{\\tau}  G(\\tau)\\nabla_w\\proba[\\tau | w],\n\\end{align}\\end{split}\\]</div>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
