selector_to_html = {"a[href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Roccurves.png/320px-Roccurves.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>A <b>receiver operating characteristic curve</b>, or <b>ROC curve</b>, is a graphical plot that illustrates the performance of a binary classifier model at varying threshold values.</p>", "a[href^=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#\"]": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Roccurves.png/320px-Roccurves.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>A <b>receiver operating characteristic curve</b>, or <b>ROC curve</b>, is a graphical plot that illustrates the performance of a binary classifier model at varying threshold values.</p>", "a[href=\"#common-loss-functions-in-regression\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.3. </span>Common loss functions in regression<a class=\"headerlink\" href=\"#common-loss-functions-in-regression\" title=\"Link to this heading\">#</a></h2><p>In classification we ultimately aim to minimise the expected number of misclassified samples.\nBecause it is not computationally feasible to\ndirectly minimise zero-one loss, we use surrogate loss functions.</p><p>In regression, we normally do not need to deal with computationally infeasible loss functions.\nOften we minimise directly the metric that we are interested in. In contrast, however,\nit is not obvious anymore what the key metric is. Depending on the data and on the task at\nhand, different metrics may be appropriate to measure the quality of the regressor.</p>", "a[href=\"#lossfunsfig\"]": "<figure class=\"align-default\" id=\"lossfunsfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/loss_functions.png\"><img alt=\"../_images/loss_functions.png\" src=\"../_images/loss_functions.png\" style=\"width: 12cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 6.1 </span><span class=\"caption-text\">Some loss functions used in classification. Shown is <span class=\"math notranslate nohighlight\">\\(\\phi:\\mathbb R\\to \\mathbb R_+\\)</span>, if the\nloss written as <span class=\"math notranslate nohighlight\">\\(\\ell(y,f(x)) = \\phi(yf(x))\\)</span>.</span><a class=\"headerlink\" href=\"#lossfunsfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#imbalanced-classes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.4. </span>Imbalanced classes<a class=\"headerlink\" href=\"#imbalanced-classes\" title=\"Link to this heading\">#</a></h2><p>Not all classes will always have the same importance in classification. A prime example is a spam filter: It is merely annoying if a spam email is not\ndetected, an important mail, however, that is banished to the spam folder may cause some grief. We\u2019d prefer if the\nspam filter is more cautious when classifying emails as spam.</p><p>A somewhat similar issue may arise when the classes are not present in equal numbers in the training set. Say, we try to classify cat and dog pictures.\nScraping the internet results in 90000 cat pictures but only 10000 dog pictures. For whatever reasons, we may still insist that our classifier perform\nequally well for cat and dog pictures \u2013 and that\u2019s a problem, as a classifier that always outputs `cat\u2019 will likely achieve an accuracy\nof around 90%.</p>", "a[href=\"#distlem\"]": "<div class=\"proof lemma admonition\" id=\"distlem\">\n<p class=\"admonition-title\"><span class=\"caption-number\">Lemma 6.1 </span></p>\n<section class=\"lemma-content\" id=\"proof-content\">\n<p>Let <span class=\"math notranslate nohighlight\">\\(w:\\mathcal Y\\to\\mathbb R_+\\)</span> be a class weighting. Then for every\ndistribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D_1\\)</span> there is a distribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D_2\\)</span> and a constant <span class=\"math notranslate nohighlight\">\\(Z&gt;0\\)</span>\nsuch that for all loss function <span class=\"math notranslate nohighlight\">\\(\\ell_1\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\ell_2\\)</span> with <span class=\"math notranslate nohighlight\">\\(\\ell_1=w\\ell_2\\)</span>\nit holds that</p>\n<div class=\"math notranslate nohighlight\">\n\\[\nL_{\\mathcal D_1,\\ell_1}(h) = ZL_{\\mathcal D_2,\\ell_2}(h)\n\\]</div>\n<p>for every classifier <span class=\"math notranslate nohighlight\">\\(h:\\mathcal X\\to\\mathcal Y\\)</span>.</p>\n</section>\n</div>", "a[href=\"convex.html#sgdsec\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">3.6. </span>Stochastic gradient descent<a class=\"headerlink\" href=\"#sgdsec\" title=\"Link to this heading\">#</a></h2><p>Gradient descent is a quite efficient algorithm. Under mild assumptions and with the right\n(adaptable) learning rate it can be shown that the error <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span>, the difference <span class=\"math notranslate nohighlight\">\\(f(\\overline x)-f(x^*)\\)</span>,\ndecreases exponentially with the number of iterations, i.e. that</p>", "a[href=\"conslossproof.html#consproofsec\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Proof of consistency theorem<a class=\"headerlink\" href=\"#proof-of-consistency-theorem\" title=\"Link to this heading\">#</a></h1><p>We prove <a class=\"reference internal\" href=\"loss.html#consistencythm\">Theorem 6.1</a><label class=\"margin-toggle\" for=\"sidenote-role-1\"><span id=\"id1\">\n<sup>1</sup></span>\n</label><input class=\"margin-toggle\" id=\"sidenote-role-1\" name=\"sidenote-role-1\" type=\"checkbox\"/><span class=\"sidenote\"><sup>1</sup><em>Convexity, Classification, and Risk Bounds</em>, P.L. Bartlett, M.I. Jordan and J.D. MacAuliffe (2003)</span> by establishing a number of claims.\nWe repeat the theorem here:</p>", "a[href=\"#bayes-consistency-and-arbitrary-classification-losses\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.7. </span>Bayes consistency and arbitrary classification losses<a class=\"headerlink\" href=\"#bayes-consistency-and-arbitrary-classification-losses\" title=\"Link to this heading\">#</a></h2><p>Surrogate loss functions are computationally feasible loss function that can be minimised\ninstead of zero-one loss. If the loss function is Bayes-consistent, then the minimiser\nof the surrogate loss function will be a Bayes-classifier. How do we need to adapt\nthe surrogate losses if we aim for a different loss than zero-one loss?</p><p>Let <span class=\"math notranslate nohighlight\">\\(\\ell^*:\\mathcal Y\\times\\mathcal Y\\to\\mathbb R_+\\)</span> be a loss function.\nA (surrogate) loss function <span class=\"math notranslate nohighlight\">\\(\\ell:\\mathcal Y\\times\\mathcal Y\\to\\mathbb R_+\\)</span>\nis <em>Bayes-consistent for <span class=\"math notranslate nohighlight\">\\(\\ell^*\\)</span></em> if for every distribution <span class=\"math notranslate nohighlight\">\\(\\mathcal D\\)</span> on\n<span class=\"math notranslate nohighlight\">\\(\\mathcal X\\times\\mathcal Y\\)</span> and\nfor every sequence\n<span class=\"math notranslate nohighlight\">\\(h_1,h_2,\\ldots:\\mathcal X\\to\\mathcal Y\\)</span> of classifiers with</p>", "a[href=\"#trade-off-between-true-positive-and-false-positive-rate\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.5. </span>Trade-off between true positive and false positive rate<a class=\"headerlink\" href=\"#trade-off-between-true-positive-and-false-positive-rate\" title=\"Link to this heading\">#</a></h2><p><label class=\"margin-toggle marginnote-label\" for=\"marginnote-role-4\"></label><input class=\"margin-toggle\" id=\"marginnote-role-4\" name=\"marginnote-role-4\" type=\"checkbox\"/><span class=\"marginnote\"> <a class=\"reference external\" href=\"https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/losses/lendingclub.ipynb\"><svg aria-hidden=\"true\" class=\"sd-material-icon sd-material-icon-terminal\" height=\"2.0em\" version=\"4.0.0.63c5cb3\" viewbox=\"0 0 24 24\" width=\"2.0em\"><g><rect fill=\"none\" height=\"24\" width=\"24\"></rect></g><g><path d=\"M20,4H4C2.89,4,2,4.9,2,6v12c0,1.1,0.89,2,2,2h16c1.1,0,2-0.9,2-2V6C22,4.9,21.11,4,20,4z M20,18H4V8h16V18z M18,17h-6v-2 h6V17z M7.5,17l-1.41-1.41L8.67,13l-2.59-2.59L7.5,9l4,4L7.5,17z\"></path></g></svg>lendingclub</a></span>\nFor binary classification, we can express a different importance of the classes as\ndiffering preferences for true positive and negative rate. That is, we may specify\na target true negative rate of 90% and then optimise the true positive rate as much as possible.</p>", "a[href=\"#equation-zeroonephi\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-zeroonephi\">\n\\[\\begin{split}\\begin{align}\n\\ell_{0-1}(y,f(x)) &amp; =\\begin{cases}\n0 &amp;\\text{ if }\\sgn f(x)=y\\Leftrightarrow\\sgn yf(x)=1\\\\\n1 &amp;\\text{ if }\\sgn f(x)\\neq y\\Leftrightarrow\\sgn yf(x)=-1\n\\end{cases} \\notag \\\\\n&amp; = 1_{\\sgn yf(x)=-1}, \n\\end{align}\\end{split}\\]</div>", "a[href=\"#rocfig\"]": "<figure class=\"align-default\" id=\"rocfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/roc.png\"><img alt=\"../_images/roc.png\" src=\"../_images/roc.png\" style=\"height: 6cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 6.3 </span><span class=\"caption-text\">A ROC curve.</span><a class=\"headerlink\" href=\"#rocfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#class-weights-in-losses\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.6. </span>Class weights in losses<a class=\"headerlink\" href=\"#class-weights-in-losses\" title=\"Link to this heading\">#</a></h2><p>We return to, potentially, multi-class classification.\nOften, the key metric in classification is simply <em>zero-one loss</em>, ie</p>", "a[href=\"#reglossfunsfig\"]": "<figure class=\"align-default\" id=\"reglossfunsfig\">\n<a class=\"reference internal image-reference\" href=\"../_images/regression_losses.png\"><img alt=\"../_images/regression_losses.png\" src=\"../_images/regression_losses.png\" style=\"width: 15cm;\"/>\n</a>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 6.2 </span><span class=\"caption-text\">Square error, absolute error and Huber loss, shown as functions\nof the prediction error <span class=\"math notranslate nohighlight\">\\(y^*-y\\)</span>.</span><a class=\"headerlink\" href=\"#reglossfunsfig\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"#loss-functions-in-classification\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.1. </span>Loss functions in classification<a class=\"headerlink\" href=\"#loss-functions-in-classification\" title=\"Link to this heading\">#</a></h2><p>A key metric of interest in classification is the misclassification rate, ie,\nzero-one loss. Directly minimising zero-one loss, however, is usually\ncomputationally infeasible. Instead, we minimise <em>surrogate</em> losses, losses\nthat are better behaved. What are common losses?</p><p>In a <em>binary classification</em> problem, with classes -1 and 1, we often compute\na classifier of the type <span class=\"math notranslate nohighlight\">\\(h:x\\in\\mathcal X\\mapsto \\sgn\\circ f(x)\\)</span>, where\n<span class=\"math notranslate nohighlight\">\\(f:\\mathcal X\\to\\mathbb R\\)</span> is some function. That is, if <span class=\"math notranslate nohighlight\">\\(f(x)&lt;0\\)</span> then\nwe classify <span class=\"math notranslate nohighlight\">\\(x\\)</span> as class -1, and if <span class=\"math notranslate nohighlight\">\\(f(x)\\geq 0\\)</span> then <span class=\"math notranslate nohighlight\">\\(x\\)</span> is predicted\nto have class 1. In a neural network, or in logistic regression,\n<span class=\"math notranslate nohighlight\">\\(f\\)</span> is passed further through the logistic function, so that the classifier\nultimately computes a probability estimate in <span class=\"math notranslate nohighlight\">\\([0,1]\\)</span>, how likely it is that the sample is class 1.</p>", "a[href=\"#loss-functions\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6. </span>Loss functions<a class=\"headerlink\" href=\"#loss-functions\" title=\"Link to this heading\">#</a></h1><p>Loss functions play a major role in classification as well as in regression.\nThat role, however, is not the same. Let us first examine how loss functions\ndetermine the quality of a classifier. At the end of this section, we\nconsider different loss functions for regression.</p>", "a[href=\"#bayes-consistent-loss-functions\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">6.2. </span>Bayes consistent loss functions<a class=\"headerlink\" href=\"#bayes-consistent-loss-functions\" title=\"Link to this heading\">#</a></h2><p>What properties should a good loss function satisfy? Recall that logistic, exponential and square loss are <em>surrogate</em> loss\nfunctions: We are not really interested in a small logistic loss or a small square loss \u2013 rather, our aim\nis to minimise true risk, ie, expected zero-one loss. Zero-one loss, however, is not smooth and because of that difficult to minimise directly.\nThe surrogate losses discussed in the previous section are all smooth and thus easier to minimise.</p><p>As can be seen in <a class=\"reference internal\" href=\"#lossfunsfig\"><span class=\"std std-numref\">Fig. 6.1</span></a>, each of logistic, square and exponential loss upper-bounds zero-one loss.\nThat is good, because it means that when the surrogate loss becomes smaller then, usually, zero-loss will decrease as well.</p>", "a[href=\"#equation-d1d2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-d1d2\">\n\\[\\proba_{\\mathcal D_2}[y|x]=\\frac{w(y)\\proba_{\\mathcal D_1}[y|x]}{\\sum_{y'\\in\\mathcal Y}\\proba_{\\mathcal D_1}[y'|x]w(y')}\\]</div>", "a[href=\"#equation-xd1d2\"]": "<div class=\"math notranslate nohighlight\" id=\"equation-xd1d2\">\n\\[\\proba_{\\mathcal D_2}[(x,y)]=\\frac{1}{Z}w(y)\\proba_{\\mathcal D_1}[(x,y)]\\quad\\text{for all }(x,y)\\in\\mathcal X\\times\\mathcal Y,\\]</div>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(` ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
